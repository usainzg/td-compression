{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usainzg/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorly as tl\n",
    "from tensorly.decomposition import partial_tucker\n",
    "from tltorch import FactorizedConv\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def tensorize_basic_block(basic_block, factorization=\"tucker\", rank=0.5, decompose_weights=False):\n",
    "    conv1 = basic_block.conv1\n",
    "    conv2 = basic_block.conv2\n",
    "    new_conv1 = FactorizedConv.from_conv(conv1, factorization=factorization, decompose_weights=decompose_weights, fixed_rank_modes='spatial', rank=rank)\n",
    "    new_conv2 = FactorizedConv.from_conv(conv2, factorization=factorization, decompose_weights=decompose_weights, fixed_rank_modes='spatial', rank=rank)\n",
    "    basic_block.conv1 = new_conv1\n",
    "    basic_block.conv2 = new_conv2\n",
    "    return basic_block\n",
    "\n",
    "def tensorize_resnet(model, factorization=\"tucker\", rank=0.5, decompose_weights=False):\n",
    "    tensorized_model = copy.deepcopy(model)\n",
    "    for layer in range(1, 5):\n",
    "        basic_block_1 = getattr(tensorized_model, f'layer{layer}')[0]\n",
    "        basic_block_2 = getattr(tensorized_model, f'layer{layer}')[1]\n",
    "        basic_block_1 = tensorize_basic_block(basic_block_1, factorization=factorization, rank=rank, decompose_weights=decompose_weights)\n",
    "        basic_block_2 = tensorize_basic_block(basic_block_2, factorization=factorization, rank=rank, decompose_weights=decompose_weights)\n",
    "        setattr(tensorized_model, f'layer{layer}', torch.nn.Sequential(basic_block_1, basic_block_2))\n",
    "    return tensorized_model\n",
    "\n",
    "resnet_tn = tensorize_resnet(resnet, rank=0.8, decompose_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(39, 39, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(64, 64, 3, 3), rank=(39, 39, 3, 3))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(39, 39, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(64, 64, 3, 3), rank=(39, 39, 3, 3))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(39, 39, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(64, 64, 3, 3), rank=(39, 39, 3, 3))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(39, 39, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(64, 64, 3, 3), rank=(39, 39, 3, 3))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=64, out_channels=128, kernel_size=(3, 3), rank=(74, 37, 3, 3), order=2, stride=[2, 2], padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(128, 64, 3, 3), rank=(74, 37, 3, 3))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=128, out_channels=128, kernel_size=(3, 3), rank=(77, 77, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(128, 128, 3, 3), rank=(77, 77, 3, 3))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=128, out_channels=128, kernel_size=(3, 3), rank=(77, 77, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(128, 128, 3, 3), rank=(77, 77, 3, 3))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=128, out_channels=128, kernel_size=(3, 3), rank=(77, 77, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(128, 128, 3, 3), rank=(77, 77, 3, 3))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=128, out_channels=256, kernel_size=(3, 3), rank=(149, 74, 3, 3), order=2, stride=[2, 2], padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(256, 128, 3, 3), rank=(149, 74, 3, 3))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=256, out_channels=256, kernel_size=(3, 3), rank=(155, 155, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(256, 256, 3, 3), rank=(155, 155, 3, 3))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=256, out_channels=256, kernel_size=(3, 3), rank=(155, 155, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(256, 256, 3, 3), rank=(155, 155, 3, 3))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=256, out_channels=256, kernel_size=(3, 3), rank=(155, 155, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(256, 256, 3, 3), rank=(155, 155, 3, 3))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=256, out_channels=512, kernel_size=(3, 3), rank=(298, 149, 3, 3), order=2, stride=[2, 2], padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(512, 256, 3, 3), rank=(298, 149, 3, 3))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=512, out_channels=512, kernel_size=(3, 3), rank=(310, 310, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(512, 512, 3, 3), rank=(310, 310, 3, 3))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=512, out_channels=512, kernel_size=(3, 3), rank=(310, 310, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(512, 512, 3, 3), rank=(310, 310, 3, 3))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=512, out_channels=512, kernel_size=(3, 3), rank=(310, 310, 3, 3), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TuckerTensor(shape=(512, 512, 3, 3), rank=(310, 310, 3, 3))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 32, 32)\n",
    "y = resnet(x)\n",
    "y_tn = resnet_tn(x)\n",
    "print(y.shape)\n",
    "print(y_tn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_params(net: torch.nn.Module) -> np.array:\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet: 11173962\n",
      "resnet_tn: 5690166\n",
      "compression ratio: 1.9637321652830515\n"
     ]
    }
   ],
   "source": [
    "print(f'resnet: {count_params(resnet)}')\n",
    "print(f'resnet_tn: {count_params(resnet_tn)}')\n",
    "\n",
    "print(f'compression ratio: {count_params(resnet) / count_params(resnet_tn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(resnet.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "def train(epoch, net):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "def test(epoch, net):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 391 Loss: 1.682 | Acc: 38.281% (49/128)\n",
      "1 391 Loss: 1.700 | Acc: 36.328% (93/256)\n",
      "2 391 Loss: 1.698 | Acc: 35.938% (138/384)\n",
      "3 391 Loss: 1.681 | Acc: 37.891% (194/512)\n",
      "4 391 Loss: 1.662 | Acc: 38.750% (248/640)\n",
      "5 391 Loss: 1.669 | Acc: 38.542% (296/768)\n",
      "6 391 Loss: 1.646 | Acc: 40.290% (361/896)\n",
      "7 391 Loss: 1.638 | Acc: 39.941% (409/1024)\n",
      "8 391 Loss: 1.624 | Acc: 40.972% (472/1152)\n",
      "9 391 Loss: 1.618 | Acc: 41.328% (529/1280)\n",
      "10 391 Loss: 1.621 | Acc: 41.406% (583/1408)\n",
      "11 391 Loss: 1.616 | Acc: 41.797% (642/1536)\n",
      "12 391 Loss: 1.618 | Acc: 41.406% (689/1664)\n",
      "13 391 Loss: 1.628 | Acc: 41.016% (735/1792)\n",
      "14 391 Loss: 1.629 | Acc: 40.885% (785/1920)\n",
      "15 391 Loss: 1.628 | Acc: 40.869% (837/2048)\n",
      "16 391 Loss: 1.632 | Acc: 40.349% (878/2176)\n",
      "17 391 Loss: 1.630 | Acc: 40.365% (930/2304)\n",
      "18 391 Loss: 1.631 | Acc: 40.378% (982/2432)\n",
      "19 391 Loss: 1.635 | Acc: 40.117% (1027/2560)\n",
      "20 391 Loss: 1.636 | Acc: 40.141% (1079/2688)\n",
      "21 391 Loss: 1.635 | Acc: 40.163% (1131/2816)\n",
      "22 391 Loss: 1.632 | Acc: 40.183% (1183/2944)\n",
      "23 391 Loss: 1.627 | Acc: 40.462% (1243/3072)\n",
      "24 391 Loss: 1.625 | Acc: 40.438% (1294/3200)\n",
      "25 391 Loss: 1.622 | Acc: 40.355% (1343/3328)\n",
      "26 391 Loss: 1.620 | Acc: 40.249% (1391/3456)\n",
      "27 391 Loss: 1.619 | Acc: 40.458% (1450/3584)\n",
      "28 391 Loss: 1.615 | Acc: 40.760% (1513/3712)\n",
      "29 391 Loss: 1.618 | Acc: 40.547% (1557/3840)\n",
      "30 391 Loss: 1.615 | Acc: 40.575% (1610/3968)\n",
      "31 391 Loss: 1.613 | Acc: 40.503% (1659/4096)\n",
      "32 391 Loss: 1.610 | Acc: 40.625% (1716/4224)\n",
      "33 391 Loss: 1.613 | Acc: 40.556% (1765/4352)\n",
      "34 391 Loss: 1.611 | Acc: 40.692% (1823/4480)\n",
      "35 391 Loss: 1.611 | Acc: 40.712% (1876/4608)\n",
      "36 391 Loss: 1.611 | Acc: 40.688% (1927/4736)\n",
      "37 391 Loss: 1.614 | Acc: 40.563% (1973/4864)\n",
      "38 391 Loss: 1.612 | Acc: 40.525% (2023/4992)\n",
      "39 391 Loss: 1.610 | Acc: 40.449% (2071/5120)\n",
      "40 391 Loss: 1.608 | Acc: 40.587% (2130/5248)\n",
      "41 391 Loss: 1.607 | Acc: 40.681% (2187/5376)\n",
      "42 391 Loss: 1.610 | Acc: 40.607% (2235/5504)\n",
      "43 391 Loss: 1.612 | Acc: 40.447% (2278/5632)\n",
      "44 391 Loss: 1.612 | Acc: 40.556% (2336/5760)\n",
      "45 391 Loss: 1.612 | Acc: 40.455% (2382/5888)\n",
      "46 391 Loss: 1.612 | Acc: 40.592% (2442/6016)\n",
      "47 391 Loss: 1.612 | Acc: 40.609% (2495/6144)\n",
      "48 391 Loss: 1.614 | Acc: 40.609% (2547/6272)\n",
      "49 391 Loss: 1.615 | Acc: 40.719% (2606/6400)\n",
      "50 391 Loss: 1.612 | Acc: 40.794% (2663/6528)\n",
      "51 391 Loss: 1.611 | Acc: 40.835% (2718/6656)\n",
      "52 391 Loss: 1.612 | Acc: 40.846% (2771/6784)\n",
      "53 391 Loss: 1.614 | Acc: 40.668% (2811/6912)\n",
      "54 391 Loss: 1.613 | Acc: 40.724% (2867/7040)\n",
      "55 391 Loss: 1.612 | Acc: 40.876% (2930/7168)\n",
      "56 391 Loss: 1.610 | Acc: 40.885% (2983/7296)\n",
      "57 391 Loss: 1.609 | Acc: 40.827% (3031/7424)\n",
      "58 391 Loss: 1.609 | Acc: 40.943% (3092/7552)\n",
      "59 391 Loss: 1.608 | Acc: 41.003% (3149/7680)\n",
      "60 391 Loss: 1.610 | Acc: 40.817% (3187/7808)\n",
      "61 391 Loss: 1.609 | Acc: 40.827% (3240/7936)\n",
      "62 391 Loss: 1.609 | Acc: 40.799% (3290/8064)\n",
      "63 391 Loss: 1.610 | Acc: 40.662% (3331/8192)\n",
      "64 391 Loss: 1.611 | Acc: 40.685% (3385/8320)\n",
      "65 391 Loss: 1.611 | Acc: 40.696% (3438/8448)\n",
      "66 391 Loss: 1.609 | Acc: 40.788% (3498/8576)\n",
      "67 391 Loss: 1.609 | Acc: 40.763% (3548/8704)\n",
      "68 391 Loss: 1.609 | Acc: 40.784% (3602/8832)\n",
      "69 391 Loss: 1.609 | Acc: 40.792% (3655/8960)\n",
      "70 391 Loss: 1.608 | Acc: 40.856% (3713/9088)\n",
      "71 391 Loss: 1.608 | Acc: 40.842% (3764/9216)\n",
      "72 391 Loss: 1.607 | Acc: 40.893% (3821/9344)\n",
      "73 391 Loss: 1.607 | Acc: 40.878% (3872/9472)\n",
      "74 391 Loss: 1.609 | Acc: 40.740% (3911/9600)\n",
      "75 391 Loss: 1.608 | Acc: 40.748% (3964/9728)\n",
      "76 391 Loss: 1.609 | Acc: 40.757% (4017/9856)\n",
      "77 391 Loss: 1.609 | Acc: 40.765% (4070/9984)\n",
      "78 391 Loss: 1.608 | Acc: 40.763% (4122/10112)\n",
      "79 391 Loss: 1.607 | Acc: 40.781% (4176/10240)\n",
      "80 391 Loss: 1.607 | Acc: 40.750% (4225/10368)\n",
      "81 391 Loss: 1.606 | Acc: 40.768% (4279/10496)\n",
      "82 391 Loss: 1.607 | Acc: 40.644% (4318/10624)\n",
      "83 391 Loss: 1.607 | Acc: 40.625% (4368/10752)\n",
      "84 391 Loss: 1.608 | Acc: 40.588% (4416/10880)\n",
      "85 391 Loss: 1.609 | Acc: 40.552% (4464/11008)\n",
      "86 391 Loss: 1.608 | Acc: 40.589% (4520/11136)\n",
      "87 391 Loss: 1.608 | Acc: 40.510% (4563/11264)\n",
      "88 391 Loss: 1.608 | Acc: 40.572% (4622/11392)\n",
      "89 391 Loss: 1.608 | Acc: 40.608% (4678/11520)\n",
      "90 391 Loss: 1.607 | Acc: 40.677% (4738/11648)\n",
      "91 391 Loss: 1.607 | Acc: 40.693% (4792/11776)\n",
      "92 391 Loss: 1.605 | Acc: 40.726% (4848/11904)\n",
      "93 391 Loss: 1.605 | Acc: 40.725% (4900/12032)\n",
      "94 391 Loss: 1.605 | Acc: 40.715% (4951/12160)\n",
      "95 391 Loss: 1.605 | Acc: 40.706% (5002/12288)\n",
      "96 391 Loss: 1.605 | Acc: 40.746% (5059/12416)\n",
      "97 391 Loss: 1.604 | Acc: 40.784% (5116/12544)\n",
      "98 391 Loss: 1.605 | Acc: 40.759% (5165/12672)\n",
      "99 391 Loss: 1.605 | Acc: 40.766% (5218/12800)\n",
      "100 391 Loss: 1.604 | Acc: 40.795% (5274/12928)\n",
      "101 391 Loss: 1.604 | Acc: 40.809% (5328/13056)\n",
      "102 391 Loss: 1.603 | Acc: 40.875% (5389/13184)\n",
      "103 391 Loss: 1.603 | Acc: 40.828% (5435/13312)\n",
      "104 391 Loss: 1.604 | Acc: 40.804% (5484/13440)\n",
      "105 391 Loss: 1.603 | Acc: 40.853% (5543/13568)\n",
      "106 391 Loss: 1.602 | Acc: 40.881% (5599/13696)\n",
      "107 391 Loss: 1.602 | Acc: 40.820% (5643/13824)\n",
      "108 391 Loss: 1.602 | Acc: 40.826% (5696/13952)\n",
      "109 391 Loss: 1.602 | Acc: 40.788% (5743/14080)\n",
      "110 391 Loss: 1.601 | Acc: 40.878% (5808/14208)\n",
      "111 391 Loss: 1.600 | Acc: 40.960% (5872/14336)\n",
      "112 391 Loss: 1.599 | Acc: 40.985% (5928/14464)\n",
      "113 391 Loss: 1.599 | Acc: 41.016% (5985/14592)\n",
      "114 391 Loss: 1.599 | Acc: 41.073% (6046/14720)\n",
      "115 391 Loss: 1.599 | Acc: 41.029% (6092/14848)\n",
      "116 391 Loss: 1.597 | Acc: 41.092% (6154/14976)\n",
      "117 391 Loss: 1.597 | Acc: 41.075% (6204/15104)\n",
      "118 391 Loss: 1.597 | Acc: 41.091% (6259/15232)\n",
      "119 391 Loss: 1.596 | Acc: 41.081% (6310/15360)\n",
      "120 391 Loss: 1.597 | Acc: 41.071% (6361/15488)\n",
      "121 391 Loss: 1.596 | Acc: 41.157% (6427/15616)\n",
      "122 391 Loss: 1.596 | Acc: 41.159% (6480/15744)\n",
      "123 391 Loss: 1.597 | Acc: 41.091% (6522/15872)\n",
      "124 391 Loss: 1.597 | Acc: 41.100% (6576/16000)\n",
      "125 391 Loss: 1.597 | Acc: 41.102% (6629/16128)\n",
      "126 391 Loss: 1.596 | Acc: 41.080% (6678/16256)\n",
      "127 391 Loss: 1.597 | Acc: 41.101% (6734/16384)\n",
      "128 391 Loss: 1.596 | Acc: 41.152% (6795/16512)\n",
      "129 391 Loss: 1.596 | Acc: 41.196% (6855/16640)\n",
      "130 391 Loss: 1.595 | Acc: 41.209% (6910/16768)\n",
      "131 391 Loss: 1.595 | Acc: 41.223% (6965/16896)\n",
      "132 391 Loss: 1.596 | Acc: 41.183% (7011/17024)\n",
      "133 391 Loss: 1.596 | Acc: 41.179% (7063/17152)\n",
      "134 391 Loss: 1.595 | Acc: 41.163% (7113/17280)\n",
      "135 391 Loss: 1.596 | Acc: 41.171% (7167/17408)\n",
      "136 391 Loss: 1.596 | Acc: 41.172% (7220/17536)\n",
      "137 391 Loss: 1.594 | Acc: 41.248% (7286/17664)\n",
      "138 391 Loss: 1.595 | Acc: 41.243% (7338/17792)\n",
      "139 391 Loss: 1.594 | Acc: 41.211% (7385/17920)\n",
      "140 391 Loss: 1.594 | Acc: 41.146% (7426/18048)\n",
      "141 391 Loss: 1.595 | Acc: 41.082% (7467/18176)\n",
      "142 391 Loss: 1.595 | Acc: 41.051% (7514/18304)\n",
      "143 391 Loss: 1.595 | Acc: 41.005% (7558/18432)\n",
      "144 391 Loss: 1.595 | Acc: 41.045% (7618/18560)\n",
      "145 391 Loss: 1.595 | Acc: 41.074% (7676/18688)\n",
      "146 391 Loss: 1.594 | Acc: 41.061% (7726/18816)\n",
      "147 391 Loss: 1.594 | Acc: 41.095% (7785/18944)\n",
      "148 391 Loss: 1.594 | Acc: 41.097% (7838/19072)\n",
      "149 391 Loss: 1.593 | Acc: 41.135% (7898/19200)\n",
      "150 391 Loss: 1.593 | Acc: 41.142% (7952/19328)\n",
      "151 391 Loss: 1.593 | Acc: 41.144% (8005/19456)\n",
      "152 391 Loss: 1.592 | Acc: 41.156% (8060/19584)\n",
      "153 391 Loss: 1.593 | Acc: 41.127% (8107/19712)\n",
      "154 391 Loss: 1.593 | Acc: 41.139% (8162/19840)\n",
      "155 391 Loss: 1.593 | Acc: 41.166% (8220/19968)\n",
      "156 391 Loss: 1.593 | Acc: 41.187% (8277/20096)\n",
      "157 391 Loss: 1.592 | Acc: 41.223% (8337/20224)\n",
      "158 391 Loss: 1.593 | Acc: 41.195% (8384/20352)\n",
      "159 391 Loss: 1.593 | Acc: 41.143% (8426/20480)\n",
      "160 391 Loss: 1.593 | Acc: 41.130% (8476/20608)\n",
      "161 391 Loss: 1.593 | Acc: 41.180% (8539/20736)\n",
      "162 391 Loss: 1.593 | Acc: 41.205% (8597/20864)\n",
      "163 391 Loss: 1.592 | Acc: 41.216% (8652/20992)\n",
      "164 391 Loss: 1.592 | Acc: 41.222% (8706/21120)\n",
      "165 391 Loss: 1.591 | Acc: 41.260% (8767/21248)\n",
      "166 391 Loss: 1.590 | Acc: 41.317% (8832/21376)\n",
      "167 391 Loss: 1.591 | Acc: 41.285% (8878/21504)\n",
      "168 391 Loss: 1.591 | Acc: 41.235% (8920/21632)\n",
      "169 391 Loss: 1.591 | Acc: 41.245% (8975/21760)\n",
      "170 391 Loss: 1.591 | Acc: 41.255% (9030/21888)\n",
      "171 391 Loss: 1.592 | Acc: 41.229% (9077/22016)\n",
      "172 391 Loss: 1.591 | Acc: 41.221% (9128/22144)\n",
      "173 391 Loss: 1.592 | Acc: 41.191% (9174/22272)\n",
      "174 391 Loss: 1.592 | Acc: 41.196% (9228/22400)\n",
      "175 391 Loss: 1.591 | Acc: 41.220% (9286/22528)\n",
      "176 391 Loss: 1.590 | Acc: 41.252% (9346/22656)\n",
      "177 391 Loss: 1.590 | Acc: 41.239% (9396/22784)\n",
      "178 391 Loss: 1.589 | Acc: 41.258% (9453/22912)\n",
      "179 391 Loss: 1.589 | Acc: 41.246% (9503/23040)\n",
      "180 391 Loss: 1.589 | Acc: 41.259% (9559/23168)\n",
      "181 391 Loss: 1.589 | Acc: 41.260% (9612/23296)\n",
      "182 391 Loss: 1.589 | Acc: 41.270% (9667/23424)\n",
      "183 391 Loss: 1.588 | Acc: 41.279% (9722/23552)\n",
      "184 391 Loss: 1.588 | Acc: 41.343% (9790/23680)\n",
      "185 391 Loss: 1.587 | Acc: 41.343% (9843/23808)\n",
      "186 391 Loss: 1.588 | Acc: 41.352% (9898/23936)\n",
      "187 391 Loss: 1.587 | Acc: 41.340% (9948/24064)\n",
      "188 391 Loss: 1.587 | Acc: 41.357% (10005/24192)\n",
      "189 391 Loss: 1.588 | Acc: 41.324% (10050/24320)\n",
      "190 391 Loss: 1.589 | Acc: 41.296% (10096/24448)\n",
      "191 391 Loss: 1.588 | Acc: 41.300% (10150/24576)\n",
      "192 391 Loss: 1.588 | Acc: 41.341% (10213/24704)\n",
      "193 391 Loss: 1.588 | Acc: 41.350% (10268/24832)\n",
      "194 391 Loss: 1.587 | Acc: 41.386% (10330/24960)\n",
      "195 391 Loss: 1.587 | Acc: 41.370% (10379/25088)\n",
      "196 391 Loss: 1.587 | Acc: 41.386% (10436/25216)\n",
      "197 391 Loss: 1.586 | Acc: 41.410% (10495/25344)\n",
      "198 391 Loss: 1.587 | Acc: 41.391% (10543/25472)\n",
      "199 391 Loss: 1.586 | Acc: 41.383% (10594/25600)\n",
      "200 391 Loss: 1.587 | Acc: 41.352% (10639/25728)\n",
      "201 391 Loss: 1.586 | Acc: 41.364% (10695/25856)\n",
      "202 391 Loss: 1.586 | Acc: 41.372% (10750/25984)\n",
      "203 391 Loss: 1.586 | Acc: 41.376% (10804/26112)\n",
      "204 391 Loss: 1.586 | Acc: 41.395% (10862/26240)\n",
      "205 391 Loss: 1.586 | Acc: 41.380% (10911/26368)\n",
      "206 391 Loss: 1.586 | Acc: 41.406% (10971/26496)\n",
      "207 391 Loss: 1.586 | Acc: 41.406% (11024/26624)\n",
      "208 391 Loss: 1.586 | Acc: 41.399% (11075/26752)\n",
      "209 391 Loss: 1.586 | Acc: 41.362% (11118/26880)\n",
      "210 391 Loss: 1.585 | Acc: 41.362% (11171/27008)\n",
      "211 391 Loss: 1.585 | Acc: 41.358% (11223/27136)\n",
      "212 391 Loss: 1.585 | Acc: 41.399% (11287/27264)\n",
      "213 391 Loss: 1.585 | Acc: 41.373% (11333/27392)\n",
      "214 391 Loss: 1.585 | Acc: 41.399% (11393/27520)\n",
      "215 391 Loss: 1.585 | Acc: 41.410% (11449/27648)\n",
      "216 391 Loss: 1.585 | Acc: 41.399% (11499/27776)\n",
      "217 391 Loss: 1.584 | Acc: 41.417% (11557/27904)\n",
      "218 391 Loss: 1.584 | Acc: 41.463% (11623/28032)\n",
      "219 391 Loss: 1.584 | Acc: 41.452% (11673/28160)\n",
      "220 391 Loss: 1.584 | Acc: 41.466% (11730/28288)\n",
      "221 391 Loss: 1.584 | Acc: 41.487% (11789/28416)\n",
      "222 391 Loss: 1.584 | Acc: 41.504% (11847/28544)\n",
      "223 391 Loss: 1.584 | Acc: 41.511% (11902/28672)\n",
      "224 391 Loss: 1.584 | Acc: 41.503% (11953/28800)\n",
      "225 391 Loss: 1.584 | Acc: 41.493% (12003/28928)\n",
      "226 391 Loss: 1.584 | Acc: 41.513% (12062/29056)\n",
      "227 391 Loss: 1.583 | Acc: 41.530% (12120/29184)\n",
      "228 391 Loss: 1.582 | Acc: 41.550% (12179/29312)\n",
      "229 391 Loss: 1.582 | Acc: 41.566% (12237/29440)\n",
      "230 391 Loss: 1.582 | Acc: 41.592% (12298/29568)\n",
      "231 391 Loss: 1.582 | Acc: 41.598% (12353/29696)\n",
      "232 391 Loss: 1.582 | Acc: 41.607% (12409/29824)\n",
      "233 391 Loss: 1.582 | Acc: 41.633% (12470/29952)\n",
      "234 391 Loss: 1.581 | Acc: 41.649% (12528/30080)\n",
      "235 391 Loss: 1.581 | Acc: 41.655% (12583/30208)\n",
      "236 391 Loss: 1.581 | Acc: 41.670% (12641/30336)\n",
      "237 391 Loss: 1.581 | Acc: 41.656% (12690/30464)\n",
      "238 391 Loss: 1.581 | Acc: 41.668% (12747/30592)\n",
      "239 391 Loss: 1.581 | Acc: 41.683% (12805/30720)\n",
      "240 391 Loss: 1.582 | Acc: 41.672% (12855/30848)\n",
      "241 391 Loss: 1.582 | Acc: 41.684% (12912/30976)\n",
      "242 391 Loss: 1.582 | Acc: 41.660% (12958/31104)\n",
      "243 391 Loss: 1.582 | Acc: 41.659% (13011/31232)\n",
      "244 391 Loss: 1.582 | Acc: 41.680% (13071/31360)\n",
      "245 391 Loss: 1.582 | Acc: 41.689% (13127/31488)\n",
      "246 391 Loss: 1.582 | Acc: 41.678% (13177/31616)\n",
      "247 391 Loss: 1.582 | Acc: 41.718% (13243/31744)\n",
      "248 391 Loss: 1.582 | Acc: 41.729% (13300/31872)\n",
      "249 391 Loss: 1.582 | Acc: 41.725% (13352/32000)\n",
      "250 391 Loss: 1.582 | Acc: 41.699% (13397/32128)\n",
      "251 391 Loss: 1.582 | Acc: 41.698% (13450/32256)\n",
      "252 391 Loss: 1.582 | Acc: 41.706% (13506/32384)\n",
      "253 391 Loss: 1.581 | Acc: 41.726% (13566/32512)\n",
      "254 391 Loss: 1.581 | Acc: 41.768% (13633/32640)\n",
      "255 391 Loss: 1.581 | Acc: 41.760% (13684/32768)\n",
      "256 391 Loss: 1.581 | Acc: 41.741% (13731/32896)\n",
      "257 391 Loss: 1.581 | Acc: 41.724% (13779/33024)\n",
      "258 391 Loss: 1.581 | Acc: 41.717% (13830/33152)\n",
      "259 391 Loss: 1.581 | Acc: 41.758% (13897/33280)\n",
      "260 391 Loss: 1.581 | Acc: 41.739% (13944/33408)\n",
      "261 391 Loss: 1.581 | Acc: 41.719% (13991/33536)\n",
      "262 391 Loss: 1.581 | Acc: 41.697% (14037/33664)\n",
      "263 391 Loss: 1.581 | Acc: 41.699% (14091/33792)\n",
      "264 391 Loss: 1.581 | Acc: 41.716% (14150/33920)\n",
      "265 391 Loss: 1.581 | Acc: 41.679% (14191/34048)\n",
      "266 391 Loss: 1.581 | Acc: 41.673% (14242/34176)\n",
      "267 391 Loss: 1.581 | Acc: 41.666% (14293/34304)\n",
      "268 391 Loss: 1.580 | Acc: 41.671% (14348/34432)\n",
      "269 391 Loss: 1.580 | Acc: 41.678% (14404/34560)\n",
      "270 391 Loss: 1.580 | Acc: 41.709% (14468/34688)\n",
      "271 391 Loss: 1.579 | Acc: 41.739% (14532/34816)\n",
      "272 391 Loss: 1.579 | Acc: 41.741% (14586/34944)\n",
      "273 391 Loss: 1.579 | Acc: 41.774% (14651/35072)\n",
      "274 391 Loss: 1.579 | Acc: 41.759% (14699/35200)\n",
      "275 391 Loss: 1.580 | Acc: 41.749% (14749/35328)\n",
      "276 391 Loss: 1.579 | Acc: 41.764% (14808/35456)\n",
      "277 391 Loss: 1.579 | Acc: 41.752% (14857/35584)\n",
      "278 391 Loss: 1.579 | Acc: 41.742% (14907/35712)\n",
      "279 391 Loss: 1.579 | Acc: 41.747% (14962/35840)\n",
      "280 391 Loss: 1.579 | Acc: 41.754% (15018/35968)\n",
      "281 391 Loss: 1.578 | Acc: 41.747% (15069/36096)\n",
      "282 391 Loss: 1.578 | Acc: 41.743% (15121/36224)\n",
      "283 391 Loss: 1.578 | Acc: 41.742% (15174/36352)\n",
      "284 391 Loss: 1.578 | Acc: 41.754% (15232/36480)\n",
      "285 391 Loss: 1.577 | Acc: 41.770% (15291/36608)\n",
      "286 391 Loss: 1.578 | Acc: 41.785% (15350/36736)\n",
      "287 391 Loss: 1.577 | Acc: 41.800% (15409/36864)\n",
      "288 391 Loss: 1.577 | Acc: 41.801% (15463/36992)\n",
      "289 391 Loss: 1.577 | Acc: 41.791% (15513/37120)\n",
      "290 391 Loss: 1.577 | Acc: 41.793% (15567/37248)\n",
      "291 391 Loss: 1.577 | Acc: 41.792% (15620/37376)\n",
      "292 391 Loss: 1.577 | Acc: 41.785% (15671/37504)\n",
      "293 391 Loss: 1.577 | Acc: 41.770% (15719/37632)\n",
      "294 391 Loss: 1.578 | Acc: 41.753% (15766/37760)\n",
      "295 391 Loss: 1.578 | Acc: 41.726% (15809/37888)\n",
      "296 391 Loss: 1.577 | Acc: 41.738% (15867/38016)\n",
      "297 391 Loss: 1.578 | Acc: 41.721% (15914/38144)\n",
      "298 391 Loss: 1.578 | Acc: 41.735% (15973/38272)\n",
      "299 391 Loss: 1.577 | Acc: 41.763% (16037/38400)\n",
      "300 391 Loss: 1.577 | Acc: 41.777% (16096/38528)\n",
      "301 391 Loss: 1.577 | Acc: 41.784% (16152/38656)\n",
      "302 391 Loss: 1.576 | Acc: 41.821% (16220/38784)\n",
      "303 391 Loss: 1.577 | Acc: 41.812% (16270/38912)\n",
      "304 391 Loss: 1.577 | Acc: 41.808% (16322/39040)\n",
      "305 391 Loss: 1.577 | Acc: 41.822% (16381/39168)\n",
      "306 391 Loss: 1.576 | Acc: 41.829% (16437/39296)\n",
      "307 391 Loss: 1.576 | Acc: 41.837% (16494/39424)\n",
      "308 391 Loss: 1.576 | Acc: 41.831% (16545/39552)\n",
      "309 391 Loss: 1.576 | Acc: 41.840% (16602/39680)\n",
      "310 391 Loss: 1.576 | Acc: 41.858% (16663/39808)\n",
      "311 391 Loss: 1.576 | Acc: 41.877% (16724/39936)\n",
      "312 391 Loss: 1.575 | Acc: 41.873% (16776/40064)\n",
      "313 391 Loss: 1.575 | Acc: 41.867% (16827/40192)\n",
      "314 391 Loss: 1.575 | Acc: 41.865% (16880/40320)\n",
      "315 391 Loss: 1.575 | Acc: 41.874% (16937/40448)\n",
      "316 391 Loss: 1.575 | Acc: 41.867% (16988/40576)\n",
      "317 391 Loss: 1.574 | Acc: 41.878% (17046/40704)\n",
      "318 391 Loss: 1.574 | Acc: 41.923% (17118/40832)\n",
      "319 391 Loss: 1.574 | Acc: 41.926% (17173/40960)\n",
      "320 391 Loss: 1.573 | Acc: 41.925% (17226/41088)\n",
      "321 391 Loss: 1.573 | Acc: 41.935% (17284/41216)\n",
      "322 391 Loss: 1.573 | Acc: 41.924% (17333/41344)\n",
      "323 391 Loss: 1.573 | Acc: 41.937% (17392/41472)\n",
      "324 391 Loss: 1.573 | Acc: 41.925% (17441/41600)\n",
      "325 391 Loss: 1.573 | Acc: 41.929% (17496/41728)\n",
      "326 391 Loss: 1.573 | Acc: 41.927% (17549/41856)\n",
      "327 391 Loss: 1.573 | Acc: 41.928% (17603/41984)\n",
      "328 391 Loss: 1.573 | Acc: 41.905% (17647/42112)\n",
      "329 391 Loss: 1.573 | Acc: 41.894% (17696/42240)\n",
      "330 391 Loss: 1.573 | Acc: 41.909% (17756/42368)\n",
      "331 391 Loss: 1.572 | Acc: 41.905% (17808/42496)\n",
      "332 391 Loss: 1.572 | Acc: 41.899% (17859/42624)\n",
      "333 391 Loss: 1.572 | Acc: 41.902% (17914/42752)\n",
      "334 391 Loss: 1.572 | Acc: 41.896% (17965/42880)\n",
      "335 391 Loss: 1.573 | Acc: 41.888% (18015/43008)\n",
      "336 391 Loss: 1.572 | Acc: 41.907% (18077/43136)\n",
      "337 391 Loss: 1.572 | Acc: 41.903% (18129/43264)\n",
      "338 391 Loss: 1.572 | Acc: 41.904% (18183/43392)\n",
      "339 391 Loss: 1.572 | Acc: 41.926% (18246/43520)\n",
      "340 391 Loss: 1.571 | Acc: 41.942% (18307/43648)\n",
      "341 391 Loss: 1.571 | Acc: 41.923% (18352/43776)\n",
      "342 391 Loss: 1.571 | Acc: 41.926% (18407/43904)\n",
      "343 391 Loss: 1.571 | Acc: 41.954% (18473/44032)\n",
      "344 391 Loss: 1.571 | Acc: 41.947% (18524/44160)\n",
      "345 391 Loss: 1.571 | Acc: 41.948% (18578/44288)\n",
      "346 391 Loss: 1.571 | Acc: 41.958% (18636/44416)\n",
      "347 391 Loss: 1.571 | Acc: 41.970% (18695/44544)\n",
      "348 391 Loss: 1.570 | Acc: 41.979% (18753/44672)\n",
      "349 391 Loss: 1.570 | Acc: 42.009% (18820/44800)\n",
      "350 391 Loss: 1.570 | Acc: 42.014% (18876/44928)\n",
      "351 391 Loss: 1.570 | Acc: 42.014% (18930/45056)\n",
      "352 391 Loss: 1.570 | Acc: 42.015% (18984/45184)\n",
      "353 391 Loss: 1.570 | Acc: 42.029% (19044/45312)\n",
      "354 391 Loss: 1.570 | Acc: 42.036% (19101/45440)\n",
      "355 391 Loss: 1.570 | Acc: 42.056% (19164/45568)\n",
      "356 391 Loss: 1.570 | Acc: 42.052% (19216/45696)\n",
      "357 391 Loss: 1.570 | Acc: 42.054% (19271/45824)\n",
      "358 391 Loss: 1.570 | Acc: 42.046% (19321/45952)\n",
      "359 391 Loss: 1.570 | Acc: 42.062% (19382/46080)\n",
      "360 391 Loss: 1.570 | Acc: 42.051% (19431/46208)\n",
      "361 391 Loss: 1.570 | Acc: 42.062% (19490/46336)\n",
      "362 391 Loss: 1.570 | Acc: 42.065% (19545/46464)\n",
      "363 391 Loss: 1.570 | Acc: 42.067% (19600/46592)\n",
      "364 391 Loss: 1.570 | Acc: 42.059% (19650/46720)\n",
      "365 391 Loss: 1.570 | Acc: 42.049% (19699/46848)\n",
      "366 391 Loss: 1.570 | Acc: 42.051% (19754/46976)\n",
      "367 391 Loss: 1.569 | Acc: 42.077% (19820/47104)\n",
      "368 391 Loss: 1.569 | Acc: 42.101% (19885/47232)\n",
      "369 391 Loss: 1.569 | Acc: 42.109% (19943/47360)\n",
      "370 391 Loss: 1.568 | Acc: 42.141% (20012/47488)\n",
      "371 391 Loss: 1.568 | Acc: 42.158% (20074/47616)\n",
      "372 391 Loss: 1.568 | Acc: 42.185% (20141/47744)\n",
      "373 391 Loss: 1.568 | Acc: 42.200% (20202/47872)\n",
      "374 391 Loss: 1.568 | Acc: 42.212% (20262/48000)\n",
      "375 391 Loss: 1.567 | Acc: 42.221% (20320/48128)\n",
      "376 391 Loss: 1.567 | Acc: 42.217% (20372/48256)\n",
      "377 391 Loss: 1.567 | Acc: 42.214% (20425/48384)\n",
      "378 391 Loss: 1.567 | Acc: 42.220% (20482/48512)\n",
      "379 391 Loss: 1.567 | Acc: 42.229% (20540/48640)\n",
      "380 391 Loss: 1.567 | Acc: 42.224% (20592/48768)\n",
      "381 391 Loss: 1.566 | Acc: 42.249% (20658/48896)\n",
      "382 391 Loss: 1.566 | Acc: 42.269% (20722/49024)\n",
      "383 391 Loss: 1.566 | Acc: 42.279% (20781/49152)\n",
      "384 391 Loss: 1.566 | Acc: 42.291% (20841/49280)\n",
      "385 391 Loss: 1.566 | Acc: 42.275% (20887/49408)\n",
      "386 391 Loss: 1.566 | Acc: 42.276% (20942/49536)\n",
      "387 391 Loss: 1.565 | Acc: 42.292% (21004/49664)\n",
      "388 391 Loss: 1.566 | Acc: 42.300% (21062/49792)\n",
      "389 391 Loss: 1.566 | Acc: 42.298% (21115/49920)\n",
      "390 391 Loss: 1.565 | Acc: 42.304% (21152/50000)\n",
      "0 100 Loss: 1.420 | Acc: 49.000% (49/100)\n",
      "1 100 Loss: 1.453 | Acc: 47.500% (95/200)\n",
      "2 100 Loss: 1.464 | Acc: 46.667% (140/300)\n",
      "3 100 Loss: 1.494 | Acc: 45.500% (182/400)\n",
      "4 100 Loss: 1.499 | Acc: 45.000% (225/500)\n",
      "5 100 Loss: 1.500 | Acc: 44.333% (266/600)\n",
      "6 100 Loss: 1.512 | Acc: 44.000% (308/700)\n",
      "7 100 Loss: 1.515 | Acc: 44.500% (356/800)\n",
      "8 100 Loss: 1.512 | Acc: 44.889% (404/900)\n",
      "9 100 Loss: 1.510 | Acc: 45.100% (451/1000)\n",
      "10 100 Loss: 1.500 | Acc: 45.455% (500/1100)\n",
      "11 100 Loss: 1.496 | Acc: 44.667% (536/1200)\n",
      "12 100 Loss: 1.507 | Acc: 44.077% (573/1300)\n",
      "13 100 Loss: 1.509 | Acc: 43.857% (614/1400)\n",
      "14 100 Loss: 1.498 | Acc: 44.000% (660/1500)\n",
      "15 100 Loss: 1.499 | Acc: 44.000% (704/1600)\n",
      "16 100 Loss: 1.496 | Acc: 44.353% (754/1700)\n",
      "17 100 Loss: 1.486 | Acc: 44.778% (806/1800)\n",
      "18 100 Loss: 1.485 | Acc: 45.053% (856/1900)\n",
      "19 100 Loss: 1.486 | Acc: 44.850% (897/2000)\n",
      "20 100 Loss: 1.484 | Acc: 45.095% (947/2100)\n",
      "21 100 Loss: 1.482 | Acc: 45.136% (993/2200)\n",
      "22 100 Loss: 1.478 | Acc: 45.261% (1041/2300)\n",
      "23 100 Loss: 1.478 | Acc: 45.292% (1087/2400)\n",
      "24 100 Loss: 1.477 | Acc: 45.400% (1135/2500)\n",
      "25 100 Loss: 1.488 | Acc: 44.885% (1167/2600)\n",
      "26 100 Loss: 1.489 | Acc: 44.926% (1213/2700)\n",
      "27 100 Loss: 1.491 | Acc: 44.750% (1253/2800)\n",
      "28 100 Loss: 1.496 | Acc: 44.483% (1290/2900)\n",
      "29 100 Loss: 1.493 | Acc: 44.667% (1340/3000)\n",
      "30 100 Loss: 1.489 | Acc: 44.871% (1391/3100)\n",
      "31 100 Loss: 1.486 | Acc: 44.969% (1439/3200)\n",
      "32 100 Loss: 1.485 | Acc: 45.061% (1487/3300)\n",
      "33 100 Loss: 1.484 | Acc: 45.029% (1531/3400)\n",
      "34 100 Loss: 1.483 | Acc: 45.114% (1579/3500)\n",
      "35 100 Loss: 1.482 | Acc: 45.111% (1624/3600)\n",
      "36 100 Loss: 1.482 | Acc: 45.189% (1672/3700)\n",
      "37 100 Loss: 1.482 | Acc: 45.211% (1718/3800)\n",
      "38 100 Loss: 1.482 | Acc: 45.128% (1760/3900)\n",
      "39 100 Loss: 1.483 | Acc: 45.275% (1811/4000)\n",
      "40 100 Loss: 1.485 | Acc: 45.293% (1857/4100)\n",
      "41 100 Loss: 1.486 | Acc: 45.167% (1897/4200)\n",
      "42 100 Loss: 1.484 | Acc: 45.186% (1943/4300)\n",
      "43 100 Loss: 1.481 | Acc: 45.455% (2000/4400)\n",
      "44 100 Loss: 1.480 | Acc: 45.556% (2050/4500)\n",
      "45 100 Loss: 1.478 | Acc: 45.565% (2096/4600)\n",
      "46 100 Loss: 1.476 | Acc: 45.638% (2145/4700)\n",
      "47 100 Loss: 1.478 | Acc: 45.604% (2189/4800)\n",
      "48 100 Loss: 1.476 | Acc: 45.633% (2236/4900)\n",
      "49 100 Loss: 1.474 | Acc: 45.660% (2283/5000)\n",
      "50 100 Loss: 1.474 | Acc: 45.549% (2323/5100)\n",
      "51 100 Loss: 1.474 | Acc: 45.558% (2369/5200)\n",
      "52 100 Loss: 1.473 | Acc: 45.528% (2413/5300)\n",
      "53 100 Loss: 1.475 | Acc: 45.407% (2452/5400)\n",
      "54 100 Loss: 1.478 | Acc: 45.127% (2482/5500)\n",
      "55 100 Loss: 1.482 | Acc: 44.911% (2515/5600)\n",
      "56 100 Loss: 1.483 | Acc: 44.947% (2562/5700)\n",
      "57 100 Loss: 1.479 | Acc: 45.155% (2619/5800)\n",
      "58 100 Loss: 1.479 | Acc: 45.169% (2665/5900)\n",
      "59 100 Loss: 1.478 | Acc: 45.217% (2713/6000)\n",
      "60 100 Loss: 1.479 | Acc: 45.246% (2760/6100)\n",
      "61 100 Loss: 1.479 | Acc: 45.210% (2803/6200)\n",
      "62 100 Loss: 1.479 | Acc: 45.286% (2853/6300)\n",
      "63 100 Loss: 1.479 | Acc: 45.328% (2901/6400)\n",
      "64 100 Loss: 1.480 | Acc: 45.231% (2940/6500)\n",
      "65 100 Loss: 1.480 | Acc: 45.227% (2985/6600)\n",
      "66 100 Loss: 1.481 | Acc: 45.194% (3028/6700)\n",
      "67 100 Loss: 1.481 | Acc: 45.162% (3071/6800)\n",
      "68 100 Loss: 1.482 | Acc: 45.101% (3112/6900)\n",
      "69 100 Loss: 1.483 | Acc: 45.043% (3153/7000)\n",
      "70 100 Loss: 1.483 | Acc: 44.958% (3192/7100)\n",
      "71 100 Loss: 1.483 | Acc: 45.000% (3240/7200)\n",
      "72 100 Loss: 1.482 | Acc: 45.110% (3293/7300)\n",
      "73 100 Loss: 1.480 | Acc: 45.135% (3340/7400)\n",
      "74 100 Loss: 1.480 | Acc: 45.053% (3379/7500)\n",
      "75 100 Loss: 1.480 | Acc: 45.105% (3428/7600)\n",
      "76 100 Loss: 1.479 | Acc: 45.091% (3472/7700)\n",
      "77 100 Loss: 1.478 | Acc: 45.218% (3527/7800)\n",
      "78 100 Loss: 1.476 | Acc: 45.304% (3579/7900)\n",
      "79 100 Loss: 1.478 | Acc: 45.237% (3619/8000)\n",
      "80 100 Loss: 1.478 | Acc: 45.284% (3668/8100)\n",
      "81 100 Loss: 1.480 | Acc: 45.195% (3706/8200)\n",
      "82 100 Loss: 1.481 | Acc: 45.096% (3743/8300)\n",
      "83 100 Loss: 1.482 | Acc: 45.036% (3783/8400)\n",
      "84 100 Loss: 1.483 | Acc: 45.012% (3826/8500)\n",
      "85 100 Loss: 1.483 | Acc: 45.070% (3876/8600)\n",
      "86 100 Loss: 1.486 | Acc: 44.989% (3914/8700)\n",
      "87 100 Loss: 1.485 | Acc: 44.977% (3958/8800)\n",
      "88 100 Loss: 1.485 | Acc: 45.011% (4006/8900)\n",
      "89 100 Loss: 1.486 | Acc: 45.000% (4050/9000)\n",
      "90 100 Loss: 1.486 | Acc: 44.934% (4089/9100)\n",
      "91 100 Loss: 1.486 | Acc: 45.000% (4140/9200)\n",
      "92 100 Loss: 1.484 | Acc: 45.065% (4191/9300)\n",
      "93 100 Loss: 1.485 | Acc: 45.064% (4236/9400)\n",
      "94 100 Loss: 1.486 | Acc: 45.053% (4280/9500)\n",
      "95 100 Loss: 1.486 | Acc: 45.052% (4325/9600)\n",
      "96 100 Loss: 1.485 | Acc: 45.082% (4373/9700)\n",
      "97 100 Loss: 1.486 | Acc: 45.051% (4415/9800)\n",
      "98 100 Loss: 1.486 | Acc: 45.020% (4457/9900)\n",
      "99 100 Loss: 1.486 | Acc: 45.000% (4500/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      "0 391 Loss: 1.446 | Acc: 42.969% (55/128)\n",
      "1 391 Loss: 1.501 | Acc: 44.922% (115/256)\n",
      "2 391 Loss: 1.500 | Acc: 47.396% (182/384)\n",
      "3 391 Loss: 1.510 | Acc: 45.898% (235/512)\n",
      "4 391 Loss: 1.510 | Acc: 44.844% (287/640)\n",
      "5 391 Loss: 1.514 | Acc: 45.443% (349/768)\n",
      "6 391 Loss: 1.516 | Acc: 44.866% (402/896)\n",
      "7 391 Loss: 1.528 | Acc: 44.727% (458/1024)\n",
      "8 391 Loss: 1.518 | Acc: 45.052% (519/1152)\n",
      "9 391 Loss: 1.519 | Acc: 45.234% (579/1280)\n",
      "10 391 Loss: 1.527 | Acc: 45.170% (636/1408)\n",
      "11 391 Loss: 1.527 | Acc: 45.117% (693/1536)\n",
      "12 391 Loss: 1.518 | Acc: 45.493% (757/1664)\n",
      "13 391 Loss: 1.522 | Acc: 45.033% (807/1792)\n",
      "14 391 Loss: 1.520 | Acc: 45.104% (866/1920)\n",
      "15 391 Loss: 1.520 | Acc: 45.215% (926/2048)\n",
      "16 391 Loss: 1.512 | Acc: 45.450% (989/2176)\n",
      "17 391 Loss: 1.516 | Acc: 45.269% (1043/2304)\n",
      "18 391 Loss: 1.511 | Acc: 45.436% (1105/2432)\n",
      "19 391 Loss: 1.519 | Acc: 44.922% (1150/2560)\n",
      "20 391 Loss: 1.519 | Acc: 44.978% (1209/2688)\n",
      "21 391 Loss: 1.524 | Acc: 44.425% (1251/2816)\n",
      "22 391 Loss: 1.525 | Acc: 44.463% (1309/2944)\n",
      "23 391 Loss: 1.525 | Acc: 44.466% (1366/3072)\n",
      "24 391 Loss: 1.528 | Acc: 44.188% (1414/3200)\n",
      "25 391 Loss: 1.532 | Acc: 44.050% (1466/3328)\n",
      "26 391 Loss: 1.531 | Acc: 44.039% (1522/3456)\n",
      "27 391 Loss: 1.535 | Acc: 43.917% (1574/3584)\n",
      "28 391 Loss: 1.531 | Acc: 43.912% (1630/3712)\n",
      "29 391 Loss: 1.528 | Acc: 44.089% (1693/3840)\n",
      "30 391 Loss: 1.525 | Acc: 44.279% (1757/3968)\n",
      "31 391 Loss: 1.524 | Acc: 44.336% (1816/4096)\n",
      "32 391 Loss: 1.521 | Acc: 44.389% (1875/4224)\n",
      "33 391 Loss: 1.519 | Acc: 44.554% (1939/4352)\n",
      "34 391 Loss: 1.516 | Acc: 44.643% (2000/4480)\n",
      "35 391 Loss: 1.515 | Acc: 44.705% (2060/4608)\n",
      "36 391 Loss: 1.516 | Acc: 44.510% (2108/4736)\n",
      "37 391 Loss: 1.518 | Acc: 44.387% (2159/4864)\n",
      "38 391 Loss: 1.518 | Acc: 44.331% (2213/4992)\n",
      "39 391 Loss: 1.517 | Acc: 44.219% (2264/5120)\n",
      "40 391 Loss: 1.516 | Acc: 44.284% (2324/5248)\n",
      "41 391 Loss: 1.514 | Acc: 44.327% (2383/5376)\n",
      "42 391 Loss: 1.516 | Acc: 44.350% (2441/5504)\n",
      "43 391 Loss: 1.517 | Acc: 44.212% (2490/5632)\n",
      "44 391 Loss: 1.516 | Acc: 44.184% (2545/5760)\n",
      "45 391 Loss: 1.519 | Acc: 44.073% (2595/5888)\n",
      "46 391 Loss: 1.523 | Acc: 43.717% (2630/6016)\n",
      "47 391 Loss: 1.522 | Acc: 43.766% (2689/6144)\n",
      "48 391 Loss: 1.526 | Acc: 43.607% (2735/6272)\n",
      "49 391 Loss: 1.523 | Acc: 43.719% (2798/6400)\n",
      "50 391 Loss: 1.523 | Acc: 43.735% (2855/6528)\n",
      "51 391 Loss: 1.524 | Acc: 43.720% (2910/6656)\n",
      "52 391 Loss: 1.524 | Acc: 43.691% (2964/6784)\n",
      "53 391 Loss: 1.521 | Acc: 43.837% (3030/6912)\n",
      "54 391 Loss: 1.522 | Acc: 43.778% (3082/7040)\n",
      "55 391 Loss: 1.522 | Acc: 43.848% (3143/7168)\n",
      "56 391 Loss: 1.520 | Acc: 43.914% (3204/7296)\n",
      "57 391 Loss: 1.522 | Acc: 43.763% (3249/7424)\n",
      "58 391 Loss: 1.521 | Acc: 43.776% (3306/7552)\n",
      "59 391 Loss: 1.524 | Acc: 43.633% (3351/7680)\n",
      "60 391 Loss: 1.524 | Acc: 43.519% (3398/7808)\n",
      "61 391 Loss: 1.523 | Acc: 43.574% (3458/7936)\n",
      "62 391 Loss: 1.523 | Acc: 43.626% (3518/8064)\n",
      "63 391 Loss: 1.522 | Acc: 43.738% (3583/8192)\n",
      "64 391 Loss: 1.523 | Acc: 43.714% (3637/8320)\n",
      "65 391 Loss: 1.522 | Acc: 43.786% (3699/8448)\n",
      "66 391 Loss: 1.520 | Acc: 43.773% (3754/8576)\n",
      "67 391 Loss: 1.520 | Acc: 43.796% (3812/8704)\n",
      "68 391 Loss: 1.519 | Acc: 43.829% (3871/8832)\n",
      "69 391 Loss: 1.517 | Acc: 43.862% (3930/8960)\n",
      "70 391 Loss: 1.518 | Acc: 43.805% (3981/9088)\n",
      "71 391 Loss: 1.519 | Acc: 43.750% (4032/9216)\n",
      "72 391 Loss: 1.517 | Acc: 43.921% (4104/9344)\n",
      "73 391 Loss: 1.516 | Acc: 43.982% (4166/9472)\n",
      "74 391 Loss: 1.515 | Acc: 44.031% (4227/9600)\n",
      "75 391 Loss: 1.516 | Acc: 43.945% (4275/9728)\n",
      "76 391 Loss: 1.516 | Acc: 43.963% (4333/9856)\n",
      "77 391 Loss: 1.517 | Acc: 43.900% (4383/9984)\n",
      "78 391 Loss: 1.516 | Acc: 43.888% (4438/10112)\n",
      "79 391 Loss: 1.515 | Acc: 43.975% (4503/10240)\n",
      "80 391 Loss: 1.514 | Acc: 44.020% (4564/10368)\n",
      "81 391 Loss: 1.516 | Acc: 43.912% (4609/10496)\n",
      "82 391 Loss: 1.517 | Acc: 43.938% (4668/10624)\n",
      "83 391 Loss: 1.515 | Acc: 44.076% (4739/10752)\n",
      "84 391 Loss: 1.516 | Acc: 44.044% (4792/10880)\n",
      "85 391 Loss: 1.517 | Acc: 43.986% (4842/11008)\n",
      "86 391 Loss: 1.518 | Acc: 43.921% (4891/11136)\n",
      "87 391 Loss: 1.518 | Acc: 43.945% (4950/11264)\n",
      "88 391 Loss: 1.516 | Acc: 43.917% (5003/11392)\n",
      "89 391 Loss: 1.516 | Acc: 43.889% (5056/11520)\n",
      "90 391 Loss: 1.517 | Acc: 43.862% (5109/11648)\n",
      "91 391 Loss: 1.518 | Acc: 43.826% (5161/11776)\n",
      "92 391 Loss: 1.520 | Acc: 43.767% (5210/11904)\n",
      "93 391 Loss: 1.521 | Acc: 43.775% (5267/12032)\n",
      "94 391 Loss: 1.521 | Acc: 43.775% (5323/12160)\n",
      "95 391 Loss: 1.521 | Acc: 43.815% (5384/12288)\n",
      "96 391 Loss: 1.522 | Acc: 43.750% (5432/12416)\n",
      "97 391 Loss: 1.523 | Acc: 43.750% (5488/12544)\n",
      "98 391 Loss: 1.523 | Acc: 43.687% (5536/12672)\n",
      "99 391 Loss: 1.522 | Acc: 43.766% (5602/12800)\n",
      "100 391 Loss: 1.522 | Acc: 43.735% (5654/12928)\n",
      "101 391 Loss: 1.522 | Acc: 43.765% (5714/13056)\n",
      "102 391 Loss: 1.522 | Acc: 43.788% (5773/13184)\n",
      "103 391 Loss: 1.522 | Acc: 43.750% (5824/13312)\n",
      "104 391 Loss: 1.522 | Acc: 43.750% (5880/13440)\n",
      "105 391 Loss: 1.521 | Acc: 43.787% (5941/13568)\n",
      "106 391 Loss: 1.522 | Acc: 43.750% (5992/13696)\n",
      "107 391 Loss: 1.521 | Acc: 43.793% (6054/13824)\n",
      "108 391 Loss: 1.520 | Acc: 43.836% (6116/13952)\n",
      "109 391 Loss: 1.521 | Acc: 43.793% (6166/14080)\n",
      "110 391 Loss: 1.521 | Acc: 43.778% (6220/14208)\n",
      "111 391 Loss: 1.520 | Acc: 43.764% (6274/14336)\n",
      "112 391 Loss: 1.521 | Acc: 43.729% (6325/14464)\n",
      "113 391 Loss: 1.521 | Acc: 43.709% (6378/14592)\n",
      "114 391 Loss: 1.521 | Acc: 43.702% (6433/14720)\n",
      "115 391 Loss: 1.521 | Acc: 43.750% (6496/14848)\n",
      "116 391 Loss: 1.520 | Acc: 43.777% (6556/14976)\n",
      "117 391 Loss: 1.519 | Acc: 43.796% (6615/15104)\n",
      "118 391 Loss: 1.519 | Acc: 43.796% (6671/15232)\n",
      "119 391 Loss: 1.519 | Acc: 43.802% (6728/15360)\n",
      "120 391 Loss: 1.518 | Acc: 43.853% (6792/15488)\n",
      "121 391 Loss: 1.517 | Acc: 43.872% (6851/15616)\n",
      "122 391 Loss: 1.518 | Acc: 43.852% (6904/15744)\n",
      "123 391 Loss: 1.517 | Acc: 43.901% (6968/15872)\n",
      "124 391 Loss: 1.517 | Acc: 43.919% (7027/16000)\n",
      "125 391 Loss: 1.517 | Acc: 43.942% (7087/16128)\n",
      "126 391 Loss: 1.516 | Acc: 43.971% (7148/16256)\n",
      "127 391 Loss: 1.516 | Acc: 44.043% (7216/16384)\n",
      "128 391 Loss: 1.515 | Acc: 44.047% (7273/16512)\n",
      "129 391 Loss: 1.516 | Acc: 44.020% (7325/16640)\n",
      "130 391 Loss: 1.515 | Acc: 44.024% (7382/16768)\n",
      "131 391 Loss: 1.515 | Acc: 44.004% (7435/16896)\n",
      "132 391 Loss: 1.515 | Acc: 44.055% (7500/17024)\n",
      "133 391 Loss: 1.514 | Acc: 44.100% (7564/17152)\n",
      "134 391 Loss: 1.515 | Acc: 44.097% (7620/17280)\n",
      "135 391 Loss: 1.514 | Acc: 44.100% (7677/17408)\n",
      "136 391 Loss: 1.514 | Acc: 44.086% (7731/17536)\n",
      "137 391 Loss: 1.514 | Acc: 44.061% (7783/17664)\n",
      "138 391 Loss: 1.515 | Acc: 44.054% (7838/17792)\n",
      "139 391 Loss: 1.514 | Acc: 44.096% (7902/17920)\n",
      "140 391 Loss: 1.513 | Acc: 44.116% (7962/18048)\n",
      "141 391 Loss: 1.513 | Acc: 44.135% (8022/18176)\n",
      "142 391 Loss: 1.513 | Acc: 44.138% (8079/18304)\n",
      "143 391 Loss: 1.513 | Acc: 44.124% (8133/18432)\n",
      "144 391 Loss: 1.512 | Acc: 44.149% (8194/18560)\n",
      "145 391 Loss: 1.512 | Acc: 44.141% (8249/18688)\n",
      "146 391 Loss: 1.511 | Acc: 44.175% (8312/18816)\n",
      "147 391 Loss: 1.512 | Acc: 44.188% (8371/18944)\n",
      "148 391 Loss: 1.512 | Acc: 44.227% (8435/19072)\n",
      "149 391 Loss: 1.511 | Acc: 44.276% (8501/19200)\n",
      "150 391 Loss: 1.511 | Acc: 44.319% (8566/19328)\n",
      "151 391 Loss: 1.510 | Acc: 44.326% (8624/19456)\n",
      "152 391 Loss: 1.509 | Acc: 44.393% (8694/19584)\n",
      "153 391 Loss: 1.509 | Acc: 44.359% (8744/19712)\n",
      "154 391 Loss: 1.509 | Acc: 44.355% (8800/19840)\n",
      "155 391 Loss: 1.509 | Acc: 44.351% (8856/19968)\n",
      "156 391 Loss: 1.509 | Acc: 44.347% (8912/20096)\n",
      "157 391 Loss: 1.509 | Acc: 44.324% (8964/20224)\n",
      "158 391 Loss: 1.508 | Acc: 44.359% (9028/20352)\n",
      "159 391 Loss: 1.508 | Acc: 44.375% (9088/20480)\n",
      "160 391 Loss: 1.508 | Acc: 44.337% (9137/20608)\n",
      "161 391 Loss: 1.508 | Acc: 44.290% (9184/20736)\n",
      "162 391 Loss: 1.508 | Acc: 44.292% (9241/20864)\n",
      "163 391 Loss: 1.508 | Acc: 44.298% (9299/20992)\n",
      "164 391 Loss: 1.508 | Acc: 44.304% (9357/21120)\n",
      "165 391 Loss: 1.508 | Acc: 44.296% (9412/21248)\n",
      "166 391 Loss: 1.508 | Acc: 44.297% (9469/21376)\n",
      "167 391 Loss: 1.508 | Acc: 44.294% (9525/21504)\n",
      "168 391 Loss: 1.507 | Acc: 44.309% (9585/21632)\n",
      "169 391 Loss: 1.508 | Acc: 44.311% (9642/21760)\n",
      "170 391 Loss: 1.508 | Acc: 44.280% (9692/21888)\n",
      "171 391 Loss: 1.509 | Acc: 44.291% (9751/22016)\n",
      "172 391 Loss: 1.509 | Acc: 44.305% (9811/22144)\n",
      "173 391 Loss: 1.508 | Acc: 44.361% (9880/22272)\n",
      "174 391 Loss: 1.509 | Acc: 44.366% (9938/22400)\n",
      "175 391 Loss: 1.508 | Acc: 44.367% (9995/22528)\n",
      "176 391 Loss: 1.508 | Acc: 44.381% (10055/22656)\n",
      "177 391 Loss: 1.507 | Acc: 44.391% (10114/22784)\n",
      "178 391 Loss: 1.507 | Acc: 44.405% (10174/22912)\n",
      "179 391 Loss: 1.507 | Acc: 44.375% (10224/23040)\n",
      "180 391 Loss: 1.507 | Acc: 44.367% (10279/23168)\n",
      "181 391 Loss: 1.507 | Acc: 44.351% (10332/23296)\n",
      "182 391 Loss: 1.507 | Acc: 44.331% (10384/23424)\n",
      "183 391 Loss: 1.507 | Acc: 44.344% (10444/23552)\n",
      "184 391 Loss: 1.507 | Acc: 44.337% (10499/23680)\n",
      "185 391 Loss: 1.507 | Acc: 44.372% (10564/23808)\n",
      "186 391 Loss: 1.507 | Acc: 44.368% (10620/23936)\n",
      "187 391 Loss: 1.508 | Acc: 44.340% (10670/24064)\n",
      "188 391 Loss: 1.507 | Acc: 44.362% (10732/24192)\n",
      "189 391 Loss: 1.507 | Acc: 44.383% (10794/24320)\n",
      "190 391 Loss: 1.507 | Acc: 44.404% (10856/24448)\n",
      "191 391 Loss: 1.507 | Acc: 44.397% (10911/24576)\n",
      "192 391 Loss: 1.507 | Acc: 44.430% (10976/24704)\n",
      "193 391 Loss: 1.506 | Acc: 44.414% (11029/24832)\n",
      "194 391 Loss: 1.506 | Acc: 44.407% (11084/24960)\n",
      "195 391 Loss: 1.507 | Acc: 44.404% (11140/25088)\n",
      "196 391 Loss: 1.507 | Acc: 44.412% (11199/25216)\n",
      "197 391 Loss: 1.507 | Acc: 44.389% (11250/25344)\n",
      "198 391 Loss: 1.506 | Acc: 44.406% (11311/25472)\n",
      "199 391 Loss: 1.506 | Acc: 44.414% (11370/25600)\n",
      "200 391 Loss: 1.506 | Acc: 44.387% (11420/25728)\n",
      "201 391 Loss: 1.506 | Acc: 44.377% (11474/25856)\n",
      "202 391 Loss: 1.506 | Acc: 44.393% (11535/25984)\n",
      "203 391 Loss: 1.506 | Acc: 44.401% (11594/26112)\n",
      "204 391 Loss: 1.505 | Acc: 44.409% (11653/26240)\n",
      "205 391 Loss: 1.505 | Acc: 44.417% (11712/26368)\n",
      "206 391 Loss: 1.505 | Acc: 44.414% (11768/26496)\n",
      "207 391 Loss: 1.505 | Acc: 44.419% (11826/26624)\n",
      "208 391 Loss: 1.505 | Acc: 44.397% (11877/26752)\n",
      "209 391 Loss: 1.506 | Acc: 44.360% (11924/26880)\n",
      "210 391 Loss: 1.506 | Acc: 44.339% (11975/27008)\n",
      "211 391 Loss: 1.506 | Acc: 44.340% (12032/27136)\n",
      "212 391 Loss: 1.505 | Acc: 44.330% (12086/27264)\n",
      "213 391 Loss: 1.505 | Acc: 44.334% (12144/27392)\n",
      "214 391 Loss: 1.505 | Acc: 44.386% (12215/27520)\n",
      "215 391 Loss: 1.504 | Acc: 44.387% (12272/27648)\n",
      "216 391 Loss: 1.504 | Acc: 44.369% (12324/27776)\n",
      "217 391 Loss: 1.504 | Acc: 44.391% (12387/27904)\n",
      "218 391 Loss: 1.505 | Acc: 44.389% (12443/28032)\n",
      "219 391 Loss: 1.504 | Acc: 44.400% (12503/28160)\n",
      "220 391 Loss: 1.504 | Acc: 44.429% (12568/28288)\n",
      "221 391 Loss: 1.504 | Acc: 44.433% (12626/28416)\n",
      "222 391 Loss: 1.504 | Acc: 44.458% (12690/28544)\n",
      "223 391 Loss: 1.504 | Acc: 44.486% (12755/28672)\n",
      "224 391 Loss: 1.504 | Acc: 44.476% (12809/28800)\n",
      "225 391 Loss: 1.503 | Acc: 44.511% (12876/28928)\n",
      "226 391 Loss: 1.503 | Acc: 44.507% (12932/29056)\n",
      "227 391 Loss: 1.503 | Acc: 44.514% (12991/29184)\n",
      "228 391 Loss: 1.503 | Acc: 44.559% (13061/29312)\n",
      "229 391 Loss: 1.502 | Acc: 44.616% (13135/29440)\n",
      "230 391 Loss: 1.502 | Acc: 44.633% (13197/29568)\n",
      "231 391 Loss: 1.502 | Acc: 44.619% (13250/29696)\n",
      "232 391 Loss: 1.501 | Acc: 44.625% (13309/29824)\n",
      "233 391 Loss: 1.501 | Acc: 44.648% (13373/29952)\n",
      "234 391 Loss: 1.501 | Acc: 44.658% (13433/30080)\n",
      "235 391 Loss: 1.501 | Acc: 44.660% (13491/30208)\n",
      "236 391 Loss: 1.500 | Acc: 44.686% (13556/30336)\n",
      "237 391 Loss: 1.500 | Acc: 44.705% (13619/30464)\n",
      "238 391 Loss: 1.499 | Acc: 44.727% (13683/30592)\n",
      "239 391 Loss: 1.499 | Acc: 44.736% (13743/30720)\n",
      "240 391 Loss: 1.499 | Acc: 44.752% (13805/30848)\n",
      "241 391 Loss: 1.499 | Acc: 44.748% (13861/30976)\n",
      "242 391 Loss: 1.499 | Acc: 44.769% (13925/31104)\n",
      "243 391 Loss: 1.499 | Acc: 44.797% (13991/31232)\n",
      "244 391 Loss: 1.499 | Acc: 44.786% (14045/31360)\n",
      "245 391 Loss: 1.498 | Acc: 44.801% (14107/31488)\n",
      "246 391 Loss: 1.498 | Acc: 44.800% (14164/31616)\n",
      "247 391 Loss: 1.498 | Acc: 44.802% (14222/31744)\n",
      "248 391 Loss: 1.497 | Acc: 44.801% (14279/31872)\n",
      "249 391 Loss: 1.497 | Acc: 44.834% (14347/32000)\n",
      "250 391 Loss: 1.497 | Acc: 44.814% (14398/32128)\n",
      "251 391 Loss: 1.497 | Acc: 44.820% (14457/32256)\n",
      "252 391 Loss: 1.496 | Acc: 44.846% (14523/32384)\n",
      "253 391 Loss: 1.496 | Acc: 44.842% (14579/32512)\n",
      "254 391 Loss: 1.496 | Acc: 44.850% (14639/32640)\n",
      "255 391 Loss: 1.496 | Acc: 44.861% (14700/32768)\n",
      "256 391 Loss: 1.496 | Acc: 44.869% (14760/32896)\n",
      "257 391 Loss: 1.496 | Acc: 44.870% (14818/33024)\n",
      "258 391 Loss: 1.496 | Acc: 44.836% (14864/33152)\n",
      "259 391 Loss: 1.496 | Acc: 44.832% (14920/33280)\n",
      "260 391 Loss: 1.497 | Acc: 44.828% (14976/33408)\n",
      "261 391 Loss: 1.497 | Acc: 44.829% (15034/33536)\n",
      "262 391 Loss: 1.497 | Acc: 44.834% (15093/33664)\n",
      "263 391 Loss: 1.496 | Acc: 44.866% (15161/33792)\n",
      "264 391 Loss: 1.496 | Acc: 44.858% (15216/33920)\n",
      "265 391 Loss: 1.496 | Acc: 44.848% (15270/34048)\n",
      "266 391 Loss: 1.496 | Acc: 44.830% (15321/34176)\n",
      "267 391 Loss: 1.495 | Acc: 44.832% (15379/34304)\n",
      "268 391 Loss: 1.495 | Acc: 44.827% (15435/34432)\n",
      "269 391 Loss: 1.496 | Acc: 44.826% (15492/34560)\n",
      "270 391 Loss: 1.495 | Acc: 44.843% (15555/34688)\n",
      "271 391 Loss: 1.495 | Acc: 44.864% (15620/34816)\n",
      "272 391 Loss: 1.495 | Acc: 44.866% (15678/34944)\n",
      "273 391 Loss: 1.495 | Acc: 44.848% (15729/35072)\n",
      "274 391 Loss: 1.495 | Acc: 44.841% (15784/35200)\n",
      "275 391 Loss: 1.494 | Acc: 44.862% (15849/35328)\n",
      "276 391 Loss: 1.494 | Acc: 44.878% (15912/35456)\n",
      "277 391 Loss: 1.495 | Acc: 44.877% (15969/35584)\n",
      "278 391 Loss: 1.494 | Acc: 44.904% (16036/35712)\n",
      "279 391 Loss: 1.494 | Acc: 44.908% (16095/35840)\n",
      "280 391 Loss: 1.494 | Acc: 44.926% (16159/35968)\n",
      "281 391 Loss: 1.494 | Acc: 44.922% (16215/36096)\n",
      "282 391 Loss: 1.494 | Acc: 44.929% (16275/36224)\n",
      "283 391 Loss: 1.494 | Acc: 44.922% (16330/36352)\n",
      "284 391 Loss: 1.494 | Acc: 44.899% (16379/36480)\n",
      "285 391 Loss: 1.494 | Acc: 44.892% (16434/36608)\n",
      "286 391 Loss: 1.494 | Acc: 44.912% (16499/36736)\n",
      "287 391 Loss: 1.494 | Acc: 44.892% (16549/36864)\n",
      "288 391 Loss: 1.494 | Acc: 44.891% (16606/36992)\n",
      "289 391 Loss: 1.494 | Acc: 44.887% (16662/37120)\n",
      "290 391 Loss: 1.494 | Acc: 44.888% (16720/37248)\n",
      "291 391 Loss: 1.494 | Acc: 44.874% (16772/37376)\n",
      "292 391 Loss: 1.494 | Acc: 44.883% (16833/37504)\n",
      "293 391 Loss: 1.494 | Acc: 44.874% (16887/37632)\n",
      "294 391 Loss: 1.494 | Acc: 44.844% (16933/37760)\n",
      "295 391 Loss: 1.494 | Acc: 44.835% (16987/37888)\n",
      "296 391 Loss: 1.495 | Acc: 44.842% (17047/38016)\n",
      "297 391 Loss: 1.494 | Acc: 44.848% (17107/38144)\n",
      "298 391 Loss: 1.494 | Acc: 44.834% (17159/38272)\n",
      "299 391 Loss: 1.494 | Acc: 44.833% (17216/38400)\n",
      "300 391 Loss: 1.494 | Acc: 44.835% (17274/38528)\n",
      "301 391 Loss: 1.494 | Acc: 44.868% (17344/38656)\n",
      "302 391 Loss: 1.493 | Acc: 44.869% (17402/38784)\n",
      "303 391 Loss: 1.493 | Acc: 44.883% (17465/38912)\n",
      "304 391 Loss: 1.493 | Acc: 44.874% (17519/39040)\n",
      "305 391 Loss: 1.493 | Acc: 44.876% (17577/39168)\n",
      "306 391 Loss: 1.493 | Acc: 44.893% (17641/39296)\n",
      "307 391 Loss: 1.493 | Acc: 44.904% (17703/39424)\n",
      "308 391 Loss: 1.493 | Acc: 44.916% (17765/39552)\n",
      "309 391 Loss: 1.493 | Acc: 44.914% (17822/39680)\n",
      "310 391 Loss: 1.493 | Acc: 44.911% (17878/39808)\n",
      "311 391 Loss: 1.493 | Acc: 44.907% (17934/39936)\n",
      "312 391 Loss: 1.493 | Acc: 44.933% (18002/40064)\n",
      "313 391 Loss: 1.492 | Acc: 44.952% (18067/40192)\n",
      "314 391 Loss: 1.492 | Acc: 44.983% (18137/40320)\n",
      "315 391 Loss: 1.492 | Acc: 44.989% (18197/40448)\n",
      "316 391 Loss: 1.492 | Acc: 45.014% (18265/40576)\n",
      "317 391 Loss: 1.492 | Acc: 45.013% (18322/40704)\n",
      "318 391 Loss: 1.492 | Acc: 45.011% (18379/40832)\n",
      "319 391 Loss: 1.492 | Acc: 45.022% (18441/40960)\n",
      "320 391 Loss: 1.491 | Acc: 45.047% (18509/41088)\n",
      "321 391 Loss: 1.491 | Acc: 45.070% (18576/41216)\n",
      "322 391 Loss: 1.491 | Acc: 45.059% (18629/41344)\n",
      "323 391 Loss: 1.491 | Acc: 45.076% (18694/41472)\n",
      "324 391 Loss: 1.491 | Acc: 45.079% (18753/41600)\n",
      "325 391 Loss: 1.491 | Acc: 45.068% (18806/41728)\n",
      "326 391 Loss: 1.491 | Acc: 45.054% (18858/41856)\n",
      "327 391 Loss: 1.491 | Acc: 45.058% (18917/41984)\n",
      "328 391 Loss: 1.491 | Acc: 45.051% (18972/42112)\n",
      "329 391 Loss: 1.491 | Acc: 45.069% (19037/42240)\n",
      "330 391 Loss: 1.491 | Acc: 45.067% (19094/42368)\n",
      "331 391 Loss: 1.491 | Acc: 45.072% (19154/42496)\n",
      "332 391 Loss: 1.491 | Acc: 45.061% (19207/42624)\n",
      "333 391 Loss: 1.490 | Acc: 45.074% (19270/42752)\n",
      "334 391 Loss: 1.491 | Acc: 45.051% (19318/42880)\n",
      "335 391 Loss: 1.491 | Acc: 45.040% (19371/43008)\n",
      "336 391 Loss: 1.491 | Acc: 45.039% (19428/43136)\n",
      "337 391 Loss: 1.490 | Acc: 45.040% (19486/43264)\n",
      "338 391 Loss: 1.490 | Acc: 45.064% (19554/43392)\n",
      "339 391 Loss: 1.490 | Acc: 45.055% (19608/43520)\n",
      "340 391 Loss: 1.490 | Acc: 45.054% (19665/43648)\n",
      "341 391 Loss: 1.489 | Acc: 45.077% (19733/43776)\n",
      "342 391 Loss: 1.489 | Acc: 45.098% (19800/43904)\n",
      "343 391 Loss: 1.488 | Acc: 45.104% (19860/44032)\n",
      "344 391 Loss: 1.488 | Acc: 45.134% (19931/44160)\n",
      "345 391 Loss: 1.488 | Acc: 45.123% (19984/44288)\n",
      "346 391 Loss: 1.487 | Acc: 45.137% (20048/44416)\n",
      "347 391 Loss: 1.487 | Acc: 45.160% (20116/44544)\n",
      "348 391 Loss: 1.487 | Acc: 45.167% (20177/44672)\n",
      "349 391 Loss: 1.487 | Acc: 45.181% (20241/44800)\n",
      "350 391 Loss: 1.486 | Acc: 45.199% (20307/44928)\n",
      "351 391 Loss: 1.486 | Acc: 45.213% (20371/45056)\n",
      "352 391 Loss: 1.486 | Acc: 45.215% (20430/45184)\n",
      "353 391 Loss: 1.486 | Acc: 45.220% (20490/45312)\n",
      "354 391 Loss: 1.486 | Acc: 45.220% (20548/45440)\n",
      "355 391 Loss: 1.486 | Acc: 45.214% (20603/45568)\n",
      "356 391 Loss: 1.486 | Acc: 45.212% (20660/45696)\n",
      "357 391 Loss: 1.486 | Acc: 45.232% (20727/45824)\n",
      "358 391 Loss: 1.486 | Acc: 45.232% (20785/45952)\n",
      "359 391 Loss: 1.486 | Acc: 45.211% (20833/46080)\n",
      "360 391 Loss: 1.486 | Acc: 45.211% (20891/46208)\n",
      "361 391 Loss: 1.486 | Acc: 45.209% (20948/46336)\n",
      "362 391 Loss: 1.486 | Acc: 45.216% (21009/46464)\n",
      "363 391 Loss: 1.486 | Acc: 45.229% (21073/46592)\n",
      "364 391 Loss: 1.486 | Acc: 45.238% (21135/46720)\n",
      "365 391 Loss: 1.486 | Acc: 45.234% (21191/46848)\n",
      "366 391 Loss: 1.486 | Acc: 45.253% (21258/46976)\n",
      "367 391 Loss: 1.486 | Acc: 45.249% (21314/47104)\n",
      "368 391 Loss: 1.486 | Acc: 45.249% (21372/47232)\n",
      "369 391 Loss: 1.486 | Acc: 45.262% (21436/47360)\n",
      "370 391 Loss: 1.486 | Acc: 45.262% (21494/47488)\n",
      "371 391 Loss: 1.486 | Acc: 45.254% (21548/47616)\n",
      "372 391 Loss: 1.486 | Acc: 45.225% (21592/47744)\n",
      "373 391 Loss: 1.486 | Acc: 45.235% (21655/47872)\n",
      "374 391 Loss: 1.486 | Acc: 45.219% (21705/48000)\n",
      "375 391 Loss: 1.486 | Acc: 45.236% (21771/48128)\n",
      "376 391 Loss: 1.486 | Acc: 45.248% (21835/48256)\n",
      "377 391 Loss: 1.485 | Acc: 45.242% (21890/48384)\n",
      "378 391 Loss: 1.485 | Acc: 45.251% (21952/48512)\n",
      "379 391 Loss: 1.485 | Acc: 45.241% (22005/48640)\n",
      "380 391 Loss: 1.485 | Acc: 45.251% (22068/48768)\n",
      "381 391 Loss: 1.485 | Acc: 45.243% (22122/48896)\n",
      "382 391 Loss: 1.486 | Acc: 45.231% (22174/49024)\n",
      "383 391 Loss: 1.485 | Acc: 45.247% (22240/49152)\n",
      "384 391 Loss: 1.485 | Acc: 45.241% (22295/49280)\n",
      "385 391 Loss: 1.486 | Acc: 45.236% (22350/49408)\n",
      "386 391 Loss: 1.485 | Acc: 45.258% (22419/49536)\n",
      "387 391 Loss: 1.485 | Acc: 45.268% (22482/49664)\n",
      "388 391 Loss: 1.485 | Acc: 45.268% (22540/49792)\n",
      "389 391 Loss: 1.484 | Acc: 45.274% (22601/49920)\n",
      "390 391 Loss: 1.485 | Acc: 45.272% (22636/50000)\n",
      "0 100 Loss: 1.395 | Acc: 48.000% (48/100)\n",
      "1 100 Loss: 1.406 | Acc: 46.000% (92/200)\n",
      "2 100 Loss: 1.390 | Acc: 45.667% (137/300)\n",
      "3 100 Loss: 1.424 | Acc: 45.250% (181/400)\n",
      "4 100 Loss: 1.427 | Acc: 45.400% (227/500)\n",
      "5 100 Loss: 1.434 | Acc: 45.000% (270/600)\n",
      "6 100 Loss: 1.437 | Acc: 45.714% (320/700)\n",
      "7 100 Loss: 1.446 | Acc: 45.250% (362/800)\n",
      "8 100 Loss: 1.451 | Acc: 45.667% (411/900)\n",
      "9 100 Loss: 1.443 | Acc: 46.100% (461/1000)\n",
      "10 100 Loss: 1.428 | Acc: 46.636% (513/1100)\n",
      "11 100 Loss: 1.426 | Acc: 46.250% (555/1200)\n",
      "12 100 Loss: 1.442 | Acc: 45.846% (596/1300)\n",
      "13 100 Loss: 1.450 | Acc: 45.714% (640/1400)\n",
      "14 100 Loss: 1.439 | Acc: 46.200% (693/1500)\n",
      "15 100 Loss: 1.438 | Acc: 46.438% (743/1600)\n",
      "16 100 Loss: 1.436 | Acc: 46.941% (798/1700)\n",
      "17 100 Loss: 1.429 | Acc: 47.278% (851/1800)\n",
      "18 100 Loss: 1.430 | Acc: 47.632% (905/1900)\n",
      "19 100 Loss: 1.432 | Acc: 47.350% (947/2000)\n",
      "20 100 Loss: 1.426 | Acc: 47.619% (1000/2100)\n",
      "21 100 Loss: 1.426 | Acc: 47.682% (1049/2200)\n",
      "22 100 Loss: 1.424 | Acc: 47.435% (1091/2300)\n",
      "23 100 Loss: 1.422 | Acc: 47.417% (1138/2400)\n",
      "24 100 Loss: 1.420 | Acc: 47.480% (1187/2500)\n",
      "25 100 Loss: 1.432 | Acc: 47.115% (1225/2600)\n",
      "26 100 Loss: 1.430 | Acc: 47.185% (1274/2700)\n",
      "27 100 Loss: 1.431 | Acc: 46.786% (1310/2800)\n",
      "28 100 Loss: 1.435 | Acc: 46.828% (1358/2900)\n",
      "29 100 Loss: 1.431 | Acc: 47.000% (1410/3000)\n",
      "30 100 Loss: 1.426 | Acc: 47.387% (1469/3100)\n",
      "31 100 Loss: 1.424 | Acc: 47.375% (1516/3200)\n",
      "32 100 Loss: 1.423 | Acc: 47.455% (1566/3300)\n",
      "33 100 Loss: 1.424 | Acc: 47.559% (1617/3400)\n",
      "34 100 Loss: 1.423 | Acc: 47.543% (1664/3500)\n",
      "35 100 Loss: 1.419 | Acc: 47.722% (1718/3600)\n",
      "36 100 Loss: 1.420 | Acc: 47.811% (1769/3700)\n",
      "37 100 Loss: 1.422 | Acc: 47.684% (1812/3800)\n",
      "38 100 Loss: 1.422 | Acc: 47.538% (1854/3900)\n",
      "39 100 Loss: 1.424 | Acc: 47.600% (1904/4000)\n",
      "40 100 Loss: 1.427 | Acc: 47.659% (1954/4100)\n",
      "41 100 Loss: 1.429 | Acc: 47.595% (1999/4200)\n",
      "42 100 Loss: 1.427 | Acc: 47.651% (2049/4300)\n",
      "43 100 Loss: 1.423 | Acc: 47.750% (2101/4400)\n",
      "44 100 Loss: 1.421 | Acc: 47.844% (2153/4500)\n",
      "45 100 Loss: 1.420 | Acc: 47.826% (2200/4600)\n",
      "46 100 Loss: 1.416 | Acc: 47.957% (2254/4700)\n",
      "47 100 Loss: 1.418 | Acc: 48.083% (2308/4800)\n",
      "48 100 Loss: 1.416 | Acc: 48.265% (2365/4900)\n",
      "49 100 Loss: 1.415 | Acc: 48.340% (2417/5000)\n",
      "50 100 Loss: 1.416 | Acc: 48.333% (2465/5100)\n",
      "51 100 Loss: 1.415 | Acc: 48.346% (2514/5200)\n",
      "52 100 Loss: 1.413 | Acc: 48.302% (2560/5300)\n",
      "53 100 Loss: 1.415 | Acc: 48.370% (2612/5400)\n",
      "54 100 Loss: 1.419 | Acc: 48.200% (2651/5500)\n",
      "55 100 Loss: 1.422 | Acc: 48.036% (2690/5600)\n",
      "56 100 Loss: 1.423 | Acc: 48.088% (2741/5700)\n",
      "57 100 Loss: 1.418 | Acc: 48.207% (2796/5800)\n",
      "58 100 Loss: 1.418 | Acc: 48.153% (2841/5900)\n",
      "59 100 Loss: 1.417 | Acc: 48.200% (2892/6000)\n",
      "60 100 Loss: 1.418 | Acc: 48.148% (2937/6100)\n",
      "61 100 Loss: 1.419 | Acc: 48.097% (2982/6200)\n",
      "62 100 Loss: 1.418 | Acc: 48.127% (3032/6300)\n",
      "63 100 Loss: 1.418 | Acc: 48.094% (3078/6400)\n",
      "64 100 Loss: 1.418 | Acc: 48.092% (3126/6500)\n",
      "65 100 Loss: 1.419 | Acc: 48.152% (3178/6600)\n",
      "66 100 Loss: 1.418 | Acc: 48.254% (3233/6700)\n",
      "67 100 Loss: 1.418 | Acc: 48.309% (3285/6800)\n",
      "68 100 Loss: 1.418 | Acc: 48.246% (3329/6900)\n",
      "69 100 Loss: 1.420 | Acc: 48.300% (3381/7000)\n",
      "70 100 Loss: 1.420 | Acc: 48.268% (3427/7100)\n",
      "71 100 Loss: 1.420 | Acc: 48.292% (3477/7200)\n",
      "72 100 Loss: 1.419 | Acc: 48.301% (3526/7300)\n",
      "73 100 Loss: 1.417 | Acc: 48.351% (3578/7400)\n",
      "74 100 Loss: 1.417 | Acc: 48.213% (3616/7500)\n",
      "75 100 Loss: 1.416 | Acc: 48.250% (3667/7600)\n",
      "76 100 Loss: 1.416 | Acc: 48.169% (3709/7700)\n",
      "77 100 Loss: 1.415 | Acc: 48.192% (3759/7800)\n",
      "78 100 Loss: 1.414 | Acc: 48.304% (3816/7900)\n",
      "79 100 Loss: 1.415 | Acc: 48.275% (3862/8000)\n",
      "80 100 Loss: 1.414 | Acc: 48.284% (3911/8100)\n",
      "81 100 Loss: 1.415 | Acc: 48.244% (3956/8200)\n",
      "82 100 Loss: 1.417 | Acc: 48.181% (3999/8300)\n",
      "83 100 Loss: 1.417 | Acc: 48.143% (4044/8400)\n",
      "84 100 Loss: 1.419 | Acc: 48.094% (4088/8500)\n",
      "85 100 Loss: 1.418 | Acc: 48.174% (4143/8600)\n",
      "86 100 Loss: 1.420 | Acc: 48.115% (4186/8700)\n",
      "87 100 Loss: 1.420 | Acc: 48.080% (4231/8800)\n",
      "88 100 Loss: 1.420 | Acc: 48.101% (4281/8900)\n",
      "89 100 Loss: 1.420 | Acc: 48.144% (4333/9000)\n",
      "90 100 Loss: 1.420 | Acc: 48.077% (4375/9100)\n",
      "91 100 Loss: 1.420 | Acc: 48.109% (4426/9200)\n",
      "92 100 Loss: 1.418 | Acc: 48.172% (4480/9300)\n",
      "93 100 Loss: 1.419 | Acc: 48.149% (4526/9400)\n",
      "94 100 Loss: 1.420 | Acc: 48.095% (4569/9500)\n",
      "95 100 Loss: 1.419 | Acc: 48.125% (4620/9600)\n",
      "96 100 Loss: 1.419 | Acc: 48.124% (4668/9700)\n",
      "97 100 Loss: 1.420 | Acc: 48.061% (4710/9800)\n",
      "98 100 Loss: 1.420 | Acc: 48.040% (4756/9900)\n",
      "99 100 Loss: 1.421 | Acc: 48.000% (4800/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      "0 391 Loss: 1.437 | Acc: 39.844% (51/128)\n",
      "1 391 Loss: 1.438 | Acc: 43.750% (112/256)\n",
      "2 391 Loss: 1.455 | Acc: 43.750% (168/384)\n",
      "3 391 Loss: 1.450 | Acc: 43.750% (224/512)\n",
      "4 391 Loss: 1.484 | Acc: 43.750% (280/640)\n",
      "5 391 Loss: 1.467 | Acc: 45.182% (347/768)\n",
      "6 391 Loss: 1.465 | Acc: 45.089% (404/896)\n",
      "7 391 Loss: 1.448 | Acc: 46.289% (474/1024)\n",
      "8 391 Loss: 1.445 | Acc: 46.701% (538/1152)\n",
      "9 391 Loss: 1.451 | Acc: 46.719% (598/1280)\n",
      "10 391 Loss: 1.457 | Acc: 46.236% (651/1408)\n",
      "11 391 Loss: 1.459 | Acc: 45.898% (705/1536)\n",
      "12 391 Loss: 1.455 | Acc: 45.974% (765/1664)\n",
      "13 391 Loss: 1.460 | Acc: 45.703% (819/1792)\n",
      "14 391 Loss: 1.453 | Acc: 45.833% (880/1920)\n",
      "15 391 Loss: 1.449 | Acc: 46.191% (946/2048)\n",
      "16 391 Loss: 1.451 | Acc: 46.186% (1005/2176)\n",
      "17 391 Loss: 1.457 | Acc: 45.833% (1056/2304)\n",
      "18 391 Loss: 1.469 | Acc: 45.436% (1105/2432)\n",
      "19 391 Loss: 1.466 | Acc: 45.664% (1169/2560)\n",
      "20 391 Loss: 1.463 | Acc: 45.908% (1234/2688)\n",
      "21 391 Loss: 1.460 | Acc: 46.094% (1298/2816)\n",
      "22 391 Loss: 1.457 | Acc: 46.128% (1358/2944)\n",
      "23 391 Loss: 1.460 | Acc: 45.898% (1410/3072)\n",
      "24 391 Loss: 1.461 | Acc: 45.875% (1468/3200)\n",
      "25 391 Loss: 1.457 | Acc: 46.154% (1536/3328)\n",
      "26 391 Loss: 1.458 | Acc: 46.152% (1595/3456)\n",
      "27 391 Loss: 1.461 | Acc: 46.038% (1650/3584)\n",
      "28 391 Loss: 1.464 | Acc: 45.905% (1704/3712)\n",
      "29 391 Loss: 1.462 | Acc: 45.911% (1763/3840)\n",
      "30 391 Loss: 1.459 | Acc: 45.943% (1823/3968)\n",
      "31 391 Loss: 1.461 | Acc: 45.728% (1873/4096)\n",
      "32 391 Loss: 1.459 | Acc: 45.739% (1932/4224)\n",
      "33 391 Loss: 1.459 | Acc: 45.703% (1989/4352)\n",
      "34 391 Loss: 1.459 | Acc: 45.737% (2049/4480)\n",
      "35 391 Loss: 1.457 | Acc: 45.833% (2112/4608)\n",
      "36 391 Loss: 1.456 | Acc: 45.946% (2176/4736)\n",
      "37 391 Loss: 1.457 | Acc: 45.929% (2234/4864)\n",
      "38 391 Loss: 1.457 | Acc: 45.833% (2288/4992)\n",
      "39 391 Loss: 1.454 | Acc: 45.957% (2353/5120)\n",
      "40 391 Loss: 1.455 | Acc: 45.884% (2408/5248)\n",
      "41 391 Loss: 1.457 | Acc: 45.833% (2464/5376)\n",
      "42 391 Loss: 1.456 | Acc: 45.894% (2526/5504)\n",
      "43 391 Loss: 1.455 | Acc: 45.934% (2587/5632)\n",
      "44 391 Loss: 1.455 | Acc: 46.059% (2653/5760)\n",
      "45 391 Loss: 1.454 | Acc: 45.975% (2707/5888)\n",
      "46 391 Loss: 1.455 | Acc: 45.928% (2763/6016)\n",
      "47 391 Loss: 1.456 | Acc: 45.736% (2810/6144)\n",
      "48 391 Loss: 1.455 | Acc: 45.855% (2876/6272)\n",
      "49 391 Loss: 1.455 | Acc: 45.984% (2943/6400)\n",
      "50 391 Loss: 1.454 | Acc: 46.078% (3008/6528)\n",
      "51 391 Loss: 1.457 | Acc: 46.004% (3062/6656)\n",
      "52 391 Loss: 1.454 | Acc: 46.108% (3128/6784)\n",
      "53 391 Loss: 1.456 | Acc: 46.152% (3190/6912)\n",
      "54 391 Loss: 1.457 | Acc: 46.108% (3246/7040)\n",
      "55 391 Loss: 1.456 | Acc: 46.094% (3304/7168)\n",
      "56 391 Loss: 1.457 | Acc: 46.039% (3359/7296)\n",
      "57 391 Loss: 1.454 | Acc: 46.148% (3426/7424)\n",
      "58 391 Loss: 1.453 | Acc: 46.226% (3491/7552)\n",
      "59 391 Loss: 1.454 | Acc: 46.042% (3536/7680)\n",
      "60 391 Loss: 1.452 | Acc: 46.132% (3602/7808)\n",
      "61 391 Loss: 1.452 | Acc: 46.270% (3672/7936)\n",
      "62 391 Loss: 1.450 | Acc: 46.404% (3742/8064)\n",
      "63 391 Loss: 1.448 | Acc: 46.436% (3804/8192)\n",
      "64 391 Loss: 1.448 | Acc: 46.454% (3865/8320)\n",
      "65 391 Loss: 1.447 | Acc: 46.473% (3926/8448)\n",
      "66 391 Loss: 1.447 | Acc: 46.479% (3986/8576)\n",
      "67 391 Loss: 1.446 | Acc: 46.507% (4048/8704)\n",
      "68 391 Loss: 1.447 | Acc: 46.479% (4105/8832)\n",
      "69 391 Loss: 1.446 | Acc: 46.473% (4164/8960)\n",
      "70 391 Loss: 1.445 | Acc: 46.446% (4221/9088)\n",
      "71 391 Loss: 1.446 | Acc: 46.484% (4284/9216)\n",
      "72 391 Loss: 1.447 | Acc: 46.415% (4337/9344)\n",
      "73 391 Loss: 1.446 | Acc: 46.463% (4401/9472)\n",
      "74 391 Loss: 1.446 | Acc: 46.448% (4459/9600)\n",
      "75 391 Loss: 1.445 | Acc: 46.474% (4521/9728)\n",
      "76 391 Loss: 1.444 | Acc: 46.520% (4585/9856)\n",
      "77 391 Loss: 1.445 | Acc: 46.484% (4641/9984)\n",
      "78 391 Loss: 1.445 | Acc: 46.470% (4699/10112)\n",
      "79 391 Loss: 1.445 | Acc: 46.436% (4755/10240)\n",
      "80 391 Loss: 1.444 | Acc: 46.470% (4818/10368)\n",
      "81 391 Loss: 1.444 | Acc: 46.456% (4876/10496)\n",
      "82 391 Loss: 1.444 | Acc: 46.442% (4934/10624)\n",
      "83 391 Loss: 1.443 | Acc: 46.559% (5006/10752)\n",
      "84 391 Loss: 1.443 | Acc: 46.599% (5070/10880)\n",
      "85 391 Loss: 1.442 | Acc: 46.648% (5135/11008)\n",
      "86 391 Loss: 1.441 | Acc: 46.767% (5208/11136)\n",
      "87 391 Loss: 1.443 | Acc: 46.715% (5262/11264)\n",
      "88 391 Loss: 1.443 | Acc: 46.664% (5316/11392)\n",
      "89 391 Loss: 1.445 | Acc: 46.597% (5368/11520)\n",
      "90 391 Loss: 1.444 | Acc: 46.643% (5433/11648)\n",
      "91 391 Loss: 1.445 | Acc: 46.697% (5499/11776)\n",
      "92 391 Loss: 1.447 | Acc: 46.581% (5545/11904)\n",
      "93 391 Loss: 1.444 | Acc: 46.734% (5623/12032)\n",
      "94 391 Loss: 1.445 | Acc: 46.711% (5680/12160)\n",
      "95 391 Loss: 1.444 | Acc: 46.745% (5744/12288)\n",
      "96 391 Loss: 1.444 | Acc: 46.778% (5808/12416)\n",
      "97 391 Loss: 1.443 | Acc: 46.787% (5869/12544)\n",
      "98 391 Loss: 1.444 | Acc: 46.765% (5926/12672)\n",
      "99 391 Loss: 1.442 | Acc: 46.820% (5993/12800)\n",
      "100 391 Loss: 1.442 | Acc: 46.836% (6055/12928)\n",
      "101 391 Loss: 1.442 | Acc: 46.760% (6105/13056)\n",
      "102 391 Loss: 1.442 | Acc: 46.769% (6166/13184)\n",
      "103 391 Loss: 1.442 | Acc: 46.732% (6221/13312)\n",
      "104 391 Loss: 1.444 | Acc: 46.667% (6272/13440)\n",
      "105 391 Loss: 1.444 | Acc: 46.683% (6334/13568)\n",
      "106 391 Loss: 1.444 | Acc: 46.649% (6389/13696)\n",
      "107 391 Loss: 1.445 | Acc: 46.578% (6439/13824)\n",
      "108 391 Loss: 1.444 | Acc: 46.610% (6503/13952)\n",
      "109 391 Loss: 1.443 | Acc: 46.655% (6569/14080)\n",
      "110 391 Loss: 1.443 | Acc: 46.650% (6628/14208)\n",
      "111 391 Loss: 1.443 | Acc: 46.694% (6694/14336)\n",
      "112 391 Loss: 1.442 | Acc: 46.702% (6755/14464)\n",
      "113 391 Loss: 1.442 | Acc: 46.717% (6817/14592)\n",
      "114 391 Loss: 1.441 | Acc: 46.705% (6875/14720)\n",
      "115 391 Loss: 1.440 | Acc: 46.720% (6937/14848)\n",
      "116 391 Loss: 1.440 | Acc: 46.721% (6997/14976)\n",
      "117 391 Loss: 1.440 | Acc: 46.696% (7053/15104)\n",
      "118 391 Loss: 1.441 | Acc: 46.665% (7108/15232)\n",
      "119 391 Loss: 1.441 | Acc: 46.680% (7170/15360)\n",
      "120 391 Loss: 1.441 | Acc: 46.681% (7230/15488)\n",
      "121 391 Loss: 1.442 | Acc: 46.683% (7290/15616)\n",
      "122 391 Loss: 1.441 | Acc: 46.704% (7353/15744)\n",
      "123 391 Loss: 1.441 | Acc: 46.717% (7415/15872)\n",
      "124 391 Loss: 1.440 | Acc: 46.737% (7478/16000)\n",
      "125 391 Loss: 1.440 | Acc: 46.763% (7542/16128)\n",
      "126 391 Loss: 1.440 | Acc: 46.795% (7607/16256)\n",
      "127 391 Loss: 1.439 | Acc: 46.808% (7669/16384)\n",
      "128 391 Loss: 1.439 | Acc: 46.814% (7730/16512)\n",
      "129 391 Loss: 1.440 | Acc: 46.803% (7788/16640)\n",
      "130 391 Loss: 1.439 | Acc: 46.815% (7850/16768)\n",
      "131 391 Loss: 1.440 | Acc: 46.810% (7909/16896)\n",
      "132 391 Loss: 1.439 | Acc: 46.852% (7976/17024)\n",
      "133 391 Loss: 1.438 | Acc: 46.881% (8041/17152)\n",
      "134 391 Loss: 1.440 | Acc: 46.863% (8098/17280)\n",
      "135 391 Loss: 1.440 | Acc: 46.846% (8155/17408)\n",
      "136 391 Loss: 1.439 | Acc: 46.858% (8217/17536)\n",
      "137 391 Loss: 1.438 | Acc: 46.909% (8286/17664)\n",
      "138 391 Loss: 1.439 | Acc: 46.914% (8347/17792)\n",
      "139 391 Loss: 1.440 | Acc: 46.908% (8406/17920)\n",
      "140 391 Loss: 1.439 | Acc: 46.947% (8473/18048)\n",
      "141 391 Loss: 1.440 | Acc: 46.936% (8531/18176)\n",
      "142 391 Loss: 1.440 | Acc: 46.902% (8585/18304)\n",
      "143 391 Loss: 1.440 | Acc: 46.924% (8649/18432)\n",
      "144 391 Loss: 1.440 | Acc: 46.934% (8711/18560)\n",
      "145 391 Loss: 1.439 | Acc: 46.987% (8781/18688)\n",
      "146 391 Loss: 1.439 | Acc: 46.981% (8840/18816)\n",
      "147 391 Loss: 1.439 | Acc: 46.959% (8896/18944)\n",
      "148 391 Loss: 1.439 | Acc: 46.933% (8951/19072)\n",
      "149 391 Loss: 1.440 | Acc: 46.917% (9008/19200)\n",
      "150 391 Loss: 1.440 | Acc: 46.916% (9068/19328)\n",
      "151 391 Loss: 1.439 | Acc: 46.906% (9126/19456)\n",
      "152 391 Loss: 1.439 | Acc: 46.952% (9195/19584)\n",
      "153 391 Loss: 1.439 | Acc: 46.971% (9259/19712)\n",
      "154 391 Loss: 1.438 | Acc: 46.961% (9317/19840)\n",
      "155 391 Loss: 1.437 | Acc: 47.010% (9387/19968)\n",
      "156 391 Loss: 1.437 | Acc: 47.004% (9446/20096)\n",
      "157 391 Loss: 1.438 | Acc: 47.023% (9510/20224)\n",
      "158 391 Loss: 1.438 | Acc: 46.983% (9562/20352)\n",
      "159 391 Loss: 1.438 | Acc: 46.992% (9624/20480)\n",
      "160 391 Loss: 1.437 | Acc: 47.030% (9692/20608)\n",
      "161 391 Loss: 1.437 | Acc: 47.049% (9756/20736)\n",
      "162 391 Loss: 1.436 | Acc: 47.043% (9815/20864)\n",
      "163 391 Loss: 1.436 | Acc: 47.056% (9878/20992)\n",
      "164 391 Loss: 1.436 | Acc: 47.003% (9927/21120)\n",
      "165 391 Loss: 1.437 | Acc: 46.974% (9981/21248)\n",
      "166 391 Loss: 1.436 | Acc: 47.034% (10054/21376)\n",
      "167 391 Loss: 1.435 | Acc: 47.080% (10124/21504)\n",
      "168 391 Loss: 1.434 | Acc: 47.134% (10196/21632)\n",
      "169 391 Loss: 1.434 | Acc: 47.160% (10262/21760)\n",
      "170 391 Loss: 1.433 | Acc: 47.181% (10327/21888)\n",
      "171 391 Loss: 1.433 | Acc: 47.166% (10384/22016)\n",
      "172 391 Loss: 1.433 | Acc: 47.169% (10445/22144)\n",
      "173 391 Loss: 1.433 | Acc: 47.167% (10505/22272)\n",
      "174 391 Loss: 1.433 | Acc: 47.161% (10564/22400)\n",
      "175 391 Loss: 1.433 | Acc: 47.168% (10626/22528)\n",
      "176 391 Loss: 1.433 | Acc: 47.149% (10682/22656)\n",
      "177 391 Loss: 1.433 | Acc: 47.147% (10742/22784)\n",
      "178 391 Loss: 1.433 | Acc: 47.189% (10812/22912)\n",
      "179 391 Loss: 1.434 | Acc: 47.183% (10871/23040)\n",
      "180 391 Loss: 1.434 | Acc: 47.169% (10928/23168)\n",
      "181 391 Loss: 1.433 | Acc: 47.171% (10989/23296)\n",
      "182 391 Loss: 1.433 | Acc: 47.170% (11049/23424)\n",
      "183 391 Loss: 1.433 | Acc: 47.159% (11107/23552)\n",
      "184 391 Loss: 1.432 | Acc: 47.196% (11176/23680)\n",
      "185 391 Loss: 1.433 | Acc: 47.161% (11228/23808)\n",
      "186 391 Loss: 1.433 | Acc: 47.142% (11284/23936)\n",
      "187 391 Loss: 1.433 | Acc: 47.145% (11345/24064)\n",
      "188 391 Loss: 1.433 | Acc: 47.127% (11401/24192)\n",
      "189 391 Loss: 1.433 | Acc: 47.142% (11465/24320)\n",
      "190 391 Loss: 1.433 | Acc: 47.129% (11522/24448)\n",
      "191 391 Loss: 1.432 | Acc: 47.127% (11582/24576)\n",
      "192 391 Loss: 1.432 | Acc: 47.154% (11649/24704)\n",
      "193 391 Loss: 1.432 | Acc: 47.141% (11706/24832)\n",
      "194 391 Loss: 1.431 | Acc: 47.179% (11776/24960)\n",
      "195 391 Loss: 1.431 | Acc: 47.162% (11832/25088)\n",
      "196 391 Loss: 1.432 | Acc: 47.149% (11889/25216)\n",
      "197 391 Loss: 1.432 | Acc: 47.139% (11947/25344)\n",
      "198 391 Loss: 1.431 | Acc: 47.150% (12010/25472)\n",
      "199 391 Loss: 1.432 | Acc: 47.125% (12064/25600)\n",
      "200 391 Loss: 1.432 | Acc: 47.124% (12124/25728)\n",
      "201 391 Loss: 1.431 | Acc: 47.153% (12192/25856)\n",
      "202 391 Loss: 1.431 | Acc: 47.160% (12254/25984)\n",
      "203 391 Loss: 1.431 | Acc: 47.158% (12314/26112)\n",
      "204 391 Loss: 1.431 | Acc: 47.127% (12366/26240)\n",
      "205 391 Loss: 1.431 | Acc: 47.137% (12429/26368)\n",
      "206 391 Loss: 1.431 | Acc: 47.147% (12492/26496)\n",
      "207 391 Loss: 1.431 | Acc: 47.145% (12552/26624)\n",
      "208 391 Loss: 1.431 | Acc: 47.125% (12607/26752)\n",
      "209 391 Loss: 1.430 | Acc: 47.154% (12675/26880)\n",
      "210 391 Loss: 1.430 | Acc: 47.160% (12737/27008)\n",
      "211 391 Loss: 1.430 | Acc: 47.196% (12807/27136)\n",
      "212 391 Loss: 1.429 | Acc: 47.198% (12868/27264)\n",
      "213 391 Loss: 1.429 | Acc: 47.218% (12934/27392)\n",
      "214 391 Loss: 1.430 | Acc: 47.166% (12980/27520)\n",
      "215 391 Loss: 1.429 | Acc: 47.186% (13046/27648)\n",
      "216 391 Loss: 1.429 | Acc: 47.199% (13110/27776)\n",
      "217 391 Loss: 1.428 | Acc: 47.219% (13176/27904)\n",
      "218 391 Loss: 1.428 | Acc: 47.217% (13236/28032)\n",
      "219 391 Loss: 1.428 | Acc: 47.241% (13303/28160)\n",
      "220 391 Loss: 1.428 | Acc: 47.232% (13361/28288)\n",
      "221 391 Loss: 1.427 | Acc: 47.220% (13418/28416)\n",
      "222 391 Loss: 1.427 | Acc: 47.222% (13479/28544)\n",
      "223 391 Loss: 1.428 | Acc: 47.199% (13533/28672)\n",
      "224 391 Loss: 1.428 | Acc: 47.188% (13590/28800)\n",
      "225 391 Loss: 1.428 | Acc: 47.207% (13656/28928)\n",
      "226 391 Loss: 1.427 | Acc: 47.202% (13715/29056)\n",
      "227 391 Loss: 1.427 | Acc: 47.269% (13795/29184)\n",
      "228 391 Loss: 1.426 | Acc: 47.267% (13855/29312)\n",
      "229 391 Loss: 1.427 | Acc: 47.272% (13917/29440)\n",
      "230 391 Loss: 1.427 | Acc: 47.213% (13960/29568)\n",
      "231 391 Loss: 1.428 | Acc: 47.192% (14014/29696)\n",
      "232 391 Loss: 1.427 | Acc: 47.220% (14083/29824)\n",
      "233 391 Loss: 1.428 | Acc: 47.189% (14134/29952)\n",
      "234 391 Loss: 1.428 | Acc: 47.197% (14197/30080)\n",
      "235 391 Loss: 1.427 | Acc: 47.203% (14259/30208)\n",
      "236 391 Loss: 1.428 | Acc: 47.178% (14312/30336)\n",
      "237 391 Loss: 1.428 | Acc: 47.167% (14369/30464)\n",
      "238 391 Loss: 1.428 | Acc: 47.166% (14429/30592)\n",
      "239 391 Loss: 1.429 | Acc: 47.148% (14484/30720)\n",
      "240 391 Loss: 1.428 | Acc: 47.147% (14544/30848)\n",
      "241 391 Loss: 1.429 | Acc: 47.133% (14600/30976)\n",
      "242 391 Loss: 1.428 | Acc: 47.164% (14670/31104)\n",
      "243 391 Loss: 1.428 | Acc: 47.186% (14737/31232)\n",
      "244 391 Loss: 1.427 | Acc: 47.216% (14807/31360)\n",
      "245 391 Loss: 1.427 | Acc: 47.247% (14877/31488)\n",
      "246 391 Loss: 1.427 | Acc: 47.251% (14939/31616)\n",
      "247 391 Loss: 1.427 | Acc: 47.253% (15000/31744)\n",
      "248 391 Loss: 1.426 | Acc: 47.283% (15070/31872)\n",
      "249 391 Loss: 1.427 | Acc: 47.269% (15126/32000)\n",
      "250 391 Loss: 1.426 | Acc: 47.295% (15195/32128)\n",
      "251 391 Loss: 1.426 | Acc: 47.284% (15252/32256)\n",
      "252 391 Loss: 1.426 | Acc: 47.273% (15309/32384)\n",
      "253 391 Loss: 1.426 | Acc: 47.256% (15364/32512)\n",
      "254 391 Loss: 1.426 | Acc: 47.270% (15429/32640)\n",
      "255 391 Loss: 1.426 | Acc: 47.281% (15493/32768)\n",
      "256 391 Loss: 1.426 | Acc: 47.291% (15557/32896)\n",
      "257 391 Loss: 1.426 | Acc: 47.299% (15620/33024)\n",
      "258 391 Loss: 1.425 | Acc: 47.321% (15688/33152)\n",
      "259 391 Loss: 1.425 | Acc: 47.302% (15742/33280)\n",
      "260 391 Loss: 1.425 | Acc: 47.291% (15799/33408)\n",
      "261 391 Loss: 1.425 | Acc: 47.313% (15867/33536)\n",
      "262 391 Loss: 1.425 | Acc: 47.306% (15925/33664)\n",
      "263 391 Loss: 1.425 | Acc: 47.304% (15985/33792)\n",
      "264 391 Loss: 1.425 | Acc: 47.294% (16042/33920)\n",
      "265 391 Loss: 1.425 | Acc: 47.318% (16111/34048)\n",
      "266 391 Loss: 1.425 | Acc: 47.331% (16176/34176)\n",
      "267 391 Loss: 1.425 | Acc: 47.318% (16232/34304)\n",
      "268 391 Loss: 1.425 | Acc: 47.322% (16294/34432)\n",
      "269 391 Loss: 1.425 | Acc: 47.338% (16360/34560)\n",
      "270 391 Loss: 1.425 | Acc: 47.354% (16426/34688)\n",
      "271 391 Loss: 1.425 | Acc: 47.358% (16488/34816)\n",
      "272 391 Loss: 1.425 | Acc: 47.382% (16557/34944)\n",
      "273 391 Loss: 1.424 | Acc: 47.394% (16622/35072)\n",
      "274 391 Loss: 1.424 | Acc: 47.403% (16686/35200)\n",
      "275 391 Loss: 1.425 | Acc: 47.396% (16744/35328)\n",
      "276 391 Loss: 1.425 | Acc: 47.383% (16800/35456)\n",
      "277 391 Loss: 1.424 | Acc: 47.389% (16863/35584)\n",
      "278 391 Loss: 1.424 | Acc: 47.404% (16929/35712)\n",
      "279 391 Loss: 1.424 | Acc: 47.425% (16997/35840)\n",
      "280 391 Loss: 1.424 | Acc: 47.417% (17055/35968)\n",
      "281 391 Loss: 1.424 | Acc: 47.443% (17125/36096)\n",
      "282 391 Loss: 1.423 | Acc: 47.463% (17193/36224)\n",
      "283 391 Loss: 1.423 | Acc: 47.483% (17261/36352)\n",
      "284 391 Loss: 1.423 | Acc: 47.500% (17328/36480)\n",
      "285 391 Loss: 1.422 | Acc: 47.511% (17393/36608)\n",
      "286 391 Loss: 1.422 | Acc: 47.528% (17460/36736)\n",
      "287 391 Loss: 1.422 | Acc: 47.542% (17526/36864)\n",
      "288 391 Loss: 1.422 | Acc: 47.554% (17591/36992)\n",
      "289 391 Loss: 1.421 | Acc: 47.573% (17659/37120)\n",
      "290 391 Loss: 1.422 | Acc: 47.554% (17713/37248)\n",
      "291 391 Loss: 1.422 | Acc: 47.552% (17773/37376)\n",
      "292 391 Loss: 1.423 | Acc: 47.510% (17818/37504)\n",
      "293 391 Loss: 1.423 | Acc: 47.529% (17886/37632)\n",
      "294 391 Loss: 1.423 | Acc: 47.537% (17950/37760)\n",
      "295 391 Loss: 1.423 | Acc: 47.530% (18008/37888)\n",
      "296 391 Loss: 1.422 | Acc: 47.538% (18072/38016)\n",
      "297 391 Loss: 1.422 | Acc: 47.536% (18132/38144)\n",
      "298 391 Loss: 1.422 | Acc: 47.533% (18192/38272)\n",
      "299 391 Loss: 1.422 | Acc: 47.534% (18253/38400)\n",
      "300 391 Loss: 1.422 | Acc: 47.537% (18315/38528)\n",
      "301 391 Loss: 1.423 | Acc: 47.524% (18371/38656)\n",
      "302 391 Loss: 1.422 | Acc: 47.540% (18438/38784)\n",
      "303 391 Loss: 1.422 | Acc: 47.556% (18505/38912)\n",
      "304 391 Loss: 1.422 | Acc: 47.572% (18572/39040)\n",
      "305 391 Loss: 1.422 | Acc: 47.575% (18634/39168)\n",
      "306 391 Loss: 1.421 | Acc: 47.600% (18705/39296)\n",
      "307 391 Loss: 1.421 | Acc: 47.585% (18760/39424)\n",
      "308 391 Loss: 1.421 | Acc: 47.603% (18828/39552)\n",
      "309 391 Loss: 1.422 | Acc: 47.586% (18882/39680)\n",
      "310 391 Loss: 1.421 | Acc: 47.601% (18949/39808)\n",
      "311 391 Loss: 1.421 | Acc: 47.609% (19013/39936)\n",
      "312 391 Loss: 1.421 | Acc: 47.621% (19079/40064)\n",
      "313 391 Loss: 1.421 | Acc: 47.619% (19139/40192)\n",
      "314 391 Loss: 1.421 | Acc: 47.604% (19194/40320)\n",
      "315 391 Loss: 1.422 | Acc: 47.614% (19259/40448)\n",
      "316 391 Loss: 1.422 | Acc: 47.639% (19330/40576)\n",
      "317 391 Loss: 1.422 | Acc: 47.646% (19394/40704)\n",
      "318 391 Loss: 1.421 | Acc: 47.642% (19453/40832)\n",
      "319 391 Loss: 1.421 | Acc: 47.651% (19518/40960)\n",
      "320 391 Loss: 1.421 | Acc: 47.661% (19583/41088)\n",
      "321 391 Loss: 1.421 | Acc: 47.671% (19648/41216)\n",
      "322 391 Loss: 1.421 | Acc: 47.644% (19698/41344)\n",
      "323 391 Loss: 1.421 | Acc: 47.627% (19752/41472)\n",
      "324 391 Loss: 1.421 | Acc: 47.618% (19809/41600)\n",
      "325 391 Loss: 1.422 | Acc: 47.618% (19870/41728)\n",
      "326 391 Loss: 1.422 | Acc: 47.623% (19933/41856)\n",
      "327 391 Loss: 1.422 | Acc: 47.630% (19997/41984)\n",
      "328 391 Loss: 1.422 | Acc: 47.635% (20060/42112)\n",
      "329 391 Loss: 1.422 | Acc: 47.635% (20121/42240)\n",
      "330 391 Loss: 1.422 | Acc: 47.644% (20186/42368)\n",
      "331 391 Loss: 1.422 | Acc: 47.654% (20251/42496)\n",
      "332 391 Loss: 1.421 | Acc: 47.684% (20325/42624)\n",
      "333 391 Loss: 1.421 | Acc: 47.708% (20396/42752)\n",
      "334 391 Loss: 1.421 | Acc: 47.703% (20455/42880)\n",
      "335 391 Loss: 1.421 | Acc: 47.693% (20512/43008)\n",
      "336 391 Loss: 1.421 | Acc: 47.728% (20588/43136)\n",
      "337 391 Loss: 1.420 | Acc: 47.744% (20656/43264)\n",
      "338 391 Loss: 1.420 | Acc: 47.744% (20717/43392)\n",
      "339 391 Loss: 1.420 | Acc: 47.732% (20773/43520)\n",
      "340 391 Loss: 1.420 | Acc: 47.762% (20847/43648)\n",
      "341 391 Loss: 1.421 | Acc: 47.738% (20898/43776)\n",
      "342 391 Loss: 1.421 | Acc: 47.736% (20958/43904)\n",
      "343 391 Loss: 1.420 | Acc: 47.743% (21022/44032)\n",
      "344 391 Loss: 1.420 | Acc: 47.765% (21093/44160)\n",
      "345 391 Loss: 1.420 | Acc: 47.776% (21159/44288)\n",
      "346 391 Loss: 1.420 | Acc: 47.776% (21220/44416)\n",
      "347 391 Loss: 1.420 | Acc: 47.793% (21289/44544)\n",
      "348 391 Loss: 1.420 | Acc: 47.793% (21350/44672)\n",
      "349 391 Loss: 1.419 | Acc: 47.806% (21417/44800)\n",
      "350 391 Loss: 1.419 | Acc: 47.834% (21491/44928)\n",
      "351 391 Loss: 1.419 | Acc: 47.840% (21555/45056)\n",
      "352 391 Loss: 1.419 | Acc: 47.853% (21622/45184)\n",
      "353 391 Loss: 1.419 | Acc: 47.855% (21684/45312)\n",
      "354 391 Loss: 1.419 | Acc: 47.857% (21746/45440)\n",
      "355 391 Loss: 1.419 | Acc: 47.856% (21807/45568)\n",
      "356 391 Loss: 1.419 | Acc: 47.864% (21872/45696)\n",
      "357 391 Loss: 1.419 | Acc: 47.870% (21936/45824)\n",
      "358 391 Loss: 1.418 | Acc: 47.907% (22014/45952)\n",
      "359 391 Loss: 1.418 | Acc: 47.925% (22084/46080)\n",
      "360 391 Loss: 1.418 | Acc: 47.935% (22150/46208)\n",
      "361 391 Loss: 1.418 | Acc: 47.917% (22203/46336)\n",
      "362 391 Loss: 1.417 | Acc: 47.945% (22277/46464)\n",
      "363 391 Loss: 1.417 | Acc: 47.948% (22340/46592)\n",
      "364 391 Loss: 1.417 | Acc: 47.947% (22401/46720)\n",
      "365 391 Loss: 1.417 | Acc: 47.938% (22458/46848)\n",
      "366 391 Loss: 1.417 | Acc: 47.927% (22514/46976)\n",
      "367 391 Loss: 1.417 | Acc: 47.936% (22580/47104)\n",
      "368 391 Loss: 1.417 | Acc: 47.944% (22645/47232)\n",
      "369 391 Loss: 1.417 | Acc: 47.969% (22718/47360)\n",
      "370 391 Loss: 1.417 | Acc: 47.976% (22783/47488)\n",
      "371 391 Loss: 1.416 | Acc: 47.975% (22844/47616)\n",
      "372 391 Loss: 1.416 | Acc: 48.010% (22922/47744)\n",
      "373 391 Loss: 1.416 | Acc: 48.018% (22987/47872)\n",
      "374 391 Loss: 1.415 | Acc: 48.038% (23058/48000)\n",
      "375 391 Loss: 1.415 | Acc: 48.041% (23121/48128)\n",
      "376 391 Loss: 1.415 | Acc: 48.019% (23172/48256)\n",
      "377 391 Loss: 1.415 | Acc: 48.041% (23244/48384)\n",
      "378 391 Loss: 1.415 | Acc: 48.048% (23309/48512)\n",
      "379 391 Loss: 1.414 | Acc: 48.065% (23379/48640)\n",
      "380 391 Loss: 1.415 | Acc: 48.066% (23441/48768)\n",
      "381 391 Loss: 1.415 | Acc: 48.061% (23500/48896)\n",
      "382 391 Loss: 1.415 | Acc: 48.050% (23556/49024)\n",
      "383 391 Loss: 1.415 | Acc: 48.055% (23620/49152)\n",
      "384 391 Loss: 1.415 | Acc: 48.074% (23691/49280)\n",
      "385 391 Loss: 1.415 | Acc: 48.075% (23753/49408)\n",
      "386 391 Loss: 1.414 | Acc: 48.086% (23820/49536)\n",
      "387 391 Loss: 1.414 | Acc: 48.085% (23881/49664)\n",
      "388 391 Loss: 1.414 | Acc: 48.088% (23944/49792)\n",
      "389 391 Loss: 1.414 | Acc: 48.099% (24011/49920)\n",
      "390 391 Loss: 1.414 | Acc: 48.098% (24049/50000)\n",
      "0 100 Loss: 1.303 | Acc: 50.000% (50/100)\n",
      "1 100 Loss: 1.335 | Acc: 47.000% (94/200)\n",
      "2 100 Loss: 1.342 | Acc: 47.333% (142/300)\n",
      "3 100 Loss: 1.383 | Acc: 47.250% (189/400)\n",
      "4 100 Loss: 1.375 | Acc: 47.200% (236/500)\n",
      "5 100 Loss: 1.383 | Acc: 47.500% (285/600)\n",
      "6 100 Loss: 1.387 | Acc: 47.714% (334/700)\n",
      "7 100 Loss: 1.390 | Acc: 48.125% (385/800)\n",
      "8 100 Loss: 1.400 | Acc: 48.111% (433/900)\n",
      "9 100 Loss: 1.386 | Acc: 48.600% (486/1000)\n",
      "10 100 Loss: 1.374 | Acc: 49.273% (542/1100)\n",
      "11 100 Loss: 1.371 | Acc: 49.167% (590/1200)\n",
      "12 100 Loss: 1.384 | Acc: 49.308% (641/1300)\n",
      "13 100 Loss: 1.391 | Acc: 48.857% (684/1400)\n",
      "14 100 Loss: 1.380 | Acc: 49.067% (736/1500)\n",
      "15 100 Loss: 1.379 | Acc: 49.125% (786/1600)\n",
      "16 100 Loss: 1.377 | Acc: 49.529% (842/1700)\n",
      "17 100 Loss: 1.370 | Acc: 49.722% (895/1800)\n",
      "18 100 Loss: 1.368 | Acc: 50.000% (950/1900)\n",
      "19 100 Loss: 1.372 | Acc: 49.650% (993/2000)\n",
      "20 100 Loss: 1.367 | Acc: 49.810% (1046/2100)\n",
      "21 100 Loss: 1.366 | Acc: 50.045% (1101/2200)\n",
      "22 100 Loss: 1.364 | Acc: 49.913% (1148/2300)\n",
      "23 100 Loss: 1.363 | Acc: 50.042% (1201/2400)\n",
      "24 100 Loss: 1.361 | Acc: 50.160% (1254/2500)\n",
      "25 100 Loss: 1.373 | Acc: 49.615% (1290/2600)\n",
      "26 100 Loss: 1.371 | Acc: 49.778% (1344/2700)\n",
      "27 100 Loss: 1.373 | Acc: 49.464% (1385/2800)\n",
      "28 100 Loss: 1.374 | Acc: 49.483% (1435/2900)\n",
      "29 100 Loss: 1.369 | Acc: 49.667% (1490/3000)\n",
      "30 100 Loss: 1.364 | Acc: 49.903% (1547/3100)\n",
      "31 100 Loss: 1.361 | Acc: 49.844% (1595/3200)\n",
      "32 100 Loss: 1.360 | Acc: 49.939% (1648/3300)\n",
      "33 100 Loss: 1.360 | Acc: 50.059% (1702/3400)\n",
      "34 100 Loss: 1.360 | Acc: 50.057% (1752/3500)\n",
      "35 100 Loss: 1.358 | Acc: 50.167% (1806/3600)\n",
      "36 100 Loss: 1.360 | Acc: 50.189% (1857/3700)\n",
      "37 100 Loss: 1.362 | Acc: 49.947% (1898/3800)\n",
      "38 100 Loss: 1.361 | Acc: 50.026% (1951/3900)\n",
      "39 100 Loss: 1.363 | Acc: 49.925% (1997/4000)\n",
      "40 100 Loss: 1.365 | Acc: 49.951% (2048/4100)\n",
      "41 100 Loss: 1.366 | Acc: 49.952% (2098/4200)\n",
      "42 100 Loss: 1.363 | Acc: 50.116% (2155/4300)\n",
      "43 100 Loss: 1.360 | Acc: 50.205% (2209/4400)\n",
      "44 100 Loss: 1.359 | Acc: 50.333% (2265/4500)\n",
      "45 100 Loss: 1.357 | Acc: 50.457% (2321/4600)\n",
      "46 100 Loss: 1.353 | Acc: 50.596% (2378/4700)\n",
      "47 100 Loss: 1.355 | Acc: 50.604% (2429/4800)\n",
      "48 100 Loss: 1.353 | Acc: 50.694% (2484/4900)\n",
      "49 100 Loss: 1.351 | Acc: 50.760% (2538/5000)\n",
      "50 100 Loss: 1.351 | Acc: 50.706% (2586/5100)\n",
      "51 100 Loss: 1.350 | Acc: 50.692% (2636/5200)\n",
      "52 100 Loss: 1.350 | Acc: 50.623% (2683/5300)\n",
      "53 100 Loss: 1.353 | Acc: 50.519% (2728/5400)\n",
      "54 100 Loss: 1.355 | Acc: 50.382% (2771/5500)\n",
      "55 100 Loss: 1.359 | Acc: 50.250% (2814/5600)\n",
      "56 100 Loss: 1.359 | Acc: 50.351% (2870/5700)\n",
      "57 100 Loss: 1.354 | Acc: 50.500% (2929/5800)\n",
      "58 100 Loss: 1.354 | Acc: 50.492% (2979/5900)\n",
      "59 100 Loss: 1.353 | Acc: 50.500% (3030/6000)\n",
      "60 100 Loss: 1.354 | Acc: 50.426% (3076/6100)\n",
      "61 100 Loss: 1.354 | Acc: 50.387% (3124/6200)\n",
      "62 100 Loss: 1.352 | Acc: 50.476% (3180/6300)\n",
      "63 100 Loss: 1.353 | Acc: 50.484% (3231/6400)\n",
      "64 100 Loss: 1.353 | Acc: 50.400% (3276/6500)\n",
      "65 100 Loss: 1.353 | Acc: 50.470% (3331/6600)\n",
      "66 100 Loss: 1.351 | Acc: 50.522% (3385/6700)\n",
      "67 100 Loss: 1.352 | Acc: 50.574% (3439/6800)\n",
      "68 100 Loss: 1.352 | Acc: 50.493% (3484/6900)\n",
      "69 100 Loss: 1.354 | Acc: 50.429% (3530/7000)\n",
      "70 100 Loss: 1.355 | Acc: 50.366% (3576/7100)\n",
      "71 100 Loss: 1.355 | Acc: 50.389% (3628/7200)\n",
      "72 100 Loss: 1.354 | Acc: 50.370% (3677/7300)\n",
      "73 100 Loss: 1.352 | Acc: 50.432% (3732/7400)\n",
      "74 100 Loss: 1.353 | Acc: 50.347% (3776/7500)\n",
      "75 100 Loss: 1.352 | Acc: 50.355% (3827/7600)\n",
      "76 100 Loss: 1.352 | Acc: 50.325% (3875/7700)\n",
      "77 100 Loss: 1.351 | Acc: 50.321% (3925/7800)\n",
      "78 100 Loss: 1.349 | Acc: 50.380% (3980/7900)\n",
      "79 100 Loss: 1.350 | Acc: 50.337% (4027/8000)\n",
      "80 100 Loss: 1.350 | Acc: 50.358% (4079/8100)\n",
      "81 100 Loss: 1.351 | Acc: 50.268% (4122/8200)\n",
      "82 100 Loss: 1.352 | Acc: 50.241% (4170/8300)\n",
      "83 100 Loss: 1.353 | Acc: 50.167% (4214/8400)\n",
      "84 100 Loss: 1.354 | Acc: 50.118% (4260/8500)\n",
      "85 100 Loss: 1.353 | Acc: 50.163% (4314/8600)\n",
      "86 100 Loss: 1.354 | Acc: 50.103% (4359/8700)\n",
      "87 100 Loss: 1.354 | Acc: 50.159% (4414/8800)\n",
      "88 100 Loss: 1.355 | Acc: 50.157% (4464/8900)\n",
      "89 100 Loss: 1.354 | Acc: 50.200% (4518/9000)\n",
      "90 100 Loss: 1.353 | Acc: 50.121% (4561/9100)\n",
      "91 100 Loss: 1.352 | Acc: 50.163% (4615/9200)\n",
      "92 100 Loss: 1.351 | Acc: 50.247% (4673/9300)\n",
      "93 100 Loss: 1.352 | Acc: 50.255% (4724/9400)\n",
      "94 100 Loss: 1.352 | Acc: 50.242% (4773/9500)\n",
      "95 100 Loss: 1.351 | Acc: 50.250% (4824/9600)\n",
      "96 100 Loss: 1.351 | Acc: 50.278% (4877/9700)\n",
      "97 100 Loss: 1.352 | Acc: 50.204% (4920/9800)\n",
      "98 100 Loss: 1.353 | Acc: 50.152% (4965/9900)\n",
      "99 100 Loss: 1.353 | Acc: 50.100% (5010/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 3\n",
      "0 391 Loss: 1.417 | Acc: 48.438% (62/128)\n",
      "1 391 Loss: 1.468 | Acc: 45.703% (117/256)\n",
      "2 391 Loss: 1.416 | Acc: 47.396% (182/384)\n",
      "3 391 Loss: 1.380 | Acc: 48.047% (246/512)\n",
      "4 391 Loss: 1.379 | Acc: 48.750% (312/640)\n",
      "5 391 Loss: 1.406 | Acc: 48.047% (369/768)\n",
      "6 391 Loss: 1.383 | Acc: 48.996% (439/896)\n",
      "7 391 Loss: 1.375 | Acc: 49.609% (508/1024)\n",
      "8 391 Loss: 1.383 | Acc: 48.958% (564/1152)\n",
      "9 391 Loss: 1.397 | Acc: 48.516% (621/1280)\n",
      "10 391 Loss: 1.384 | Acc: 49.148% (692/1408)\n",
      "11 391 Loss: 1.382 | Acc: 49.284% (757/1536)\n",
      "12 391 Loss: 1.384 | Acc: 48.858% (813/1664)\n",
      "13 391 Loss: 1.378 | Acc: 49.275% (883/1792)\n",
      "14 391 Loss: 1.378 | Acc: 49.427% (949/1920)\n",
      "15 391 Loss: 1.368 | Acc: 49.756% (1019/2048)\n",
      "16 391 Loss: 1.366 | Acc: 49.678% (1081/2176)\n",
      "17 391 Loss: 1.365 | Acc: 49.826% (1148/2304)\n",
      "18 391 Loss: 1.369 | Acc: 49.465% (1203/2432)\n",
      "19 391 Loss: 1.370 | Acc: 49.727% (1273/2560)\n",
      "20 391 Loss: 1.365 | Acc: 50.260% (1351/2688)\n",
      "21 391 Loss: 1.364 | Acc: 50.568% (1424/2816)\n",
      "22 391 Loss: 1.361 | Acc: 50.713% (1493/2944)\n",
      "23 391 Loss: 1.363 | Acc: 50.716% (1558/3072)\n",
      "24 391 Loss: 1.362 | Acc: 50.781% (1625/3200)\n",
      "25 391 Loss: 1.366 | Acc: 50.571% (1683/3328)\n",
      "26 391 Loss: 1.366 | Acc: 50.521% (1746/3456)\n",
      "27 391 Loss: 1.365 | Acc: 50.558% (1812/3584)\n",
      "28 391 Loss: 1.362 | Acc: 50.700% (1882/3712)\n",
      "29 391 Loss: 1.362 | Acc: 50.651% (1945/3840)\n",
      "30 391 Loss: 1.368 | Acc: 50.479% (2003/3968)\n",
      "31 391 Loss: 1.370 | Acc: 50.293% (2060/4096)\n",
      "32 391 Loss: 1.371 | Acc: 50.142% (2118/4224)\n",
      "33 391 Loss: 1.369 | Acc: 50.092% (2180/4352)\n",
      "34 391 Loss: 1.370 | Acc: 50.045% (2242/4480)\n",
      "35 391 Loss: 1.372 | Acc: 50.000% (2304/4608)\n",
      "36 391 Loss: 1.372 | Acc: 49.916% (2364/4736)\n",
      "37 391 Loss: 1.372 | Acc: 49.897% (2427/4864)\n",
      "38 391 Loss: 1.372 | Acc: 49.820% (2487/4992)\n",
      "39 391 Loss: 1.374 | Acc: 49.688% (2544/5120)\n",
      "40 391 Loss: 1.374 | Acc: 49.714% (2609/5248)\n",
      "41 391 Loss: 1.374 | Acc: 49.554% (2664/5376)\n",
      "42 391 Loss: 1.378 | Acc: 49.364% (2717/5504)\n",
      "43 391 Loss: 1.379 | Acc: 49.361% (2780/5632)\n",
      "44 391 Loss: 1.379 | Acc: 49.288% (2839/5760)\n",
      "45 391 Loss: 1.379 | Acc: 49.440% (2911/5888)\n",
      "46 391 Loss: 1.381 | Acc: 49.352% (2969/6016)\n",
      "47 391 Loss: 1.381 | Acc: 49.463% (3039/6144)\n",
      "48 391 Loss: 1.380 | Acc: 49.442% (3101/6272)\n",
      "49 391 Loss: 1.380 | Acc: 49.453% (3165/6400)\n",
      "50 391 Loss: 1.380 | Acc: 49.418% (3226/6528)\n",
      "51 391 Loss: 1.382 | Acc: 49.309% (3282/6656)\n",
      "52 391 Loss: 1.381 | Acc: 49.366% (3349/6784)\n",
      "53 391 Loss: 1.378 | Acc: 49.450% (3418/6912)\n",
      "54 391 Loss: 1.378 | Acc: 49.460% (3482/7040)\n",
      "55 391 Loss: 1.378 | Acc: 49.428% (3543/7168)\n",
      "56 391 Loss: 1.376 | Acc: 49.424% (3606/7296)\n",
      "57 391 Loss: 1.376 | Acc: 49.421% (3669/7424)\n",
      "58 391 Loss: 1.374 | Acc: 49.523% (3740/7552)\n",
      "59 391 Loss: 1.374 | Acc: 49.531% (3804/7680)\n",
      "60 391 Loss: 1.375 | Acc: 49.475% (3863/7808)\n",
      "61 391 Loss: 1.375 | Acc: 49.471% (3926/7936)\n",
      "62 391 Loss: 1.373 | Acc: 49.578% (3998/8064)\n",
      "63 391 Loss: 1.373 | Acc: 49.585% (4062/8192)\n",
      "64 391 Loss: 1.373 | Acc: 49.615% (4128/8320)\n",
      "65 391 Loss: 1.375 | Acc: 49.550% (4186/8448)\n",
      "66 391 Loss: 1.374 | Acc: 49.557% (4250/8576)\n",
      "67 391 Loss: 1.374 | Acc: 49.621% (4319/8704)\n",
      "68 391 Loss: 1.375 | Acc: 49.615% (4382/8832)\n",
      "69 391 Loss: 1.373 | Acc: 49.732% (4456/8960)\n",
      "70 391 Loss: 1.374 | Acc: 49.670% (4514/9088)\n",
      "71 391 Loss: 1.376 | Acc: 49.653% (4576/9216)\n",
      "72 391 Loss: 1.375 | Acc: 49.690% (4643/9344)\n",
      "73 391 Loss: 1.375 | Acc: 49.662% (4704/9472)\n",
      "74 391 Loss: 1.374 | Acc: 49.719% (4773/9600)\n",
      "75 391 Loss: 1.374 | Acc: 49.609% (4826/9728)\n",
      "76 391 Loss: 1.375 | Acc: 49.594% (4888/9856)\n",
      "77 391 Loss: 1.373 | Acc: 49.629% (4955/9984)\n",
      "78 391 Loss: 1.373 | Acc: 49.644% (5020/10112)\n",
      "79 391 Loss: 1.373 | Acc: 49.678% (5087/10240)\n",
      "80 391 Loss: 1.373 | Acc: 49.614% (5144/10368)\n",
      "81 391 Loss: 1.373 | Acc: 49.600% (5206/10496)\n",
      "82 391 Loss: 1.373 | Acc: 49.671% (5277/10624)\n",
      "83 391 Loss: 1.373 | Acc: 49.665% (5340/10752)\n",
      "84 391 Loss: 1.372 | Acc: 49.697% (5407/10880)\n",
      "85 391 Loss: 1.371 | Acc: 49.737% (5475/11008)\n",
      "86 391 Loss: 1.370 | Acc: 49.856% (5552/11136)\n",
      "87 391 Loss: 1.371 | Acc: 49.796% (5609/11264)\n",
      "88 391 Loss: 1.371 | Acc: 49.772% (5670/11392)\n",
      "89 391 Loss: 1.370 | Acc: 49.818% (5739/11520)\n",
      "90 391 Loss: 1.372 | Acc: 49.803% (5801/11648)\n",
      "91 391 Loss: 1.370 | Acc: 49.856% (5871/11776)\n",
      "92 391 Loss: 1.370 | Acc: 49.840% (5933/11904)\n",
      "93 391 Loss: 1.371 | Acc: 49.834% (5996/12032)\n",
      "94 391 Loss: 1.370 | Acc: 49.704% (6044/12160)\n",
      "95 391 Loss: 1.372 | Acc: 49.658% (6102/12288)\n",
      "96 391 Loss: 1.370 | Acc: 49.718% (6173/12416)\n",
      "97 391 Loss: 1.372 | Acc: 49.601% (6222/12544)\n",
      "98 391 Loss: 1.371 | Acc: 49.621% (6288/12672)\n",
      "99 391 Loss: 1.372 | Acc: 49.617% (6351/12800)\n",
      "100 391 Loss: 1.372 | Acc: 49.606% (6413/12928)\n",
      "101 391 Loss: 1.373 | Acc: 49.609% (6477/13056)\n",
      "102 391 Loss: 1.373 | Acc: 49.590% (6538/13184)\n",
      "103 391 Loss: 1.374 | Acc: 49.602% (6603/13312)\n",
      "104 391 Loss: 1.375 | Acc: 49.531% (6657/13440)\n",
      "105 391 Loss: 1.375 | Acc: 49.484% (6714/13568)\n",
      "106 391 Loss: 1.375 | Acc: 49.460% (6774/13696)\n",
      "107 391 Loss: 1.376 | Acc: 49.457% (6837/13824)\n",
      "108 391 Loss: 1.377 | Acc: 49.405% (6893/13952)\n",
      "109 391 Loss: 1.376 | Acc: 49.446% (6962/14080)\n",
      "110 391 Loss: 1.376 | Acc: 49.507% (7034/14208)\n",
      "111 391 Loss: 1.376 | Acc: 49.512% (7098/14336)\n",
      "112 391 Loss: 1.377 | Acc: 49.516% (7162/14464)\n",
      "113 391 Loss: 1.378 | Acc: 49.438% (7214/14592)\n",
      "114 391 Loss: 1.377 | Acc: 49.457% (7280/14720)\n",
      "115 391 Loss: 1.376 | Acc: 49.529% (7354/14848)\n",
      "116 391 Loss: 1.376 | Acc: 49.613% (7430/14976)\n",
      "117 391 Loss: 1.375 | Acc: 49.642% (7498/15104)\n",
      "118 391 Loss: 1.374 | Acc: 49.645% (7562/15232)\n",
      "119 391 Loss: 1.374 | Acc: 49.668% (7629/15360)\n",
      "120 391 Loss: 1.374 | Acc: 49.729% (7702/15488)\n",
      "121 391 Loss: 1.375 | Acc: 49.737% (7767/15616)\n",
      "122 391 Loss: 1.375 | Acc: 49.708% (7826/15744)\n",
      "123 391 Loss: 1.373 | Acc: 49.779% (7901/15872)\n",
      "124 391 Loss: 1.373 | Acc: 49.831% (7973/16000)\n",
      "125 391 Loss: 1.372 | Acc: 49.870% (8043/16128)\n",
      "126 391 Loss: 1.371 | Acc: 49.902% (8112/16256)\n",
      "127 391 Loss: 1.371 | Acc: 49.915% (8178/16384)\n",
      "128 391 Loss: 1.372 | Acc: 49.812% (8225/16512)\n",
      "129 391 Loss: 1.372 | Acc: 49.838% (8293/16640)\n",
      "130 391 Loss: 1.372 | Acc: 49.845% (8358/16768)\n",
      "131 391 Loss: 1.372 | Acc: 49.840% (8421/16896)\n",
      "132 391 Loss: 1.372 | Acc: 49.865% (8489/17024)\n",
      "133 391 Loss: 1.371 | Acc: 49.907% (8560/17152)\n",
      "134 391 Loss: 1.371 | Acc: 49.936% (8629/17280)\n",
      "135 391 Loss: 1.371 | Acc: 49.920% (8690/17408)\n",
      "136 391 Loss: 1.371 | Acc: 49.880% (8747/17536)\n",
      "137 391 Loss: 1.372 | Acc: 49.875% (8810/17664)\n",
      "138 391 Loss: 1.371 | Acc: 49.938% (8885/17792)\n",
      "139 391 Loss: 1.370 | Acc: 49.939% (8949/17920)\n",
      "140 391 Loss: 1.370 | Acc: 49.961% (9017/18048)\n",
      "141 391 Loss: 1.370 | Acc: 49.961% (9081/18176)\n",
      "142 391 Loss: 1.369 | Acc: 50.022% (9156/18304)\n",
      "143 391 Loss: 1.368 | Acc: 50.054% (9226/18432)\n",
      "144 391 Loss: 1.369 | Acc: 49.973% (9275/18560)\n",
      "145 391 Loss: 1.369 | Acc: 50.005% (9345/18688)\n",
      "146 391 Loss: 1.369 | Acc: 49.973% (9403/18816)\n",
      "147 391 Loss: 1.369 | Acc: 49.968% (9466/18944)\n",
      "148 391 Loss: 1.370 | Acc: 49.932% (9523/19072)\n",
      "149 391 Loss: 1.369 | Acc: 49.969% (9594/19200)\n",
      "150 391 Loss: 1.368 | Acc: 49.979% (9660/19328)\n",
      "151 391 Loss: 1.368 | Acc: 49.979% (9724/19456)\n",
      "152 391 Loss: 1.369 | Acc: 49.954% (9783/19584)\n",
      "153 391 Loss: 1.369 | Acc: 49.934% (9843/19712)\n",
      "154 391 Loss: 1.368 | Acc: 49.950% (9910/19840)\n",
      "155 391 Loss: 1.369 | Acc: 49.880% (9960/19968)\n",
      "156 391 Loss: 1.369 | Acc: 49.846% (10017/20096)\n",
      "157 391 Loss: 1.370 | Acc: 49.807% (10073/20224)\n",
      "158 391 Loss: 1.369 | Acc: 49.853% (10146/20352)\n",
      "159 391 Loss: 1.369 | Acc: 49.873% (10214/20480)\n",
      "160 391 Loss: 1.370 | Acc: 49.869% (10277/20608)\n",
      "161 391 Loss: 1.369 | Acc: 49.889% (10345/20736)\n",
      "162 391 Loss: 1.369 | Acc: 49.890% (10409/20864)\n",
      "163 391 Loss: 1.369 | Acc: 49.909% (10477/20992)\n",
      "164 391 Loss: 1.369 | Acc: 49.910% (10541/21120)\n",
      "165 391 Loss: 1.369 | Acc: 49.915% (10606/21248)\n",
      "166 391 Loss: 1.368 | Acc: 49.925% (10672/21376)\n",
      "167 391 Loss: 1.368 | Acc: 49.953% (10742/21504)\n",
      "168 391 Loss: 1.367 | Acc: 49.940% (10803/21632)\n",
      "169 391 Loss: 1.367 | Acc: 49.945% (10868/21760)\n",
      "170 391 Loss: 1.366 | Acc: 49.977% (10939/21888)\n",
      "171 391 Loss: 1.367 | Acc: 49.968% (11001/22016)\n",
      "172 391 Loss: 1.366 | Acc: 49.982% (11068/22144)\n",
      "173 391 Loss: 1.366 | Acc: 49.991% (11134/22272)\n",
      "174 391 Loss: 1.366 | Acc: 50.004% (11201/22400)\n",
      "175 391 Loss: 1.367 | Acc: 49.978% (11259/22528)\n",
      "176 391 Loss: 1.367 | Acc: 49.978% (11323/22656)\n",
      "177 391 Loss: 1.366 | Acc: 50.004% (11393/22784)\n",
      "178 391 Loss: 1.367 | Acc: 49.983% (11452/22912)\n",
      "179 391 Loss: 1.367 | Acc: 49.952% (11509/23040)\n",
      "180 391 Loss: 1.367 | Acc: 49.940% (11570/23168)\n",
      "181 391 Loss: 1.367 | Acc: 49.940% (11634/23296)\n",
      "182 391 Loss: 1.367 | Acc: 49.932% (11696/23424)\n",
      "183 391 Loss: 1.367 | Acc: 49.936% (11761/23552)\n",
      "184 391 Loss: 1.367 | Acc: 49.907% (11818/23680)\n",
      "185 391 Loss: 1.366 | Acc: 49.929% (11887/23808)\n",
      "186 391 Loss: 1.366 | Acc: 49.946% (11955/23936)\n",
      "187 391 Loss: 1.366 | Acc: 49.925% (12014/24064)\n",
      "188 391 Loss: 1.366 | Acc: 49.946% (12083/24192)\n",
      "189 391 Loss: 1.365 | Acc: 49.984% (12156/24320)\n",
      "190 391 Loss: 1.364 | Acc: 50.000% (12224/24448)\n",
      "191 391 Loss: 1.364 | Acc: 49.992% (12286/24576)\n",
      "192 391 Loss: 1.364 | Acc: 50.008% (12354/24704)\n",
      "193 391 Loss: 1.364 | Acc: 49.960% (12406/24832)\n",
      "194 391 Loss: 1.364 | Acc: 49.952% (12468/24960)\n",
      "195 391 Loss: 1.364 | Acc: 49.960% (12534/25088)\n",
      "196 391 Loss: 1.363 | Acc: 49.972% (12601/25216)\n",
      "197 391 Loss: 1.363 | Acc: 49.964% (12663/25344)\n",
      "198 391 Loss: 1.363 | Acc: 49.957% (12725/25472)\n",
      "199 391 Loss: 1.364 | Acc: 49.945% (12786/25600)\n",
      "200 391 Loss: 1.364 | Acc: 49.942% (12849/25728)\n",
      "201 391 Loss: 1.364 | Acc: 49.930% (12910/25856)\n",
      "202 391 Loss: 1.365 | Acc: 49.904% (12967/25984)\n",
      "203 391 Loss: 1.365 | Acc: 49.889% (13027/26112)\n",
      "204 391 Loss: 1.365 | Acc: 49.909% (13096/26240)\n",
      "205 391 Loss: 1.365 | Acc: 49.917% (13162/26368)\n",
      "206 391 Loss: 1.365 | Acc: 49.917% (13226/26496)\n",
      "207 391 Loss: 1.365 | Acc: 49.932% (13294/26624)\n",
      "208 391 Loss: 1.365 | Acc: 49.922% (13355/26752)\n",
      "209 391 Loss: 1.364 | Acc: 49.926% (13420/26880)\n",
      "210 391 Loss: 1.364 | Acc: 49.941% (13488/27008)\n",
      "211 391 Loss: 1.365 | Acc: 49.882% (13536/27136)\n",
      "212 391 Loss: 1.364 | Acc: 49.890% (13602/27264)\n",
      "213 391 Loss: 1.364 | Acc: 49.890% (13666/27392)\n",
      "214 391 Loss: 1.364 | Acc: 49.895% (13731/27520)\n",
      "215 391 Loss: 1.364 | Acc: 49.888% (13793/27648)\n",
      "216 391 Loss: 1.364 | Acc: 49.874% (13853/27776)\n",
      "217 391 Loss: 1.364 | Acc: 49.907% (13926/27904)\n",
      "218 391 Loss: 1.363 | Acc: 49.954% (14003/28032)\n",
      "219 391 Loss: 1.363 | Acc: 49.964% (14070/28160)\n",
      "220 391 Loss: 1.363 | Acc: 49.961% (14133/28288)\n",
      "221 391 Loss: 1.362 | Acc: 49.986% (14204/28416)\n",
      "222 391 Loss: 1.363 | Acc: 50.014% (14276/28544)\n",
      "223 391 Loss: 1.362 | Acc: 50.028% (14344/28672)\n",
      "224 391 Loss: 1.363 | Acc: 50.031% (14409/28800)\n",
      "225 391 Loss: 1.363 | Acc: 50.021% (14470/28928)\n",
      "226 391 Loss: 1.363 | Acc: 50.017% (14533/29056)\n",
      "227 391 Loss: 1.363 | Acc: 50.003% (14593/29184)\n",
      "228 391 Loss: 1.364 | Acc: 49.986% (14652/29312)\n",
      "229 391 Loss: 1.364 | Acc: 49.973% (14712/29440)\n",
      "230 391 Loss: 1.364 | Acc: 49.963% (14773/29568)\n",
      "231 391 Loss: 1.364 | Acc: 49.976% (14841/29696)\n",
      "232 391 Loss: 1.364 | Acc: 50.013% (14916/29824)\n",
      "233 391 Loss: 1.364 | Acc: 50.007% (14978/29952)\n",
      "234 391 Loss: 1.364 | Acc: 50.013% (15044/30080)\n",
      "235 391 Loss: 1.364 | Acc: 50.003% (15105/30208)\n",
      "236 391 Loss: 1.364 | Acc: 50.010% (15171/30336)\n",
      "237 391 Loss: 1.365 | Acc: 50.016% (15237/30464)\n",
      "238 391 Loss: 1.365 | Acc: 50.000% (15296/30592)\n",
      "239 391 Loss: 1.365 | Acc: 49.990% (15357/30720)\n",
      "240 391 Loss: 1.365 | Acc: 49.990% (15421/30848)\n",
      "241 391 Loss: 1.365 | Acc: 50.010% (15491/30976)\n",
      "242 391 Loss: 1.364 | Acc: 50.032% (15562/31104)\n",
      "243 391 Loss: 1.364 | Acc: 50.048% (15631/31232)\n",
      "244 391 Loss: 1.363 | Acc: 50.077% (15704/31360)\n",
      "245 391 Loss: 1.363 | Acc: 50.098% (15775/31488)\n",
      "246 391 Loss: 1.363 | Acc: 50.095% (15838/31616)\n",
      "247 391 Loss: 1.363 | Acc: 50.104% (15905/31744)\n",
      "248 391 Loss: 1.363 | Acc: 50.085% (15963/31872)\n",
      "249 391 Loss: 1.363 | Acc: 50.112% (16036/32000)\n",
      "250 391 Loss: 1.363 | Acc: 50.118% (16102/32128)\n",
      "251 391 Loss: 1.363 | Acc: 50.127% (16169/32256)\n",
      "252 391 Loss: 1.363 | Acc: 50.120% (16231/32384)\n",
      "253 391 Loss: 1.364 | Acc: 50.105% (16290/32512)\n",
      "254 391 Loss: 1.363 | Acc: 50.135% (16364/32640)\n",
      "255 391 Loss: 1.363 | Acc: 50.131% (16427/32768)\n",
      "256 391 Loss: 1.363 | Acc: 50.149% (16497/32896)\n",
      "257 391 Loss: 1.363 | Acc: 50.124% (16553/33024)\n",
      "258 391 Loss: 1.363 | Acc: 50.121% (16616/33152)\n",
      "259 391 Loss: 1.363 | Acc: 50.123% (16681/33280)\n",
      "260 391 Loss: 1.363 | Acc: 50.111% (16741/33408)\n",
      "261 391 Loss: 1.362 | Acc: 50.113% (16806/33536)\n",
      "262 391 Loss: 1.362 | Acc: 50.128% (16875/33664)\n",
      "263 391 Loss: 1.362 | Acc: 50.133% (16941/33792)\n",
      "264 391 Loss: 1.362 | Acc: 50.130% (17004/33920)\n",
      "265 391 Loss: 1.363 | Acc: 50.115% (17063/34048)\n",
      "266 391 Loss: 1.362 | Acc: 50.120% (17129/34176)\n",
      "267 391 Loss: 1.363 | Acc: 50.120% (17193/34304)\n",
      "268 391 Loss: 1.363 | Acc: 50.137% (17263/34432)\n",
      "269 391 Loss: 1.362 | Acc: 50.139% (17328/34560)\n",
      "270 391 Loss: 1.362 | Acc: 50.156% (17398/34688)\n",
      "271 391 Loss: 1.362 | Acc: 50.181% (17471/34816)\n",
      "272 391 Loss: 1.362 | Acc: 50.180% (17535/34944)\n",
      "273 391 Loss: 1.361 | Acc: 50.185% (17601/35072)\n",
      "274 391 Loss: 1.361 | Acc: 50.193% (17668/35200)\n",
      "275 391 Loss: 1.361 | Acc: 50.207% (17737/35328)\n",
      "276 391 Loss: 1.361 | Acc: 50.203% (17800/35456)\n",
      "277 391 Loss: 1.361 | Acc: 50.208% (17866/35584)\n",
      "278 391 Loss: 1.360 | Acc: 50.232% (17939/35712)\n",
      "279 391 Loss: 1.361 | Acc: 50.229% (18002/35840)\n",
      "280 391 Loss: 1.360 | Acc: 50.247% (18073/35968)\n",
      "281 391 Loss: 1.360 | Acc: 50.258% (18141/36096)\n",
      "282 391 Loss: 1.360 | Acc: 50.251% (18203/36224)\n",
      "283 391 Loss: 1.360 | Acc: 50.275% (18276/36352)\n",
      "284 391 Loss: 1.360 | Acc: 50.269% (18338/36480)\n",
      "285 391 Loss: 1.360 | Acc: 50.292% (18411/36608)\n",
      "286 391 Loss: 1.360 | Acc: 50.286% (18473/36736)\n",
      "287 391 Loss: 1.359 | Acc: 50.304% (18544/36864)\n",
      "288 391 Loss: 1.360 | Acc: 50.289% (18603/36992)\n",
      "289 391 Loss: 1.359 | Acc: 50.280% (18664/37120)\n",
      "290 391 Loss: 1.360 | Acc: 50.277% (18727/37248)\n",
      "291 391 Loss: 1.360 | Acc: 50.276% (18791/37376)\n",
      "292 391 Loss: 1.360 | Acc: 50.269% (18853/37504)\n",
      "293 391 Loss: 1.360 | Acc: 50.239% (18906/37632)\n",
      "294 391 Loss: 1.360 | Acc: 50.215% (18961/37760)\n",
      "295 391 Loss: 1.360 | Acc: 50.235% (19033/37888)\n",
      "296 391 Loss: 1.360 | Acc: 50.250% (19103/38016)\n",
      "297 391 Loss: 1.360 | Acc: 50.254% (19169/38144)\n",
      "298 391 Loss: 1.360 | Acc: 50.269% (19239/38272)\n",
      "299 391 Loss: 1.360 | Acc: 50.258% (19299/38400)\n",
      "300 391 Loss: 1.360 | Acc: 50.278% (19371/38528)\n",
      "301 391 Loss: 1.360 | Acc: 50.272% (19433/38656)\n",
      "302 391 Loss: 1.360 | Acc: 50.263% (19494/38784)\n",
      "303 391 Loss: 1.360 | Acc: 50.254% (19555/38912)\n",
      "304 391 Loss: 1.360 | Acc: 50.269% (19625/39040)\n",
      "305 391 Loss: 1.359 | Acc: 50.289% (19697/39168)\n",
      "306 391 Loss: 1.359 | Acc: 50.295% (19764/39296)\n",
      "307 391 Loss: 1.359 | Acc: 50.284% (19824/39424)\n",
      "308 391 Loss: 1.359 | Acc: 50.268% (19882/39552)\n",
      "309 391 Loss: 1.359 | Acc: 50.287% (19954/39680)\n",
      "310 391 Loss: 1.360 | Acc: 50.271% (20012/39808)\n",
      "311 391 Loss: 1.360 | Acc: 50.295% (20086/39936)\n",
      "312 391 Loss: 1.360 | Acc: 50.287% (20147/40064)\n",
      "313 391 Loss: 1.360 | Acc: 50.276% (20207/40192)\n",
      "314 391 Loss: 1.360 | Acc: 50.285% (20275/40320)\n",
      "315 391 Loss: 1.360 | Acc: 50.304% (20347/40448)\n",
      "316 391 Loss: 1.360 | Acc: 50.313% (20415/40576)\n",
      "317 391 Loss: 1.360 | Acc: 50.302% (20475/40704)\n",
      "318 391 Loss: 1.360 | Acc: 50.274% (20528/40832)\n",
      "319 391 Loss: 1.360 | Acc: 50.286% (20597/40960)\n",
      "320 391 Loss: 1.360 | Acc: 50.275% (20657/41088)\n",
      "321 391 Loss: 1.360 | Acc: 50.279% (20723/41216)\n",
      "322 391 Loss: 1.360 | Acc: 50.285% (20790/41344)\n",
      "323 391 Loss: 1.360 | Acc: 50.280% (20852/41472)\n",
      "324 391 Loss: 1.360 | Acc: 50.298% (20924/41600)\n",
      "325 391 Loss: 1.360 | Acc: 50.290% (20985/41728)\n",
      "326 391 Loss: 1.360 | Acc: 50.277% (21044/41856)\n",
      "327 391 Loss: 1.360 | Acc: 50.283% (21111/41984)\n",
      "328 391 Loss: 1.360 | Acc: 50.259% (21165/42112)\n",
      "329 391 Loss: 1.360 | Acc: 50.265% (21232/42240)\n",
      "330 391 Loss: 1.360 | Acc: 50.253% (21291/42368)\n",
      "331 391 Loss: 1.360 | Acc: 50.273% (21364/42496)\n",
      "332 391 Loss: 1.359 | Acc: 50.305% (21442/42624)\n",
      "333 391 Loss: 1.359 | Acc: 50.295% (21502/42752)\n",
      "334 391 Loss: 1.359 | Acc: 50.303% (21570/42880)\n",
      "335 391 Loss: 1.358 | Acc: 50.314% (21639/43008)\n",
      "336 391 Loss: 1.358 | Acc: 50.306% (21700/43136)\n",
      "337 391 Loss: 1.358 | Acc: 50.298% (21761/43264)\n",
      "338 391 Loss: 1.358 | Acc: 50.320% (21835/43392)\n",
      "339 391 Loss: 1.358 | Acc: 50.340% (21908/43520)\n",
      "340 391 Loss: 1.358 | Acc: 50.344% (21974/43648)\n",
      "341 391 Loss: 1.357 | Acc: 50.377% (22053/43776)\n",
      "342 391 Loss: 1.357 | Acc: 50.392% (22124/43904)\n",
      "343 391 Loss: 1.357 | Acc: 50.370% (22179/44032)\n",
      "344 391 Loss: 1.357 | Acc: 50.383% (22249/44160)\n",
      "345 391 Loss: 1.357 | Acc: 50.384% (22314/44288)\n",
      "346 391 Loss: 1.357 | Acc: 50.392% (22382/44416)\n",
      "347 391 Loss: 1.357 | Acc: 50.402% (22451/44544)\n",
      "348 391 Loss: 1.357 | Acc: 50.394% (22512/44672)\n",
      "349 391 Loss: 1.357 | Acc: 50.395% (22577/44800)\n",
      "350 391 Loss: 1.357 | Acc: 50.369% (22630/44928)\n",
      "351 391 Loss: 1.357 | Acc: 50.380% (22699/45056)\n",
      "352 391 Loss: 1.357 | Acc: 50.394% (22770/45184)\n",
      "353 391 Loss: 1.357 | Acc: 50.386% (22831/45312)\n",
      "354 391 Loss: 1.357 | Acc: 50.414% (22908/45440)\n",
      "355 391 Loss: 1.356 | Acc: 50.408% (22970/45568)\n",
      "356 391 Loss: 1.356 | Acc: 50.400% (23031/45696)\n",
      "357 391 Loss: 1.356 | Acc: 50.406% (23098/45824)\n",
      "358 391 Loss: 1.356 | Acc: 50.396% (23158/45952)\n",
      "359 391 Loss: 1.356 | Acc: 50.399% (23224/46080)\n",
      "360 391 Loss: 1.356 | Acc: 50.387% (23283/46208)\n",
      "361 391 Loss: 1.356 | Acc: 50.391% (23349/46336)\n",
      "362 391 Loss: 1.356 | Acc: 50.394% (23415/46464)\n",
      "363 391 Loss: 1.356 | Acc: 50.384% (23475/46592)\n",
      "364 391 Loss: 1.356 | Acc: 50.377% (23536/46720)\n",
      "365 391 Loss: 1.356 | Acc: 50.406% (23614/46848)\n",
      "366 391 Loss: 1.356 | Acc: 50.424% (23687/46976)\n",
      "367 391 Loss: 1.355 | Acc: 50.429% (23754/47104)\n",
      "368 391 Loss: 1.356 | Acc: 50.421% (23815/47232)\n",
      "369 391 Loss: 1.355 | Acc: 50.422% (23880/47360)\n",
      "370 391 Loss: 1.355 | Acc: 50.434% (23950/47488)\n",
      "371 391 Loss: 1.355 | Acc: 50.439% (24017/47616)\n",
      "372 391 Loss: 1.355 | Acc: 50.442% (24083/47744)\n",
      "373 391 Loss: 1.355 | Acc: 50.434% (24144/47872)\n",
      "374 391 Loss: 1.355 | Acc: 50.442% (24212/48000)\n",
      "375 391 Loss: 1.354 | Acc: 50.455% (24283/48128)\n",
      "376 391 Loss: 1.354 | Acc: 50.466% (24353/48256)\n",
      "377 391 Loss: 1.355 | Acc: 50.459% (24414/48384)\n",
      "378 391 Loss: 1.355 | Acc: 50.456% (24477/48512)\n",
      "379 391 Loss: 1.355 | Acc: 50.458% (24543/48640)\n",
      "380 391 Loss: 1.354 | Acc: 50.478% (24617/48768)\n",
      "381 391 Loss: 1.354 | Acc: 50.485% (24685/48896)\n",
      "382 391 Loss: 1.354 | Acc: 50.504% (24759/49024)\n",
      "383 391 Loss: 1.354 | Acc: 50.513% (24828/49152)\n",
      "384 391 Loss: 1.353 | Acc: 50.519% (24896/49280)\n",
      "385 391 Loss: 1.353 | Acc: 50.542% (24972/49408)\n",
      "386 391 Loss: 1.353 | Acc: 50.551% (25041/49536)\n",
      "387 391 Loss: 1.353 | Acc: 50.556% (25108/49664)\n",
      "388 391 Loss: 1.352 | Acc: 50.564% (25177/49792)\n",
      "389 391 Loss: 1.352 | Acc: 50.591% (25255/49920)\n",
      "390 391 Loss: 1.352 | Acc: 50.588% (25294/50000)\n",
      "0 100 Loss: 1.256 | Acc: 52.000% (52/100)\n",
      "1 100 Loss: 1.264 | Acc: 53.500% (107/200)\n",
      "2 100 Loss: 1.282 | Acc: 50.667% (152/300)\n",
      "3 100 Loss: 1.329 | Acc: 49.750% (199/400)\n",
      "4 100 Loss: 1.343 | Acc: 49.600% (248/500)\n",
      "5 100 Loss: 1.357 | Acc: 49.667% (298/600)\n",
      "6 100 Loss: 1.357 | Acc: 50.286% (352/700)\n",
      "7 100 Loss: 1.363 | Acc: 50.250% (402/800)\n",
      "8 100 Loss: 1.374 | Acc: 50.111% (451/900)\n",
      "9 100 Loss: 1.360 | Acc: 50.600% (506/1000)\n",
      "10 100 Loss: 1.345 | Acc: 51.273% (564/1100)\n",
      "11 100 Loss: 1.345 | Acc: 50.833% (610/1200)\n",
      "12 100 Loss: 1.365 | Acc: 50.692% (659/1300)\n",
      "13 100 Loss: 1.378 | Acc: 50.214% (703/1400)\n",
      "14 100 Loss: 1.367 | Acc: 50.533% (758/1500)\n",
      "15 100 Loss: 1.369 | Acc: 50.750% (812/1600)\n",
      "16 100 Loss: 1.372 | Acc: 50.882% (865/1700)\n",
      "17 100 Loss: 1.365 | Acc: 51.056% (919/1800)\n",
      "18 100 Loss: 1.363 | Acc: 51.211% (973/1900)\n",
      "19 100 Loss: 1.369 | Acc: 50.750% (1015/2000)\n",
      "20 100 Loss: 1.365 | Acc: 50.952% (1070/2100)\n",
      "21 100 Loss: 1.364 | Acc: 50.955% (1121/2200)\n",
      "22 100 Loss: 1.361 | Acc: 50.957% (1172/2300)\n",
      "23 100 Loss: 1.360 | Acc: 51.042% (1225/2400)\n",
      "24 100 Loss: 1.356 | Acc: 51.280% (1282/2500)\n",
      "25 100 Loss: 1.364 | Acc: 51.000% (1326/2600)\n",
      "26 100 Loss: 1.363 | Acc: 51.111% (1380/2700)\n",
      "27 100 Loss: 1.366 | Acc: 50.786% (1422/2800)\n",
      "28 100 Loss: 1.369 | Acc: 50.621% (1468/2900)\n",
      "29 100 Loss: 1.365 | Acc: 50.767% (1523/3000)\n",
      "30 100 Loss: 1.360 | Acc: 51.065% (1583/3100)\n",
      "31 100 Loss: 1.355 | Acc: 51.219% (1639/3200)\n",
      "32 100 Loss: 1.354 | Acc: 51.333% (1694/3300)\n",
      "33 100 Loss: 1.356 | Acc: 51.412% (1748/3400)\n",
      "34 100 Loss: 1.355 | Acc: 51.286% (1795/3500)\n",
      "35 100 Loss: 1.352 | Acc: 51.556% (1856/3600)\n",
      "36 100 Loss: 1.354 | Acc: 51.459% (1904/3700)\n",
      "37 100 Loss: 1.356 | Acc: 51.342% (1951/3800)\n",
      "38 100 Loss: 1.353 | Acc: 51.282% (2000/3900)\n",
      "39 100 Loss: 1.355 | Acc: 51.225% (2049/4000)\n",
      "40 100 Loss: 1.359 | Acc: 51.195% (2099/4100)\n",
      "41 100 Loss: 1.360 | Acc: 51.167% (2149/4200)\n",
      "42 100 Loss: 1.356 | Acc: 51.349% (2208/4300)\n",
      "43 100 Loss: 1.354 | Acc: 51.500% (2266/4400)\n",
      "44 100 Loss: 1.351 | Acc: 51.533% (2319/4500)\n",
      "45 100 Loss: 1.349 | Acc: 51.565% (2372/4600)\n",
      "46 100 Loss: 1.346 | Acc: 51.617% (2426/4700)\n",
      "47 100 Loss: 1.348 | Acc: 51.708% (2482/4800)\n",
      "48 100 Loss: 1.346 | Acc: 51.796% (2538/4900)\n",
      "49 100 Loss: 1.345 | Acc: 51.900% (2595/5000)\n",
      "50 100 Loss: 1.346 | Acc: 51.863% (2645/5100)\n",
      "51 100 Loss: 1.346 | Acc: 51.808% (2694/5200)\n",
      "52 100 Loss: 1.344 | Acc: 51.755% (2743/5300)\n",
      "53 100 Loss: 1.347 | Acc: 51.667% (2790/5400)\n",
      "54 100 Loss: 1.350 | Acc: 51.491% (2832/5500)\n",
      "55 100 Loss: 1.354 | Acc: 51.411% (2879/5600)\n",
      "56 100 Loss: 1.354 | Acc: 51.456% (2933/5700)\n",
      "57 100 Loss: 1.349 | Acc: 51.621% (2994/5800)\n",
      "58 100 Loss: 1.349 | Acc: 51.661% (3048/5900)\n",
      "59 100 Loss: 1.347 | Acc: 51.600% (3096/6000)\n",
      "60 100 Loss: 1.347 | Acc: 51.492% (3141/6100)\n",
      "61 100 Loss: 1.347 | Acc: 51.355% (3184/6200)\n",
      "62 100 Loss: 1.346 | Acc: 51.413% (3239/6300)\n",
      "63 100 Loss: 1.347 | Acc: 51.406% (3290/6400)\n",
      "64 100 Loss: 1.346 | Acc: 51.446% (3344/6500)\n",
      "65 100 Loss: 1.347 | Acc: 51.485% (3398/6600)\n",
      "66 100 Loss: 1.345 | Acc: 51.582% (3456/6700)\n",
      "67 100 Loss: 1.344 | Acc: 51.647% (3512/6800)\n",
      "68 100 Loss: 1.346 | Acc: 51.536% (3556/6900)\n",
      "69 100 Loss: 1.346 | Acc: 51.543% (3608/7000)\n",
      "70 100 Loss: 1.347 | Acc: 51.465% (3654/7100)\n",
      "71 100 Loss: 1.347 | Acc: 51.417% (3702/7200)\n",
      "72 100 Loss: 1.346 | Acc: 51.370% (3750/7300)\n",
      "73 100 Loss: 1.343 | Acc: 51.446% (3807/7400)\n",
      "74 100 Loss: 1.344 | Acc: 51.347% (3851/7500)\n",
      "75 100 Loss: 1.343 | Acc: 51.342% (3902/7600)\n",
      "76 100 Loss: 1.342 | Acc: 51.273% (3948/7700)\n",
      "77 100 Loss: 1.342 | Acc: 51.256% (3998/7800)\n",
      "78 100 Loss: 1.341 | Acc: 51.392% (4060/7900)\n",
      "79 100 Loss: 1.341 | Acc: 51.388% (4111/8000)\n",
      "80 100 Loss: 1.340 | Acc: 51.395% (4163/8100)\n",
      "81 100 Loss: 1.341 | Acc: 51.415% (4216/8200)\n",
      "82 100 Loss: 1.342 | Acc: 51.458% (4271/8300)\n",
      "83 100 Loss: 1.342 | Acc: 51.464% (4323/8400)\n",
      "84 100 Loss: 1.343 | Acc: 51.424% (4371/8500)\n",
      "85 100 Loss: 1.342 | Acc: 51.512% (4430/8600)\n",
      "86 100 Loss: 1.343 | Acc: 51.448% (4476/8700)\n",
      "87 100 Loss: 1.343 | Acc: 51.455% (4528/8800)\n",
      "88 100 Loss: 1.342 | Acc: 51.483% (4582/8900)\n",
      "89 100 Loss: 1.342 | Acc: 51.533% (4638/9000)\n",
      "90 100 Loss: 1.342 | Acc: 51.484% (4685/9100)\n",
      "91 100 Loss: 1.341 | Acc: 51.533% (4741/9200)\n",
      "92 100 Loss: 1.340 | Acc: 51.613% (4800/9300)\n",
      "93 100 Loss: 1.341 | Acc: 51.617% (4852/9400)\n",
      "94 100 Loss: 1.341 | Acc: 51.621% (4904/9500)\n",
      "95 100 Loss: 1.340 | Acc: 51.604% (4954/9600)\n",
      "96 100 Loss: 1.339 | Acc: 51.649% (5010/9700)\n",
      "97 100 Loss: 1.340 | Acc: 51.571% (5054/9800)\n",
      "98 100 Loss: 1.341 | Acc: 51.485% (5097/9900)\n",
      "99 100 Loss: 1.342 | Acc: 51.490% (5149/10000)\n",
      "Saving..\n",
      "\n",
      "Epoch: 4\n",
      "0 391 Loss: 1.359 | Acc: 50.781% (65/128)\n",
      "1 391 Loss: 1.387 | Acc: 50.391% (129/256)\n",
      "2 391 Loss: 1.342 | Acc: 52.865% (203/384)\n",
      "3 391 Loss: 1.331 | Acc: 52.930% (271/512)\n",
      "4 391 Loss: 1.276 | Acc: 54.844% (351/640)\n",
      "5 391 Loss: 1.275 | Acc: 55.078% (423/768)\n",
      "6 391 Loss: 1.271 | Acc: 55.134% (494/896)\n",
      "7 391 Loss: 1.263 | Acc: 55.371% (567/1024)\n",
      "8 391 Loss: 1.283 | Acc: 54.601% (629/1152)\n",
      "9 391 Loss: 1.305 | Acc: 54.141% (693/1280)\n",
      "10 391 Loss: 1.305 | Acc: 53.835% (758/1408)\n",
      "11 391 Loss: 1.302 | Acc: 53.971% (829/1536)\n",
      "12 391 Loss: 1.307 | Acc: 53.726% (894/1664)\n",
      "13 391 Loss: 1.303 | Acc: 53.627% (961/1792)\n",
      "14 391 Loss: 1.305 | Acc: 53.177% (1021/1920)\n",
      "15 391 Loss: 1.317 | Acc: 52.881% (1083/2048)\n",
      "16 391 Loss: 1.316 | Acc: 52.987% (1153/2176)\n",
      "17 391 Loss: 1.322 | Acc: 52.517% (1210/2304)\n",
      "18 391 Loss: 1.329 | Acc: 52.303% (1272/2432)\n",
      "19 391 Loss: 1.327 | Acc: 52.422% (1342/2560)\n",
      "20 391 Loss: 1.324 | Acc: 52.604% (1414/2688)\n",
      "21 391 Loss: 1.323 | Acc: 52.521% (1479/2816)\n",
      "22 391 Loss: 1.326 | Acc: 52.548% (1547/2944)\n",
      "23 391 Loss: 1.321 | Acc: 52.799% (1622/3072)\n",
      "24 391 Loss: 1.321 | Acc: 52.844% (1691/3200)\n",
      "25 391 Loss: 1.320 | Acc: 52.975% (1763/3328)\n",
      "26 391 Loss: 1.319 | Acc: 53.067% (1834/3456)\n",
      "27 391 Loss: 1.323 | Acc: 52.985% (1899/3584)\n",
      "28 391 Loss: 1.328 | Acc: 52.559% (1951/3712)\n",
      "29 391 Loss: 1.330 | Acc: 52.370% (2011/3840)\n",
      "30 391 Loss: 1.329 | Acc: 52.293% (2075/3968)\n",
      "31 391 Loss: 1.327 | Acc: 52.588% (2154/4096)\n",
      "32 391 Loss: 1.324 | Acc: 52.723% (2227/4224)\n",
      "33 391 Loss: 1.329 | Acc: 52.551% (2287/4352)\n",
      "34 391 Loss: 1.329 | Acc: 52.545% (2354/4480)\n",
      "35 391 Loss: 1.330 | Acc: 52.474% (2418/4608)\n",
      "36 391 Loss: 1.330 | Acc: 52.428% (2483/4736)\n",
      "37 391 Loss: 1.332 | Acc: 52.447% (2551/4864)\n",
      "38 391 Loss: 1.331 | Acc: 52.384% (2615/4992)\n",
      "39 391 Loss: 1.335 | Acc: 52.227% (2674/5120)\n",
      "40 391 Loss: 1.335 | Acc: 52.153% (2737/5248)\n",
      "41 391 Loss: 1.337 | Acc: 52.065% (2799/5376)\n",
      "42 391 Loss: 1.335 | Acc: 52.180% (2872/5504)\n",
      "43 391 Loss: 1.335 | Acc: 52.148% (2937/5632)\n",
      "44 391 Loss: 1.335 | Acc: 52.083% (3000/5760)\n",
      "45 391 Loss: 1.337 | Acc: 52.089% (3067/5888)\n",
      "46 391 Loss: 1.339 | Acc: 52.111% (3135/6016)\n",
      "47 391 Loss: 1.342 | Acc: 52.051% (3198/6144)\n",
      "48 391 Loss: 1.341 | Acc: 52.057% (3265/6272)\n",
      "49 391 Loss: 1.342 | Acc: 52.094% (3334/6400)\n",
      "50 391 Loss: 1.341 | Acc: 52.083% (3400/6528)\n",
      "51 391 Loss: 1.341 | Acc: 52.148% (3471/6656)\n",
      "52 391 Loss: 1.340 | Acc: 52.108% (3535/6784)\n",
      "53 391 Loss: 1.339 | Acc: 52.199% (3608/6912)\n",
      "54 391 Loss: 1.339 | Acc: 52.116% (3669/7040)\n",
      "55 391 Loss: 1.340 | Acc: 52.065% (3732/7168)\n",
      "56 391 Loss: 1.339 | Acc: 52.220% (3810/7296)\n",
      "57 391 Loss: 1.338 | Acc: 52.155% (3872/7424)\n",
      "58 391 Loss: 1.338 | Acc: 52.132% (3937/7552)\n",
      "59 391 Loss: 1.335 | Acc: 52.227% (4011/7680)\n",
      "60 391 Loss: 1.334 | Acc: 52.331% (4086/7808)\n",
      "61 391 Loss: 1.334 | Acc: 52.331% (4153/7936)\n",
      "62 391 Loss: 1.336 | Acc: 52.220% (4211/8064)\n",
      "63 391 Loss: 1.336 | Acc: 52.173% (4274/8192)\n",
      "64 391 Loss: 1.334 | Acc: 52.212% (4344/8320)\n",
      "65 391 Loss: 1.331 | Acc: 52.285% (4417/8448)\n",
      "66 391 Loss: 1.331 | Acc: 52.285% (4484/8576)\n",
      "67 391 Loss: 1.333 | Acc: 52.194% (4543/8704)\n",
      "68 391 Loss: 1.335 | Acc: 52.049% (4597/8832)\n",
      "69 391 Loss: 1.336 | Acc: 52.065% (4665/8960)\n",
      "70 391 Loss: 1.336 | Acc: 52.036% (4729/9088)\n",
      "71 391 Loss: 1.335 | Acc: 52.051% (4797/9216)\n",
      "72 391 Loss: 1.335 | Acc: 52.076% (4866/9344)\n",
      "73 391 Loss: 1.337 | Acc: 51.995% (4925/9472)\n",
      "74 391 Loss: 1.338 | Acc: 51.948% (4987/9600)\n",
      "75 391 Loss: 1.337 | Acc: 52.097% (5068/9728)\n",
      "76 391 Loss: 1.335 | Acc: 52.151% (5140/9856)\n",
      "77 391 Loss: 1.334 | Acc: 52.133% (5205/9984)\n",
      "78 391 Loss: 1.334 | Acc: 52.126% (5271/10112)\n",
      "79 391 Loss: 1.333 | Acc: 52.148% (5340/10240)\n",
      "80 391 Loss: 1.333 | Acc: 52.160% (5408/10368)\n",
      "81 391 Loss: 1.332 | Acc: 52.191% (5478/10496)\n",
      "82 391 Loss: 1.331 | Acc: 52.165% (5542/10624)\n",
      "83 391 Loss: 1.331 | Acc: 52.223% (5615/10752)\n",
      "84 391 Loss: 1.329 | Acc: 52.215% (5681/10880)\n",
      "85 391 Loss: 1.329 | Acc: 52.280% (5755/11008)\n",
      "86 391 Loss: 1.329 | Acc: 52.299% (5824/11136)\n",
      "87 391 Loss: 1.327 | Acc: 52.344% (5896/11264)\n",
      "88 391 Loss: 1.327 | Acc: 52.353% (5964/11392)\n",
      "89 391 Loss: 1.326 | Acc: 52.361% (6032/11520)\n",
      "90 391 Loss: 1.326 | Acc: 52.404% (6104/11648)\n",
      "91 391 Loss: 1.326 | Acc: 52.446% (6176/11776)\n",
      "92 391 Loss: 1.327 | Acc: 52.361% (6233/11904)\n",
      "93 391 Loss: 1.326 | Acc: 52.344% (6298/12032)\n",
      "94 391 Loss: 1.327 | Acc: 52.270% (6356/12160)\n",
      "95 391 Loss: 1.326 | Acc: 52.238% (6419/12288)\n",
      "96 391 Loss: 1.326 | Acc: 52.239% (6486/12416)\n",
      "97 391 Loss: 1.326 | Acc: 52.232% (6552/12544)\n",
      "98 391 Loss: 1.325 | Acc: 52.273% (6624/12672)\n",
      "99 391 Loss: 1.325 | Acc: 52.320% (6697/12800)\n",
      "100 391 Loss: 1.325 | Acc: 52.282% (6759/12928)\n",
      "101 391 Loss: 1.325 | Acc: 52.198% (6815/13056)\n",
      "102 391 Loss: 1.325 | Acc: 52.139% (6874/13184)\n",
      "103 391 Loss: 1.325 | Acc: 52.163% (6944/13312)\n",
      "104 391 Loss: 1.324 | Acc: 52.240% (7021/13440)\n",
      "105 391 Loss: 1.325 | Acc: 52.152% (7076/13568)\n",
      "106 391 Loss: 1.324 | Acc: 52.176% (7146/13696)\n",
      "107 391 Loss: 1.324 | Acc: 52.163% (7211/13824)\n",
      "108 391 Loss: 1.324 | Acc: 52.222% (7286/13952)\n",
      "109 391 Loss: 1.323 | Acc: 52.244% (7356/14080)\n",
      "110 391 Loss: 1.323 | Acc: 52.182% (7414/14208)\n",
      "111 391 Loss: 1.323 | Acc: 52.211% (7485/14336)\n",
      "112 391 Loss: 1.323 | Acc: 52.212% (7552/14464)\n",
      "113 391 Loss: 1.323 | Acc: 52.200% (7617/14592)\n",
      "114 391 Loss: 1.323 | Acc: 52.208% (7685/14720)\n",
      "115 391 Loss: 1.323 | Acc: 52.121% (7739/14848)\n",
      "116 391 Loss: 1.322 | Acc: 52.217% (7820/14976)\n",
      "117 391 Loss: 1.323 | Acc: 52.191% (7883/15104)\n",
      "118 391 Loss: 1.323 | Acc: 52.166% (7946/15232)\n",
      "119 391 Loss: 1.324 | Acc: 52.181% (8015/15360)\n",
      "120 391 Loss: 1.325 | Acc: 52.124% (8073/15488)\n",
      "121 391 Loss: 1.325 | Acc: 52.075% (8132/15616)\n",
      "122 391 Loss: 1.325 | Acc: 52.140% (8209/15744)\n",
      "123 391 Loss: 1.324 | Acc: 52.161% (8279/15872)\n",
      "124 391 Loss: 1.323 | Acc: 52.206% (8353/16000)\n",
      "125 391 Loss: 1.323 | Acc: 52.232% (8424/16128)\n",
      "126 391 Loss: 1.322 | Acc: 52.239% (8492/16256)\n",
      "127 391 Loss: 1.322 | Acc: 52.258% (8562/16384)\n",
      "128 391 Loss: 1.321 | Acc: 52.265% (8630/16512)\n",
      "129 391 Loss: 1.321 | Acc: 52.248% (8694/16640)\n",
      "130 391 Loss: 1.320 | Acc: 52.242% (8760/16768)\n",
      "131 391 Loss: 1.320 | Acc: 52.243% (8827/16896)\n",
      "132 391 Loss: 1.320 | Acc: 52.191% (8885/17024)\n",
      "133 391 Loss: 1.320 | Acc: 52.186% (8951/17152)\n",
      "134 391 Loss: 1.319 | Acc: 52.205% (9021/17280)\n",
      "135 391 Loss: 1.318 | Acc: 52.269% (9099/17408)\n",
      "136 391 Loss: 1.318 | Acc: 52.270% (9166/17536)\n",
      "137 391 Loss: 1.317 | Acc: 52.304% (9239/17664)\n",
      "138 391 Loss: 1.317 | Acc: 52.259% (9298/17792)\n",
      "139 391 Loss: 1.316 | Acc: 52.271% (9367/17920)\n",
      "140 391 Loss: 1.316 | Acc: 52.266% (9433/18048)\n",
      "141 391 Loss: 1.315 | Acc: 52.283% (9503/18176)\n",
      "142 391 Loss: 1.315 | Acc: 52.295% (9572/18304)\n",
      "143 391 Loss: 1.314 | Acc: 52.273% (9635/18432)\n",
      "144 391 Loss: 1.315 | Acc: 52.268% (9701/18560)\n",
      "145 391 Loss: 1.315 | Acc: 52.210% (9757/18688)\n",
      "146 391 Loss: 1.315 | Acc: 52.227% (9827/18816)\n",
      "147 391 Loss: 1.315 | Acc: 52.238% (9896/18944)\n",
      "148 391 Loss: 1.315 | Acc: 52.291% (9973/19072)\n",
      "149 391 Loss: 1.315 | Acc: 52.271% (10036/19200)\n",
      "150 391 Loss: 1.314 | Acc: 52.240% (10097/19328)\n",
      "151 391 Loss: 1.314 | Acc: 52.272% (10170/19456)\n",
      "152 391 Loss: 1.316 | Acc: 52.201% (10223/19584)\n",
      "153 391 Loss: 1.315 | Acc: 52.222% (10294/19712)\n",
      "154 391 Loss: 1.315 | Acc: 52.228% (10362/19840)\n",
      "155 391 Loss: 1.315 | Acc: 52.249% (10433/19968)\n",
      "156 391 Loss: 1.314 | Acc: 52.284% (10507/20096)\n",
      "157 391 Loss: 1.314 | Acc: 52.284% (10574/20224)\n",
      "158 391 Loss: 1.313 | Acc: 52.285% (10641/20352)\n",
      "159 391 Loss: 1.312 | Acc: 52.339% (10719/20480)\n",
      "160 391 Loss: 1.312 | Acc: 52.329% (10784/20608)\n",
      "161 391 Loss: 1.313 | Acc: 52.315% (10848/20736)\n",
      "162 391 Loss: 1.312 | Acc: 52.267% (10905/20864)\n",
      "163 391 Loss: 1.311 | Acc: 52.291% (10977/20992)\n",
      "164 391 Loss: 1.311 | Acc: 52.273% (11040/21120)\n",
      "165 391 Loss: 1.311 | Acc: 52.287% (11110/21248)\n",
      "166 391 Loss: 1.312 | Acc: 52.264% (11172/21376)\n",
      "167 391 Loss: 1.311 | Acc: 52.260% (11238/21504)\n",
      "168 391 Loss: 1.312 | Acc: 52.233% (11299/21632)\n",
      "169 391 Loss: 1.312 | Acc: 52.243% (11368/21760)\n",
      "170 391 Loss: 1.311 | Acc: 52.284% (11444/21888)\n",
      "171 391 Loss: 1.311 | Acc: 52.294% (11513/22016)\n",
      "172 391 Loss: 1.311 | Acc: 52.290% (11579/22144)\n",
      "173 391 Loss: 1.311 | Acc: 52.276% (11643/22272)\n",
      "174 391 Loss: 1.310 | Acc: 52.272% (11709/22400)\n",
      "175 391 Loss: 1.310 | Acc: 52.282% (11778/22528)\n",
      "176 391 Loss: 1.310 | Acc: 52.295% (11848/22656)\n",
      "177 391 Loss: 1.310 | Acc: 52.278% (11911/22784)\n",
      "178 391 Loss: 1.309 | Acc: 52.313% (11986/22912)\n",
      "179 391 Loss: 1.309 | Acc: 52.348% (12061/23040)\n",
      "180 391 Loss: 1.309 | Acc: 52.378% (12135/23168)\n",
      "181 391 Loss: 1.308 | Acc: 52.382% (12203/23296)\n",
      "182 391 Loss: 1.307 | Acc: 52.429% (12281/23424)\n",
      "183 391 Loss: 1.308 | Acc: 52.386% (12338/23552)\n",
      "184 391 Loss: 1.308 | Acc: 52.411% (12411/23680)\n",
      "185 391 Loss: 1.308 | Acc: 52.432% (12483/23808)\n",
      "186 391 Loss: 1.308 | Acc: 52.461% (12557/23936)\n",
      "187 391 Loss: 1.307 | Acc: 52.468% (12626/24064)\n",
      "188 391 Loss: 1.308 | Acc: 52.455% (12690/24192)\n",
      "189 391 Loss: 1.308 | Acc: 52.467% (12760/24320)\n",
      "190 391 Loss: 1.308 | Acc: 52.462% (12826/24448)\n",
      "191 391 Loss: 1.307 | Acc: 52.470% (12895/24576)\n",
      "192 391 Loss: 1.306 | Acc: 52.489% (12967/24704)\n",
      "193 391 Loss: 1.306 | Acc: 52.517% (13041/24832)\n",
      "194 391 Loss: 1.307 | Acc: 52.508% (13106/24960)\n",
      "195 391 Loss: 1.307 | Acc: 52.507% (13173/25088)\n",
      "196 391 Loss: 1.307 | Acc: 52.506% (13240/25216)\n",
      "197 391 Loss: 1.306 | Acc: 52.525% (13312/25344)\n",
      "198 391 Loss: 1.306 | Acc: 52.524% (13379/25472)\n",
      "199 391 Loss: 1.306 | Acc: 52.527% (13447/25600)\n",
      "200 391 Loss: 1.307 | Acc: 52.507% (13509/25728)\n",
      "201 391 Loss: 1.307 | Acc: 52.502% (13575/25856)\n",
      "202 391 Loss: 1.307 | Acc: 52.490% (13639/25984)\n",
      "203 391 Loss: 1.307 | Acc: 52.497% (13708/26112)\n",
      "204 391 Loss: 1.307 | Acc: 52.485% (13772/26240)\n",
      "205 391 Loss: 1.307 | Acc: 52.476% (13837/26368)\n",
      "206 391 Loss: 1.307 | Acc: 52.476% (13904/26496)\n",
      "207 391 Loss: 1.307 | Acc: 52.494% (13976/26624)\n",
      "208 391 Loss: 1.307 | Acc: 52.497% (14044/26752)\n",
      "209 391 Loss: 1.307 | Acc: 52.515% (14116/26880)\n",
      "210 391 Loss: 1.307 | Acc: 52.540% (14190/27008)\n",
      "211 391 Loss: 1.306 | Acc: 52.524% (14253/27136)\n",
      "212 391 Loss: 1.307 | Acc: 52.483% (14309/27264)\n",
      "213 391 Loss: 1.307 | Acc: 52.482% (14376/27392)\n",
      "214 391 Loss: 1.308 | Acc: 52.438% (14431/27520)\n",
      "215 391 Loss: 1.308 | Acc: 52.445% (14500/27648)\n",
      "216 391 Loss: 1.309 | Acc: 52.419% (14560/27776)\n",
      "217 391 Loss: 1.308 | Acc: 52.458% (14638/27904)\n",
      "218 391 Loss: 1.308 | Acc: 52.458% (14705/28032)\n",
      "219 391 Loss: 1.308 | Acc: 52.443% (14768/28160)\n",
      "220 391 Loss: 1.308 | Acc: 52.457% (14839/28288)\n",
      "221 391 Loss: 1.308 | Acc: 52.463% (14908/28416)\n",
      "222 391 Loss: 1.308 | Acc: 52.459% (14974/28544)\n",
      "223 391 Loss: 1.307 | Acc: 52.466% (15043/28672)\n",
      "224 391 Loss: 1.307 | Acc: 52.455% (15107/28800)\n",
      "225 391 Loss: 1.308 | Acc: 52.430% (15167/28928)\n",
      "226 391 Loss: 1.308 | Acc: 52.450% (15240/29056)\n",
      "227 391 Loss: 1.309 | Acc: 52.433% (15302/29184)\n",
      "228 391 Loss: 1.309 | Acc: 52.419% (15365/29312)\n",
      "229 391 Loss: 1.309 | Acc: 52.425% (15434/29440)\n",
      "230 391 Loss: 1.309 | Acc: 52.438% (15505/29568)\n",
      "231 391 Loss: 1.310 | Acc: 52.455% (15577/29696)\n",
      "232 391 Loss: 1.310 | Acc: 52.434% (15638/29824)\n",
      "233 391 Loss: 1.310 | Acc: 52.424% (15702/29952)\n",
      "234 391 Loss: 1.311 | Acc: 52.417% (15767/30080)\n",
      "235 391 Loss: 1.311 | Acc: 52.433% (15839/30208)\n",
      "236 391 Loss: 1.311 | Acc: 52.429% (15905/30336)\n",
      "237 391 Loss: 1.310 | Acc: 52.449% (15978/30464)\n",
      "238 391 Loss: 1.311 | Acc: 52.442% (16043/30592)\n",
      "239 391 Loss: 1.311 | Acc: 52.428% (16106/30720)\n",
      "240 391 Loss: 1.311 | Acc: 52.415% (16169/30848)\n",
      "241 391 Loss: 1.311 | Acc: 52.405% (16233/30976)\n",
      "242 391 Loss: 1.311 | Acc: 52.447% (16313/31104)\n",
      "243 391 Loss: 1.311 | Acc: 52.446% (16380/31232)\n",
      "244 391 Loss: 1.311 | Acc: 52.443% (16446/31360)\n",
      "245 391 Loss: 1.311 | Acc: 52.442% (16513/31488)\n",
      "246 391 Loss: 1.310 | Acc: 52.445% (16581/31616)\n",
      "247 391 Loss: 1.310 | Acc: 52.463% (16654/31744)\n",
      "248 391 Loss: 1.310 | Acc: 52.450% (16717/31872)\n",
      "249 391 Loss: 1.310 | Acc: 52.444% (16782/32000)\n",
      "250 391 Loss: 1.310 | Acc: 52.415% (16840/32128)\n",
      "251 391 Loss: 1.310 | Acc: 52.412% (16906/32256)\n",
      "252 391 Loss: 1.309 | Acc: 52.439% (16982/32384)\n",
      "253 391 Loss: 1.309 | Acc: 52.436% (17048/32512)\n",
      "254 391 Loss: 1.310 | Acc: 52.430% (17113/32640)\n",
      "255 391 Loss: 1.309 | Acc: 52.444% (17185/32768)\n",
      "256 391 Loss: 1.309 | Acc: 52.459% (17257/32896)\n",
      "257 391 Loss: 1.309 | Acc: 52.468% (17327/33024)\n",
      "258 391 Loss: 1.309 | Acc: 52.455% (17390/33152)\n",
      "259 391 Loss: 1.309 | Acc: 52.458% (17458/33280)\n",
      "260 391 Loss: 1.309 | Acc: 52.472% (17530/33408)\n",
      "261 391 Loss: 1.309 | Acc: 52.460% (17593/33536)\n",
      "262 391 Loss: 1.309 | Acc: 52.451% (17657/33664)\n",
      "263 391 Loss: 1.309 | Acc: 52.444% (17722/33792)\n",
      "264 391 Loss: 1.309 | Acc: 52.453% (17792/33920)\n",
      "265 391 Loss: 1.310 | Acc: 52.432% (17852/34048)\n",
      "266 391 Loss: 1.310 | Acc: 52.417% (17914/34176)\n",
      "267 391 Loss: 1.310 | Acc: 52.431% (17986/34304)\n",
      "268 391 Loss: 1.310 | Acc: 52.419% (18049/34432)\n",
      "269 391 Loss: 1.310 | Acc: 52.428% (18119/34560)\n",
      "270 391 Loss: 1.311 | Acc: 52.424% (18185/34688)\n",
      "271 391 Loss: 1.310 | Acc: 52.436% (18256/34816)\n",
      "272 391 Loss: 1.310 | Acc: 52.430% (18321/34944)\n",
      "273 391 Loss: 1.310 | Acc: 52.444% (18393/35072)\n",
      "274 391 Loss: 1.310 | Acc: 52.438% (18458/35200)\n",
      "275 391 Loss: 1.310 | Acc: 52.457% (18532/35328)\n",
      "276 391 Loss: 1.310 | Acc: 52.448% (18596/35456)\n",
      "277 391 Loss: 1.310 | Acc: 52.453% (18665/35584)\n",
      "278 391 Loss: 1.310 | Acc: 52.445% (18729/35712)\n",
      "279 391 Loss: 1.310 | Acc: 52.427% (18790/35840)\n",
      "280 391 Loss: 1.310 | Acc: 52.441% (18862/35968)\n",
      "281 391 Loss: 1.309 | Acc: 52.446% (18931/36096)\n",
      "282 391 Loss: 1.309 | Acc: 52.465% (19005/36224)\n",
      "283 391 Loss: 1.308 | Acc: 52.490% (19081/36352)\n",
      "284 391 Loss: 1.308 | Acc: 52.508% (19155/36480)\n",
      "285 391 Loss: 1.308 | Acc: 52.519% (19226/36608)\n",
      "286 391 Loss: 1.308 | Acc: 52.513% (19291/36736)\n",
      "287 391 Loss: 1.308 | Acc: 52.525% (19363/36864)\n",
      "288 391 Loss: 1.308 | Acc: 52.509% (19424/36992)\n",
      "289 391 Loss: 1.308 | Acc: 52.497% (19487/37120)\n",
      "290 391 Loss: 1.308 | Acc: 52.494% (19553/37248)\n",
      "291 391 Loss: 1.308 | Acc: 52.488% (19618/37376)\n",
      "292 391 Loss: 1.309 | Acc: 52.458% (19674/37504)\n",
      "293 391 Loss: 1.309 | Acc: 52.450% (19738/37632)\n",
      "294 391 Loss: 1.309 | Acc: 52.466% (19811/37760)\n",
      "295 391 Loss: 1.309 | Acc: 52.455% (19874/37888)\n",
      "296 391 Loss: 1.309 | Acc: 52.441% (19936/38016)\n",
      "297 391 Loss: 1.309 | Acc: 52.451% (20007/38144)\n",
      "298 391 Loss: 1.308 | Acc: 52.456% (20076/38272)\n",
      "299 391 Loss: 1.309 | Acc: 52.435% (20135/38400)\n",
      "300 391 Loss: 1.309 | Acc: 52.427% (20199/38528)\n",
      "301 391 Loss: 1.309 | Acc: 52.427% (20266/38656)\n",
      "302 391 Loss: 1.309 | Acc: 52.424% (20332/38784)\n",
      "303 391 Loss: 1.309 | Acc: 52.408% (20393/38912)\n",
      "304 391 Loss: 1.309 | Acc: 52.395% (20455/39040)\n",
      "305 391 Loss: 1.309 | Acc: 52.395% (20522/39168)\n",
      "306 391 Loss: 1.309 | Acc: 52.407% (20594/39296)\n",
      "307 391 Loss: 1.309 | Acc: 52.417% (20665/39424)\n",
      "308 391 Loss: 1.309 | Acc: 52.417% (20732/39552)\n",
      "309 391 Loss: 1.309 | Acc: 52.424% (20802/39680)\n",
      "310 391 Loss: 1.309 | Acc: 52.429% (20871/39808)\n",
      "311 391 Loss: 1.308 | Acc: 52.464% (20952/39936)\n",
      "312 391 Loss: 1.308 | Acc: 52.471% (21022/40064)\n",
      "313 391 Loss: 1.308 | Acc: 52.486% (21095/40192)\n",
      "314 391 Loss: 1.308 | Acc: 52.465% (21154/40320)\n",
      "315 391 Loss: 1.308 | Acc: 52.490% (21231/40448)\n",
      "316 391 Loss: 1.308 | Acc: 52.521% (21311/40576)\n",
      "317 391 Loss: 1.308 | Acc: 52.518% (21377/40704)\n",
      "318 391 Loss: 1.308 | Acc: 52.523% (21446/40832)\n",
      "319 391 Loss: 1.308 | Acc: 52.524% (21514/40960)\n",
      "320 391 Loss: 1.307 | Acc: 52.548% (21591/41088)\n",
      "321 391 Loss: 1.307 | Acc: 52.533% (21652/41216)\n",
      "322 391 Loss: 1.307 | Acc: 52.535% (21720/41344)\n",
      "323 391 Loss: 1.307 | Acc: 52.539% (21789/41472)\n",
      "324 391 Loss: 1.307 | Acc: 52.553% (21862/41600)\n",
      "325 391 Loss: 1.306 | Acc: 52.574% (21938/41728)\n",
      "326 391 Loss: 1.306 | Acc: 52.568% (22003/41856)\n",
      "327 391 Loss: 1.306 | Acc: 52.572% (22072/41984)\n",
      "328 391 Loss: 1.306 | Acc: 52.591% (22147/42112)\n",
      "329 391 Loss: 1.306 | Acc: 52.611% (22223/42240)\n",
      "330 391 Loss: 1.306 | Acc: 52.622% (22295/42368)\n",
      "331 391 Loss: 1.305 | Acc: 52.645% (22372/42496)\n",
      "332 391 Loss: 1.305 | Acc: 52.642% (22438/42624)\n",
      "333 391 Loss: 1.305 | Acc: 52.648% (22508/42752)\n",
      "334 391 Loss: 1.305 | Acc: 52.677% (22588/42880)\n",
      "335 391 Loss: 1.305 | Acc: 52.667% (22651/43008)\n",
      "336 391 Loss: 1.305 | Acc: 52.666% (22718/43136)\n",
      "337 391 Loss: 1.305 | Acc: 52.644% (22776/43264)\n",
      "338 391 Loss: 1.304 | Acc: 52.657% (22849/43392)\n",
      "339 391 Loss: 1.304 | Acc: 52.686% (22929/43520)\n",
      "340 391 Loss: 1.304 | Acc: 52.697% (23001/43648)\n",
      "341 391 Loss: 1.304 | Acc: 52.693% (23067/43776)\n",
      "342 391 Loss: 1.303 | Acc: 52.690% (23133/43904)\n",
      "343 391 Loss: 1.303 | Acc: 52.673% (23193/44032)\n",
      "344 391 Loss: 1.303 | Acc: 52.672% (23260/44160)\n",
      "345 391 Loss: 1.304 | Acc: 52.660% (23322/44288)\n",
      "346 391 Loss: 1.304 | Acc: 52.654% (23387/44416)\n",
      "347 391 Loss: 1.304 | Acc: 52.647% (23451/44544)\n",
      "348 391 Loss: 1.304 | Acc: 52.653% (23521/44672)\n",
      "349 391 Loss: 1.304 | Acc: 52.636% (23581/44800)\n",
      "350 391 Loss: 1.304 | Acc: 52.664% (23661/44928)\n",
      "351 391 Loss: 1.304 | Acc: 52.668% (23730/45056)\n",
      "352 391 Loss: 1.304 | Acc: 52.656% (23792/45184)\n",
      "353 391 Loss: 1.304 | Acc: 52.659% (23861/45312)\n",
      "354 391 Loss: 1.304 | Acc: 52.669% (23933/45440)\n",
      "355 391 Loss: 1.304 | Acc: 52.673% (24002/45568)\n",
      "356 391 Loss: 1.304 | Acc: 52.657% (24062/45696)\n",
      "357 391 Loss: 1.304 | Acc: 52.656% (24129/45824)\n",
      "358 391 Loss: 1.304 | Acc: 52.661% (24199/45952)\n",
      "359 391 Loss: 1.304 | Acc: 52.667% (24269/46080)\n",
      "360 391 Loss: 1.304 | Acc: 52.677% (24341/46208)\n",
      "361 391 Loss: 1.304 | Acc: 52.657% (24399/46336)\n",
      "362 391 Loss: 1.304 | Acc: 52.652% (24464/46464)\n",
      "363 391 Loss: 1.304 | Acc: 52.653% (24532/46592)\n",
      "364 391 Loss: 1.304 | Acc: 52.656% (24601/46720)\n",
      "365 391 Loss: 1.304 | Acc: 52.666% (24673/46848)\n",
      "366 391 Loss: 1.304 | Acc: 52.678% (24746/46976)\n",
      "367 391 Loss: 1.304 | Acc: 52.686% (24817/47104)\n",
      "368 391 Loss: 1.303 | Acc: 52.721% (24901/47232)\n",
      "369 391 Loss: 1.303 | Acc: 52.736% (24976/47360)\n",
      "370 391 Loss: 1.303 | Acc: 52.729% (25040/47488)\n",
      "371 391 Loss: 1.303 | Acc: 52.739% (25112/47616)\n",
      "372 391 Loss: 1.303 | Acc: 52.750% (25185/47744)\n",
      "373 391 Loss: 1.303 | Acc: 52.759% (25257/47872)\n",
      "374 391 Loss: 1.303 | Acc: 52.758% (25324/48000)\n",
      "375 391 Loss: 1.303 | Acc: 52.778% (25401/48128)\n",
      "376 391 Loss: 1.303 | Acc: 52.779% (25469/48256)\n",
      "377 391 Loss: 1.303 | Acc: 52.801% (25547/48384)\n",
      "378 391 Loss: 1.302 | Acc: 52.810% (25619/48512)\n",
      "379 391 Loss: 1.303 | Acc: 52.802% (25683/48640)\n",
      "380 391 Loss: 1.302 | Acc: 52.819% (25759/48768)\n",
      "381 391 Loss: 1.302 | Acc: 52.833% (25833/48896)\n",
      "382 391 Loss: 1.302 | Acc: 52.829% (25899/49024)\n",
      "383 391 Loss: 1.302 | Acc: 52.832% (25968/49152)\n",
      "384 391 Loss: 1.303 | Acc: 52.827% (26033/49280)\n",
      "385 391 Loss: 1.302 | Acc: 52.825% (26100/49408)\n",
      "386 391 Loss: 1.302 | Acc: 52.828% (26169/49536)\n",
      "387 391 Loss: 1.302 | Acc: 52.833% (26239/49664)\n",
      "388 391 Loss: 1.302 | Acc: 52.840% (26310/49792)\n",
      "389 391 Loss: 1.302 | Acc: 52.837% (26376/49920)\n",
      "390 391 Loss: 1.302 | Acc: 52.842% (26421/50000)\n",
      "0 100 Loss: 1.147 | Acc: 55.000% (55/100)\n",
      "1 100 Loss: 1.209 | Acc: 54.500% (109/200)\n",
      "2 100 Loss: 1.231 | Acc: 53.000% (159/300)\n",
      "3 100 Loss: 1.266 | Acc: 52.500% (210/400)\n",
      "4 100 Loss: 1.267 | Acc: 52.400% (262/500)\n",
      "5 100 Loss: 1.264 | Acc: 53.000% (318/600)\n",
      "6 100 Loss: 1.283 | Acc: 52.714% (369/700)\n",
      "7 100 Loss: 1.285 | Acc: 52.625% (421/800)\n",
      "8 100 Loss: 1.297 | Acc: 52.000% (468/900)\n",
      "9 100 Loss: 1.289 | Acc: 52.500% (525/1000)\n",
      "10 100 Loss: 1.285 | Acc: 52.909% (582/1100)\n",
      "11 100 Loss: 1.279 | Acc: 53.000% (636/1200)\n",
      "12 100 Loss: 1.290 | Acc: 52.615% (684/1300)\n",
      "13 100 Loss: 1.295 | Acc: 52.071% (729/1400)\n",
      "14 100 Loss: 1.291 | Acc: 52.200% (783/1500)\n",
      "15 100 Loss: 1.296 | Acc: 52.062% (833/1600)\n",
      "16 100 Loss: 1.294 | Acc: 52.176% (887/1700)\n",
      "17 100 Loss: 1.287 | Acc: 52.611% (947/1800)\n",
      "18 100 Loss: 1.283 | Acc: 53.000% (1007/1900)\n",
      "19 100 Loss: 1.289 | Acc: 52.800% (1056/2000)\n",
      "20 100 Loss: 1.281 | Acc: 53.286% (1119/2100)\n",
      "21 100 Loss: 1.279 | Acc: 53.409% (1175/2200)\n",
      "22 100 Loss: 1.277 | Acc: 53.348% (1227/2300)\n",
      "23 100 Loss: 1.274 | Acc: 53.542% (1285/2400)\n",
      "24 100 Loss: 1.272 | Acc: 53.640% (1341/2500)\n",
      "25 100 Loss: 1.284 | Acc: 53.115% (1381/2600)\n",
      "26 100 Loss: 1.281 | Acc: 53.185% (1436/2700)\n",
      "27 100 Loss: 1.284 | Acc: 52.929% (1482/2800)\n",
      "28 100 Loss: 1.285 | Acc: 52.931% (1535/2900)\n",
      "29 100 Loss: 1.280 | Acc: 53.133% (1594/3000)\n",
      "30 100 Loss: 1.276 | Acc: 53.387% (1655/3100)\n",
      "31 100 Loss: 1.272 | Acc: 53.531% (1713/3200)\n",
      "32 100 Loss: 1.271 | Acc: 53.606% (1769/3300)\n",
      "33 100 Loss: 1.271 | Acc: 53.735% (1827/3400)\n",
      "34 100 Loss: 1.272 | Acc: 53.571% (1875/3500)\n",
      "35 100 Loss: 1.271 | Acc: 53.750% (1935/3600)\n",
      "36 100 Loss: 1.275 | Acc: 53.486% (1979/3700)\n",
      "37 100 Loss: 1.279 | Acc: 53.211% (2022/3800)\n",
      "38 100 Loss: 1.275 | Acc: 53.333% (2080/3900)\n",
      "39 100 Loss: 1.277 | Acc: 53.175% (2127/4000)\n",
      "40 100 Loss: 1.279 | Acc: 53.268% (2184/4100)\n",
      "41 100 Loss: 1.279 | Acc: 53.310% (2239/4200)\n",
      "42 100 Loss: 1.277 | Acc: 53.372% (2295/4300)\n",
      "43 100 Loss: 1.275 | Acc: 53.568% (2357/4400)\n",
      "44 100 Loss: 1.274 | Acc: 53.578% (2411/4500)\n",
      "45 100 Loss: 1.273 | Acc: 53.696% (2470/4600)\n",
      "46 100 Loss: 1.270 | Acc: 53.809% (2529/4700)\n",
      "47 100 Loss: 1.271 | Acc: 53.854% (2585/4800)\n",
      "48 100 Loss: 1.268 | Acc: 53.898% (2641/4900)\n",
      "49 100 Loss: 1.265 | Acc: 54.040% (2702/5000)\n",
      "50 100 Loss: 1.265 | Acc: 54.098% (2759/5100)\n",
      "51 100 Loss: 1.265 | Acc: 53.981% (2807/5200)\n",
      "52 100 Loss: 1.265 | Acc: 53.962% (2860/5300)\n",
      "53 100 Loss: 1.268 | Acc: 53.852% (2908/5400)\n",
      "54 100 Loss: 1.269 | Acc: 53.836% (2961/5500)\n",
      "55 100 Loss: 1.273 | Acc: 53.768% (3011/5600)\n",
      "56 100 Loss: 1.271 | Acc: 53.895% (3072/5700)\n",
      "57 100 Loss: 1.267 | Acc: 54.052% (3135/5800)\n",
      "58 100 Loss: 1.268 | Acc: 53.949% (3183/5900)\n",
      "59 100 Loss: 1.266 | Acc: 53.950% (3237/6000)\n",
      "60 100 Loss: 1.266 | Acc: 53.885% (3287/6100)\n",
      "61 100 Loss: 1.267 | Acc: 53.823% (3337/6200)\n",
      "62 100 Loss: 1.265 | Acc: 53.905% (3396/6300)\n",
      "63 100 Loss: 1.265 | Acc: 53.922% (3451/6400)\n",
      "64 100 Loss: 1.267 | Acc: 53.815% (3498/6500)\n",
      "65 100 Loss: 1.267 | Acc: 53.848% (3554/6600)\n",
      "66 100 Loss: 1.265 | Acc: 53.896% (3611/6700)\n",
      "67 100 Loss: 1.266 | Acc: 53.853% (3662/6800)\n",
      "68 100 Loss: 1.267 | Acc: 53.783% (3711/6900)\n",
      "69 100 Loss: 1.268 | Acc: 53.743% (3762/7000)\n",
      "70 100 Loss: 1.268 | Acc: 53.676% (3811/7100)\n",
      "71 100 Loss: 1.268 | Acc: 53.764% (3871/7200)\n",
      "72 100 Loss: 1.267 | Acc: 53.767% (3925/7300)\n",
      "73 100 Loss: 1.265 | Acc: 53.770% (3979/7400)\n",
      "74 100 Loss: 1.267 | Acc: 53.627% (4022/7500)\n",
      "75 100 Loss: 1.266 | Acc: 53.632% (4076/7600)\n",
      "76 100 Loss: 1.266 | Acc: 53.636% (4130/7700)\n",
      "77 100 Loss: 1.265 | Acc: 53.628% (4183/7800)\n",
      "78 100 Loss: 1.264 | Acc: 53.696% (4242/7900)\n",
      "79 100 Loss: 1.265 | Acc: 53.663% (4293/8000)\n",
      "80 100 Loss: 1.265 | Acc: 53.642% (4345/8100)\n",
      "81 100 Loss: 1.266 | Acc: 53.659% (4400/8200)\n",
      "82 100 Loss: 1.267 | Acc: 53.614% (4450/8300)\n",
      "83 100 Loss: 1.268 | Acc: 53.548% (4498/8400)\n",
      "84 100 Loss: 1.269 | Acc: 53.529% (4550/8500)\n",
      "85 100 Loss: 1.267 | Acc: 53.628% (4612/8600)\n",
      "86 100 Loss: 1.270 | Acc: 53.598% (4663/8700)\n",
      "87 100 Loss: 1.269 | Acc: 53.693% (4725/8800)\n",
      "88 100 Loss: 1.270 | Acc: 53.719% (4781/8900)\n",
      "89 100 Loss: 1.270 | Acc: 53.689% (4832/9000)\n",
      "90 100 Loss: 1.269 | Acc: 53.604% (4878/9100)\n",
      "91 100 Loss: 1.268 | Acc: 53.663% (4937/9200)\n",
      "92 100 Loss: 1.267 | Acc: 53.753% (4999/9300)\n",
      "93 100 Loss: 1.267 | Acc: 53.777% (5055/9400)\n",
      "94 100 Loss: 1.267 | Acc: 53.758% (5107/9500)\n",
      "95 100 Loss: 1.267 | Acc: 53.698% (5155/9600)\n",
      "96 100 Loss: 1.267 | Acc: 53.742% (5213/9700)\n",
      "97 100 Loss: 1.268 | Acc: 53.694% (5262/9800)\n",
      "98 100 Loss: 1.269 | Acc: 53.566% (5303/9900)\n",
      "99 100 Loss: 1.270 | Acc: 53.500% (5350/10000)\n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    train(epoch, resnet)\n",
    "    test(epoch, resnet)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_acc = best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "0 391 Loss: 2.411 | Acc: 8.594% (11/128)\n",
      "1 391 Loss: 2.380 | Acc: 9.766% (25/256)\n",
      "2 391 Loss: 2.381 | Acc: 9.635% (37/384)\n",
      "3 391 Loss: 2.374 | Acc: 8.984% (46/512)\n",
      "4 391 Loss: 2.371 | Acc: 8.594% (55/640)\n",
      "5 391 Loss: 2.373 | Acc: 7.943% (61/768)\n",
      "6 391 Loss: 2.373 | Acc: 7.701% (69/896)\n",
      "7 391 Loss: 2.372 | Acc: 8.105% (83/1024)\n",
      "8 391 Loss: 2.367 | Acc: 8.941% (103/1152)\n",
      "9 391 Loss: 2.363 | Acc: 9.453% (121/1280)\n",
      "10 391 Loss: 2.355 | Acc: 9.943% (140/1408)\n",
      "11 391 Loss: 2.355 | Acc: 9.961% (153/1536)\n",
      "12 391 Loss: 2.355 | Acc: 10.337% (172/1664)\n",
      "13 391 Loss: 2.354 | Acc: 10.547% (189/1792)\n",
      "14 391 Loss: 2.353 | Acc: 10.521% (202/1920)\n",
      "15 391 Loss: 2.350 | Acc: 10.742% (220/2048)\n",
      "16 391 Loss: 2.347 | Acc: 11.029% (240/2176)\n",
      "17 391 Loss: 2.345 | Acc: 11.111% (256/2304)\n",
      "18 391 Loss: 2.340 | Acc: 11.184% (272/2432)\n",
      "19 391 Loss: 2.337 | Acc: 11.445% (293/2560)\n",
      "20 391 Loss: 2.337 | Acc: 11.310% (304/2688)\n",
      "21 391 Loss: 2.335 | Acc: 11.435% (322/2816)\n",
      "22 391 Loss: 2.334 | Acc: 11.583% (341/2944)\n",
      "23 391 Loss: 2.332 | Acc: 11.816% (363/3072)\n",
      "24 391 Loss: 2.331 | Acc: 11.750% (376/3200)\n",
      "25 391 Loss: 2.331 | Acc: 11.809% (393/3328)\n",
      "26 391 Loss: 2.328 | Acc: 12.008% (415/3456)\n",
      "27 391 Loss: 2.325 | Acc: 12.165% (436/3584)\n",
      "28 391 Loss: 2.325 | Acc: 12.231% (454/3712)\n",
      "29 391 Loss: 2.322 | Acc: 12.396% (476/3840)\n",
      "30 391 Loss: 2.321 | Acc: 12.525% (497/3968)\n",
      "31 391 Loss: 2.320 | Acc: 12.524% (513/4096)\n",
      "32 391 Loss: 2.317 | Acc: 12.618% (533/4224)\n",
      "33 391 Loss: 2.316 | Acc: 12.592% (548/4352)\n",
      "34 391 Loss: 2.315 | Acc: 12.567% (563/4480)\n",
      "35 391 Loss: 2.315 | Acc: 12.630% (582/4608)\n",
      "36 391 Loss: 2.314 | Acc: 12.648% (599/4736)\n",
      "37 391 Loss: 2.313 | Acc: 12.850% (625/4864)\n",
      "38 391 Loss: 2.312 | Acc: 13.041% (651/4992)\n",
      "39 391 Loss: 2.310 | Acc: 13.125% (672/5120)\n",
      "40 391 Loss: 2.309 | Acc: 13.110% (688/5248)\n",
      "41 391 Loss: 2.307 | Acc: 13.170% (708/5376)\n",
      "42 391 Loss: 2.307 | Acc: 13.190% (726/5504)\n",
      "43 391 Loss: 2.306 | Acc: 13.192% (743/5632)\n",
      "44 391 Loss: 2.305 | Acc: 13.212% (761/5760)\n",
      "45 391 Loss: 2.305 | Acc: 13.179% (776/5888)\n",
      "46 391 Loss: 2.304 | Acc: 13.065% (786/6016)\n",
      "47 391 Loss: 2.304 | Acc: 13.021% (800/6144)\n",
      "48 391 Loss: 2.304 | Acc: 12.994% (815/6272)\n",
      "49 391 Loss: 2.303 | Acc: 13.109% (839/6400)\n",
      "50 391 Loss: 2.302 | Acc: 13.051% (852/6528)\n",
      "51 391 Loss: 2.302 | Acc: 13.101% (872/6656)\n",
      "52 391 Loss: 2.301 | Acc: 13.104% (889/6784)\n",
      "53 391 Loss: 2.301 | Acc: 13.122% (907/6912)\n",
      "54 391 Loss: 2.301 | Acc: 13.111% (923/7040)\n",
      "55 391 Loss: 2.300 | Acc: 13.072% (937/7168)\n",
      "56 391 Loss: 2.299 | Acc: 13.130% (958/7296)\n",
      "57 391 Loss: 2.298 | Acc: 13.120% (974/7424)\n",
      "58 391 Loss: 2.298 | Acc: 13.083% (988/7552)\n",
      "59 391 Loss: 2.298 | Acc: 13.060% (1003/7680)\n",
      "60 391 Loss: 2.297 | Acc: 13.051% (1019/7808)\n",
      "61 391 Loss: 2.297 | Acc: 13.054% (1036/7936)\n",
      "62 391 Loss: 2.296 | Acc: 13.170% (1062/8064)\n",
      "63 391 Loss: 2.295 | Acc: 13.208% (1082/8192)\n",
      "64 391 Loss: 2.295 | Acc: 13.233% (1101/8320)\n",
      "65 391 Loss: 2.294 | Acc: 13.234% (1118/8448)\n",
      "66 391 Loss: 2.293 | Acc: 13.246% (1136/8576)\n",
      "67 391 Loss: 2.293 | Acc: 13.201% (1149/8704)\n",
      "68 391 Loss: 2.293 | Acc: 13.179% (1164/8832)\n",
      "69 391 Loss: 2.293 | Acc: 13.136% (1177/8960)\n",
      "70 391 Loss: 2.292 | Acc: 13.171% (1197/9088)\n",
      "71 391 Loss: 2.292 | Acc: 13.227% (1219/9216)\n",
      "72 391 Loss: 2.291 | Acc: 13.324% (1245/9344)\n",
      "73 391 Loss: 2.291 | Acc: 13.355% (1265/9472)\n",
      "74 391 Loss: 2.290 | Acc: 13.396% (1286/9600)\n",
      "75 391 Loss: 2.290 | Acc: 13.384% (1302/9728)\n",
      "76 391 Loss: 2.290 | Acc: 13.393% (1320/9856)\n",
      "77 391 Loss: 2.290 | Acc: 13.401% (1338/9984)\n",
      "78 391 Loss: 2.289 | Acc: 13.449% (1360/10112)\n",
      "79 391 Loss: 2.289 | Acc: 13.428% (1375/10240)\n",
      "80 391 Loss: 2.289 | Acc: 13.503% (1400/10368)\n",
      "81 391 Loss: 2.288 | Acc: 13.491% (1416/10496)\n",
      "82 391 Loss: 2.289 | Acc: 13.479% (1432/10624)\n",
      "83 391 Loss: 2.289 | Acc: 13.486% (1450/10752)\n",
      "84 391 Loss: 2.288 | Acc: 13.566% (1476/10880)\n",
      "85 391 Loss: 2.288 | Acc: 13.581% (1495/11008)\n",
      "86 391 Loss: 2.288 | Acc: 13.614% (1516/11136)\n",
      "87 391 Loss: 2.287 | Acc: 13.565% (1528/11264)\n",
      "88 391 Loss: 2.287 | Acc: 13.606% (1550/11392)\n",
      "89 391 Loss: 2.287 | Acc: 13.620% (1569/11520)\n",
      "90 391 Loss: 2.287 | Acc: 13.668% (1592/11648)\n",
      "91 391 Loss: 2.286 | Acc: 13.663% (1609/11776)\n",
      "92 391 Loss: 2.286 | Acc: 13.768% (1639/11904)\n",
      "93 391 Loss: 2.286 | Acc: 13.780% (1658/12032)\n",
      "94 391 Loss: 2.285 | Acc: 13.783% (1676/12160)\n",
      "95 391 Loss: 2.285 | Acc: 13.818% (1698/12288)\n",
      "96 391 Loss: 2.285 | Acc: 13.877% (1723/12416)\n",
      "97 391 Loss: 2.284 | Acc: 13.927% (1747/12544)\n",
      "98 391 Loss: 2.284 | Acc: 13.984% (1772/12672)\n",
      "99 391 Loss: 2.283 | Acc: 14.016% (1794/12800)\n",
      "100 391 Loss: 2.283 | Acc: 13.993% (1809/12928)\n",
      "101 391 Loss: 2.283 | Acc: 14.032% (1832/13056)\n",
      "102 391 Loss: 2.283 | Acc: 14.055% (1853/13184)\n",
      "103 391 Loss: 2.283 | Acc: 14.062% (1872/13312)\n",
      "104 391 Loss: 2.282 | Acc: 14.107% (1896/13440)\n",
      "105 391 Loss: 2.282 | Acc: 14.121% (1916/13568)\n",
      "106 391 Loss: 2.282 | Acc: 14.106% (1932/13696)\n",
      "107 391 Loss: 2.282 | Acc: 14.128% (1953/13824)\n",
      "108 391 Loss: 2.281 | Acc: 14.177% (1978/13952)\n",
      "109 391 Loss: 2.281 | Acc: 14.247% (2006/14080)\n",
      "110 391 Loss: 2.281 | Acc: 14.224% (2021/14208)\n",
      "111 391 Loss: 2.281 | Acc: 14.188% (2034/14336)\n",
      "112 391 Loss: 2.281 | Acc: 14.208% (2055/14464)\n",
      "113 391 Loss: 2.280 | Acc: 14.206% (2073/14592)\n",
      "114 391 Loss: 2.280 | Acc: 14.253% (2098/14720)\n",
      "115 391 Loss: 2.280 | Acc: 14.291% (2122/14848)\n",
      "116 391 Loss: 2.279 | Acc: 14.269% (2137/14976)\n",
      "117 391 Loss: 2.279 | Acc: 14.261% (2154/15104)\n",
      "118 391 Loss: 2.279 | Acc: 14.286% (2176/15232)\n",
      "119 391 Loss: 2.279 | Acc: 14.303% (2197/15360)\n",
      "120 391 Loss: 2.279 | Acc: 14.301% (2215/15488)\n",
      "121 391 Loss: 2.279 | Acc: 14.287% (2231/15616)\n",
      "122 391 Loss: 2.279 | Acc: 14.253% (2244/15744)\n",
      "123 391 Loss: 2.278 | Acc: 14.296% (2269/15872)\n",
      "124 391 Loss: 2.278 | Acc: 14.300% (2288/16000)\n",
      "125 391 Loss: 2.278 | Acc: 14.298% (2306/16128)\n",
      "126 391 Loss: 2.278 | Acc: 14.309% (2326/16256)\n",
      "127 391 Loss: 2.278 | Acc: 14.313% (2345/16384)\n",
      "128 391 Loss: 2.277 | Acc: 14.305% (2362/16512)\n",
      "129 391 Loss: 2.277 | Acc: 14.315% (2382/16640)\n",
      "130 391 Loss: 2.277 | Acc: 14.295% (2397/16768)\n",
      "131 391 Loss: 2.276 | Acc: 14.299% (2416/16896)\n",
      "132 391 Loss: 2.276 | Acc: 14.292% (2433/17024)\n",
      "133 391 Loss: 2.276 | Acc: 14.302% (2453/17152)\n",
      "134 391 Loss: 2.276 | Acc: 14.265% (2465/17280)\n",
      "135 391 Loss: 2.276 | Acc: 14.269% (2484/17408)\n",
      "136 391 Loss: 2.275 | Acc: 14.279% (2504/17536)\n",
      "137 391 Loss: 2.275 | Acc: 14.278% (2522/17664)\n",
      "138 391 Loss: 2.275 | Acc: 14.276% (2540/17792)\n",
      "139 391 Loss: 2.275 | Acc: 14.258% (2555/17920)\n",
      "140 391 Loss: 2.275 | Acc: 14.262% (2574/18048)\n",
      "141 391 Loss: 2.275 | Acc: 14.239% (2588/18176)\n",
      "142 391 Loss: 2.274 | Acc: 14.276% (2613/18304)\n",
      "143 391 Loss: 2.274 | Acc: 14.323% (2640/18432)\n",
      "144 391 Loss: 2.273 | Acc: 14.332% (2660/18560)\n",
      "145 391 Loss: 2.273 | Acc: 14.314% (2675/18688)\n",
      "146 391 Loss: 2.273 | Acc: 14.339% (2698/18816)\n",
      "147 391 Loss: 2.273 | Acc: 14.353% (2719/18944)\n",
      "148 391 Loss: 2.272 | Acc: 14.361% (2739/19072)\n",
      "149 391 Loss: 2.272 | Acc: 14.396% (2764/19200)\n",
      "150 391 Loss: 2.272 | Acc: 14.419% (2787/19328)\n",
      "151 391 Loss: 2.271 | Acc: 14.448% (2811/19456)\n",
      "152 391 Loss: 2.272 | Acc: 14.440% (2828/19584)\n",
      "153 391 Loss: 2.271 | Acc: 14.478% (2854/19712)\n",
      "154 391 Loss: 2.271 | Acc: 14.521% (2881/19840)\n",
      "155 391 Loss: 2.271 | Acc: 14.538% (2903/19968)\n",
      "156 391 Loss: 2.270 | Acc: 14.595% (2933/20096)\n",
      "157 391 Loss: 2.270 | Acc: 14.636% (2960/20224)\n",
      "158 391 Loss: 2.269 | Acc: 14.691% (2990/20352)\n",
      "159 391 Loss: 2.269 | Acc: 14.688% (3008/20480)\n",
      "160 391 Loss: 2.269 | Acc: 14.703% (3030/20608)\n",
      "161 391 Loss: 2.269 | Acc: 14.738% (3056/20736)\n",
      "162 391 Loss: 2.268 | Acc: 14.762% (3080/20864)\n",
      "163 391 Loss: 2.268 | Acc: 14.758% (3098/20992)\n",
      "164 391 Loss: 2.268 | Acc: 14.777% (3121/21120)\n",
      "165 391 Loss: 2.268 | Acc: 14.778% (3140/21248)\n",
      "166 391 Loss: 2.268 | Acc: 14.783% (3160/21376)\n",
      "167 391 Loss: 2.268 | Acc: 14.765% (3175/21504)\n",
      "168 391 Loss: 2.268 | Acc: 14.774% (3196/21632)\n",
      "169 391 Loss: 2.267 | Acc: 14.766% (3213/21760)\n",
      "170 391 Loss: 2.267 | Acc: 14.784% (3236/21888)\n",
      "171 391 Loss: 2.267 | Acc: 14.794% (3257/22016)\n",
      "172 391 Loss: 2.267 | Acc: 14.812% (3280/22144)\n",
      "173 391 Loss: 2.266 | Acc: 14.839% (3305/22272)\n",
      "174 391 Loss: 2.266 | Acc: 14.844% (3325/22400)\n",
      "175 391 Loss: 2.266 | Acc: 14.862% (3348/22528)\n",
      "176 391 Loss: 2.266 | Acc: 14.897% (3375/22656)\n",
      "177 391 Loss: 2.265 | Acc: 14.905% (3396/22784)\n",
      "178 391 Loss: 2.265 | Acc: 14.918% (3418/22912)\n",
      "179 391 Loss: 2.265 | Acc: 14.905% (3434/23040)\n",
      "180 391 Loss: 2.265 | Acc: 14.913% (3455/23168)\n",
      "181 391 Loss: 2.265 | Acc: 14.908% (3473/23296)\n",
      "182 391 Loss: 2.264 | Acc: 14.912% (3493/23424)\n",
      "183 391 Loss: 2.264 | Acc: 14.920% (3514/23552)\n",
      "184 391 Loss: 2.264 | Acc: 14.932% (3536/23680)\n",
      "185 391 Loss: 2.264 | Acc: 14.928% (3554/23808)\n",
      "186 391 Loss: 2.263 | Acc: 14.961% (3581/23936)\n",
      "187 391 Loss: 2.263 | Acc: 14.977% (3604/24064)\n",
      "188 391 Loss: 2.263 | Acc: 14.993% (3627/24192)\n",
      "189 391 Loss: 2.263 | Acc: 15.008% (3650/24320)\n",
      "190 391 Loss: 2.263 | Acc: 14.983% (3663/24448)\n",
      "191 391 Loss: 2.262 | Acc: 14.994% (3685/24576)\n",
      "192 391 Loss: 2.262 | Acc: 15.002% (3706/24704)\n",
      "193 391 Loss: 2.262 | Acc: 14.989% (3722/24832)\n",
      "194 391 Loss: 2.262 | Acc: 15.016% (3748/24960)\n",
      "195 391 Loss: 2.262 | Acc: 15.023% (3769/25088)\n",
      "196 391 Loss: 2.262 | Acc: 15.046% (3794/25216)\n",
      "197 391 Loss: 2.261 | Acc: 15.049% (3814/25344)\n",
      "198 391 Loss: 2.261 | Acc: 15.040% (3831/25472)\n",
      "199 391 Loss: 2.261 | Acc: 15.039% (3850/25600)\n",
      "200 391 Loss: 2.261 | Acc: 15.058% (3874/25728)\n",
      "201 391 Loss: 2.261 | Acc: 15.033% (3887/25856)\n",
      "202 391 Loss: 2.261 | Acc: 15.044% (3909/25984)\n",
      "203 391 Loss: 2.261 | Acc: 15.058% (3932/26112)\n",
      "204 391 Loss: 2.261 | Acc: 15.057% (3951/26240)\n",
      "205 391 Loss: 2.261 | Acc: 15.098% (3981/26368)\n",
      "206 391 Loss: 2.260 | Acc: 15.119% (4006/26496)\n",
      "207 391 Loss: 2.260 | Acc: 15.129% (4028/26624)\n",
      "208 391 Loss: 2.260 | Acc: 15.128% (4047/26752)\n",
      "209 391 Loss: 2.259 | Acc: 15.141% (4070/26880)\n",
      "210 391 Loss: 2.259 | Acc: 15.125% (4085/27008)\n",
      "211 391 Loss: 2.259 | Acc: 15.146% (4110/27136)\n",
      "212 391 Loss: 2.259 | Acc: 15.170% (4136/27264)\n",
      "213 391 Loss: 2.258 | Acc: 15.176% (4157/27392)\n",
      "214 391 Loss: 2.258 | Acc: 15.174% (4176/27520)\n",
      "215 391 Loss: 2.258 | Acc: 15.195% (4201/27648)\n",
      "216 391 Loss: 2.258 | Acc: 15.243% (4234/27776)\n",
      "217 391 Loss: 2.258 | Acc: 15.252% (4256/27904)\n",
      "218 391 Loss: 2.257 | Acc: 15.268% (4280/28032)\n",
      "219 391 Loss: 2.257 | Acc: 15.273% (4301/28160)\n",
      "220 391 Loss: 2.257 | Acc: 15.282% (4323/28288)\n",
      "221 391 Loss: 2.257 | Acc: 15.305% (4349/28416)\n",
      "222 391 Loss: 2.257 | Acc: 15.348% (4381/28544)\n",
      "223 391 Loss: 2.256 | Acc: 15.363% (4405/28672)\n",
      "224 391 Loss: 2.256 | Acc: 15.361% (4424/28800)\n",
      "225 391 Loss: 2.256 | Acc: 15.386% (4451/28928)\n",
      "226 391 Loss: 2.256 | Acc: 15.415% (4479/29056)\n",
      "227 391 Loss: 2.256 | Acc: 15.406% (4496/29184)\n",
      "228 391 Loss: 2.256 | Acc: 15.413% (4518/29312)\n",
      "229 391 Loss: 2.255 | Acc: 15.411% (4537/29440)\n",
      "230 391 Loss: 2.255 | Acc: 15.419% (4559/29568)\n",
      "231 391 Loss: 2.255 | Acc: 15.426% (4581/29696)\n",
      "232 391 Loss: 2.255 | Acc: 15.417% (4598/29824)\n",
      "233 391 Loss: 2.255 | Acc: 15.415% (4617/29952)\n",
      "234 391 Loss: 2.254 | Acc: 15.426% (4640/30080)\n",
      "235 391 Loss: 2.254 | Acc: 15.440% (4664/30208)\n",
      "236 391 Loss: 2.254 | Acc: 15.444% (4685/30336)\n",
      "237 391 Loss: 2.254 | Acc: 15.461% (4710/30464)\n",
      "238 391 Loss: 2.254 | Acc: 15.481% (4736/30592)\n",
      "239 391 Loss: 2.254 | Acc: 15.475% (4754/30720)\n",
      "240 391 Loss: 2.254 | Acc: 15.450% (4766/30848)\n",
      "241 391 Loss: 2.254 | Acc: 15.435% (4781/30976)\n",
      "242 391 Loss: 2.254 | Acc: 15.416% (4795/31104)\n",
      "243 391 Loss: 2.253 | Acc: 15.465% (4830/31232)\n",
      "244 391 Loss: 2.253 | Acc: 15.469% (4851/31360)\n",
      "245 391 Loss: 2.253 | Acc: 15.482% (4875/31488)\n",
      "246 391 Loss: 2.252 | Acc: 15.508% (4903/31616)\n",
      "247 391 Loss: 2.252 | Acc: 15.524% (4928/31744)\n",
      "248 391 Loss: 2.252 | Acc: 15.543% (4954/31872)\n",
      "249 391 Loss: 2.252 | Acc: 15.544% (4974/32000)\n",
      "250 391 Loss: 2.251 | Acc: 15.572% (5003/32128)\n",
      "251 391 Loss: 2.251 | Acc: 15.569% (5022/32256)\n",
      "252 391 Loss: 2.251 | Acc: 15.600% (5052/32384)\n",
      "253 391 Loss: 2.251 | Acc: 15.603% (5073/32512)\n",
      "254 391 Loss: 2.251 | Acc: 15.610% (5095/32640)\n",
      "255 391 Loss: 2.251 | Acc: 15.607% (5114/32768)\n",
      "256 391 Loss: 2.250 | Acc: 15.616% (5137/32896)\n",
      "257 391 Loss: 2.250 | Acc: 15.637% (5164/33024)\n",
      "258 391 Loss: 2.250 | Acc: 15.628% (5181/33152)\n",
      "259 391 Loss: 2.250 | Acc: 15.676% (5217/33280)\n",
      "260 391 Loss: 2.250 | Acc: 15.697% (5244/33408)\n",
      "261 391 Loss: 2.249 | Acc: 15.729% (5275/33536)\n",
      "262 391 Loss: 2.249 | Acc: 15.738% (5298/33664)\n",
      "263 391 Loss: 2.249 | Acc: 15.746% (5321/33792)\n",
      "264 391 Loss: 2.249 | Acc: 15.767% (5348/33920)\n",
      "265 391 Loss: 2.249 | Acc: 15.766% (5368/34048)\n",
      "266 391 Loss: 2.248 | Acc: 15.786% (5395/34176)\n",
      "267 391 Loss: 2.248 | Acc: 15.797% (5419/34304)\n",
      "268 391 Loss: 2.248 | Acc: 15.776% (5432/34432)\n",
      "269 391 Loss: 2.248 | Acc: 15.799% (5460/34560)\n",
      "270 391 Loss: 2.248 | Acc: 15.833% (5492/34688)\n",
      "271 391 Loss: 2.247 | Acc: 15.838% (5514/34816)\n",
      "272 391 Loss: 2.247 | Acc: 15.848% (5538/34944)\n",
      "273 391 Loss: 2.247 | Acc: 15.853% (5560/35072)\n",
      "274 391 Loss: 2.247 | Acc: 15.855% (5581/35200)\n",
      "275 391 Loss: 2.247 | Acc: 15.857% (5602/35328)\n",
      "276 391 Loss: 2.247 | Acc: 15.868% (5626/35456)\n",
      "277 391 Loss: 2.247 | Acc: 15.878% (5650/35584)\n",
      "278 391 Loss: 2.246 | Acc: 15.888% (5674/35712)\n",
      "279 391 Loss: 2.246 | Acc: 15.882% (5692/35840)\n",
      "280 391 Loss: 2.246 | Acc: 15.875% (5710/35968)\n",
      "281 391 Loss: 2.246 | Acc: 15.883% (5733/36096)\n",
      "282 391 Loss: 2.246 | Acc: 15.920% (5767/36224)\n",
      "283 391 Loss: 2.245 | Acc: 15.939% (5794/36352)\n",
      "284 391 Loss: 2.245 | Acc: 15.962% (5823/36480)\n",
      "285 391 Loss: 2.245 | Acc: 15.997% (5856/36608)\n",
      "286 391 Loss: 2.245 | Acc: 16.003% (5879/36736)\n",
      "287 391 Loss: 2.245 | Acc: 15.999% (5898/36864)\n",
      "288 391 Loss: 2.244 | Acc: 16.014% (5924/36992)\n",
      "289 391 Loss: 2.244 | Acc: 16.032% (5951/37120)\n",
      "290 391 Loss: 2.244 | Acc: 16.041% (5975/37248)\n",
      "291 391 Loss: 2.244 | Acc: 16.053% (6000/37376)\n",
      "292 391 Loss: 2.244 | Acc: 16.057% (6022/37504)\n",
      "293 391 Loss: 2.244 | Acc: 16.061% (6044/37632)\n",
      "294 391 Loss: 2.244 | Acc: 16.067% (6067/37760)\n",
      "295 391 Loss: 2.243 | Acc: 16.095% (6098/37888)\n",
      "296 391 Loss: 2.243 | Acc: 16.122% (6129/38016)\n",
      "297 391 Loss: 2.243 | Acc: 16.128% (6152/38144)\n",
      "298 391 Loss: 2.243 | Acc: 16.129% (6173/38272)\n",
      "299 391 Loss: 2.242 | Acc: 16.146% (6200/38400)\n",
      "300 391 Loss: 2.242 | Acc: 16.162% (6227/38528)\n",
      "301 391 Loss: 2.242 | Acc: 16.171% (6251/38656)\n",
      "302 391 Loss: 2.241 | Acc: 16.195% (6281/38784)\n",
      "303 391 Loss: 2.241 | Acc: 16.216% (6310/38912)\n",
      "304 391 Loss: 2.241 | Acc: 16.219% (6332/39040)\n",
      "305 391 Loss: 2.241 | Acc: 16.238% (6360/39168)\n",
      "306 391 Loss: 2.240 | Acc: 16.261% (6390/39296)\n",
      "307 391 Loss: 2.240 | Acc: 16.269% (6414/39424)\n",
      "308 391 Loss: 2.240 | Acc: 16.277% (6438/39552)\n",
      "309 391 Loss: 2.240 | Acc: 16.285% (6462/39680)\n",
      "310 391 Loss: 2.240 | Acc: 16.278% (6480/39808)\n",
      "311 391 Loss: 2.240 | Acc: 16.279% (6501/39936)\n",
      "312 391 Loss: 2.240 | Acc: 16.279% (6522/40064)\n",
      "313 391 Loss: 2.240 | Acc: 16.297% (6550/40192)\n",
      "314 391 Loss: 2.239 | Acc: 16.312% (6577/40320)\n",
      "315 391 Loss: 2.239 | Acc: 16.322% (6602/40448)\n",
      "316 391 Loss: 2.239 | Acc: 16.335% (6628/40576)\n",
      "317 391 Loss: 2.239 | Acc: 16.360% (6659/40704)\n",
      "318 391 Loss: 2.238 | Acc: 16.372% (6685/40832)\n",
      "319 391 Loss: 2.238 | Acc: 16.401% (6718/40960)\n",
      "320 391 Loss: 2.238 | Acc: 16.406% (6741/41088)\n",
      "321 391 Loss: 2.237 | Acc: 16.406% (6762/41216)\n",
      "322 391 Loss: 2.237 | Acc: 16.435% (6795/41344)\n",
      "323 391 Loss: 2.237 | Acc: 16.442% (6819/41472)\n",
      "324 391 Loss: 2.237 | Acc: 16.442% (6840/41600)\n",
      "325 391 Loss: 2.237 | Acc: 16.447% (6863/41728)\n",
      "326 391 Loss: 2.237 | Acc: 16.454% (6887/41856)\n",
      "327 391 Loss: 2.236 | Acc: 16.463% (6912/41984)\n",
      "328 391 Loss: 2.236 | Acc: 16.466% (6934/42112)\n",
      "329 391 Loss: 2.236 | Acc: 16.487% (6964/42240)\n",
      "330 391 Loss: 2.236 | Acc: 16.510% (6995/42368)\n",
      "331 391 Loss: 2.235 | Acc: 16.510% (7016/42496)\n",
      "332 391 Loss: 2.235 | Acc: 16.531% (7046/42624)\n",
      "333 391 Loss: 2.235 | Acc: 16.540% (7071/42752)\n",
      "334 391 Loss: 2.235 | Acc: 16.542% (7093/42880)\n",
      "335 391 Loss: 2.234 | Acc: 16.557% (7121/43008)\n",
      "336 391 Loss: 2.234 | Acc: 16.564% (7145/43136)\n",
      "337 391 Loss: 2.234 | Acc: 16.559% (7164/43264)\n",
      "338 391 Loss: 2.234 | Acc: 16.577% (7193/43392)\n",
      "339 391 Loss: 2.234 | Acc: 16.602% (7225/43520)\n",
      "340 391 Loss: 2.234 | Acc: 16.624% (7256/43648)\n",
      "341 391 Loss: 2.234 | Acc: 16.621% (7276/43776)\n",
      "342 391 Loss: 2.234 | Acc: 16.620% (7297/43904)\n",
      "343 391 Loss: 2.234 | Acc: 16.642% (7328/44032)\n",
      "344 391 Loss: 2.233 | Acc: 16.642% (7349/44160)\n",
      "345 391 Loss: 2.233 | Acc: 16.652% (7375/44288)\n",
      "346 391 Loss: 2.233 | Acc: 16.649% (7395/44416)\n",
      "347 391 Loss: 2.233 | Acc: 16.660% (7421/44544)\n",
      "348 391 Loss: 2.232 | Acc: 16.682% (7452/44672)\n",
      "349 391 Loss: 2.232 | Acc: 16.699% (7481/44800)\n",
      "350 391 Loss: 2.232 | Acc: 16.716% (7510/44928)\n",
      "351 391 Loss: 2.232 | Acc: 16.735% (7540/45056)\n",
      "352 391 Loss: 2.232 | Acc: 16.743% (7565/45184)\n",
      "353 391 Loss: 2.231 | Acc: 16.770% (7599/45312)\n",
      "354 391 Loss: 2.231 | Acc: 16.765% (7618/45440)\n",
      "355 391 Loss: 2.231 | Acc: 16.771% (7642/45568)\n",
      "356 391 Loss: 2.231 | Acc: 16.767% (7662/45696)\n",
      "357 391 Loss: 2.231 | Acc: 16.771% (7685/45824)\n",
      "358 391 Loss: 2.231 | Acc: 16.796% (7718/45952)\n",
      "359 391 Loss: 2.230 | Acc: 16.808% (7745/46080)\n",
      "360 391 Loss: 2.230 | Acc: 16.800% (7763/46208)\n",
      "361 391 Loss: 2.230 | Acc: 16.816% (7792/46336)\n",
      "362 391 Loss: 2.230 | Acc: 16.819% (7815/46464)\n",
      "363 391 Loss: 2.230 | Acc: 16.840% (7846/46592)\n",
      "364 391 Loss: 2.229 | Acc: 16.860% (7877/46720)\n",
      "365 391 Loss: 2.229 | Acc: 16.872% (7904/46848)\n",
      "366 391 Loss: 2.229 | Acc: 16.858% (7919/46976)\n",
      "367 391 Loss: 2.229 | Acc: 16.867% (7945/47104)\n",
      "368 391 Loss: 2.229 | Acc: 16.878% (7972/47232)\n",
      "369 391 Loss: 2.229 | Acc: 16.890% (7999/47360)\n",
      "370 391 Loss: 2.228 | Acc: 16.897% (8024/47488)\n",
      "371 391 Loss: 2.228 | Acc: 16.887% (8041/47616)\n",
      "372 391 Loss: 2.228 | Acc: 16.892% (8065/47744)\n",
      "373 391 Loss: 2.228 | Acc: 16.916% (8098/47872)\n",
      "374 391 Loss: 2.228 | Acc: 16.925% (8124/48000)\n",
      "375 391 Loss: 2.228 | Acc: 16.932% (8149/48128)\n",
      "376 391 Loss: 2.227 | Acc: 16.945% (8177/48256)\n",
      "377 391 Loss: 2.227 | Acc: 16.939% (8196/48384)\n",
      "378 391 Loss: 2.227 | Acc: 16.944% (8220/48512)\n",
      "379 391 Loss: 2.227 | Acc: 16.955% (8247/48640)\n",
      "380 391 Loss: 2.227 | Acc: 16.960% (8271/48768)\n",
      "381 391 Loss: 2.226 | Acc: 16.973% (8299/48896)\n",
      "382 391 Loss: 2.226 | Acc: 16.967% (8318/49024)\n",
      "383 391 Loss: 2.226 | Acc: 16.980% (8346/49152)\n",
      "384 391 Loss: 2.226 | Acc: 16.983% (8369/49280)\n",
      "385 391 Loss: 2.226 | Acc: 16.981% (8390/49408)\n",
      "386 391 Loss: 2.226 | Acc: 16.971% (8407/49536)\n",
      "387 391 Loss: 2.226 | Acc: 16.966% (8426/49664)\n",
      "388 391 Loss: 2.226 | Acc: 16.981% (8455/49792)\n",
      "389 391 Loss: 2.225 | Acc: 16.999% (8486/49920)\n",
      "390 391 Loss: 2.225 | Acc: 17.002% (8501/50000)\n",
      "0 100 Loss: 2.117 | Acc: 26.000% (26/100)\n",
      "1 100 Loss: 2.150 | Acc: 23.000% (46/200)\n",
      "2 100 Loss: 2.140 | Acc: 24.333% (73/300)\n",
      "3 100 Loss: 2.151 | Acc: 22.250% (89/400)\n",
      "4 100 Loss: 2.152 | Acc: 21.000% (105/500)\n",
      "5 100 Loss: 2.149 | Acc: 21.000% (126/600)\n",
      "6 100 Loss: 2.151 | Acc: 20.857% (146/700)\n",
      "7 100 Loss: 2.155 | Acc: 20.500% (164/800)\n",
      "8 100 Loss: 2.164 | Acc: 20.667% (186/900)\n",
      "9 100 Loss: 2.168 | Acc: 20.000% (200/1000)\n",
      "10 100 Loss: 2.168 | Acc: 20.636% (227/1100)\n",
      "11 100 Loss: 2.170 | Acc: 20.667% (248/1200)\n",
      "12 100 Loss: 2.166 | Acc: 20.692% (269/1300)\n",
      "13 100 Loss: 2.168 | Acc: 20.429% (286/1400)\n",
      "14 100 Loss: 2.169 | Acc: 20.400% (306/1500)\n",
      "15 100 Loss: 2.168 | Acc: 20.438% (327/1600)\n",
      "16 100 Loss: 2.162 | Acc: 20.353% (346/1700)\n",
      "17 100 Loss: 2.162 | Acc: 20.167% (363/1800)\n",
      "18 100 Loss: 2.160 | Acc: 20.421% (388/1900)\n",
      "19 100 Loss: 2.159 | Acc: 20.450% (409/2000)\n",
      "20 100 Loss: 2.159 | Acc: 20.429% (429/2100)\n",
      "21 100 Loss: 2.160 | Acc: 20.455% (450/2200)\n",
      "22 100 Loss: 2.159 | Acc: 20.304% (467/2300)\n",
      "23 100 Loss: 2.162 | Acc: 20.167% (484/2400)\n",
      "24 100 Loss: 2.157 | Acc: 20.320% (508/2500)\n",
      "25 100 Loss: 2.156 | Acc: 20.462% (532/2600)\n",
      "26 100 Loss: 2.154 | Acc: 20.556% (555/2700)\n",
      "27 100 Loss: 2.153 | Acc: 20.893% (585/2800)\n",
      "28 100 Loss: 2.153 | Acc: 20.828% (604/2900)\n",
      "29 100 Loss: 2.153 | Acc: 20.900% (627/3000)\n",
      "30 100 Loss: 2.152 | Acc: 20.774% (644/3100)\n",
      "31 100 Loss: 2.154 | Acc: 20.750% (664/3200)\n",
      "32 100 Loss: 2.154 | Acc: 20.727% (684/3300)\n",
      "33 100 Loss: 2.155 | Acc: 20.618% (701/3400)\n",
      "34 100 Loss: 2.155 | Acc: 20.486% (717/3500)\n",
      "35 100 Loss: 2.153 | Acc: 20.556% (740/3600)\n",
      "36 100 Loss: 2.152 | Acc: 20.541% (760/3700)\n",
      "37 100 Loss: 2.152 | Acc: 20.553% (781/3800)\n",
      "38 100 Loss: 2.154 | Acc: 20.333% (793/3900)\n",
      "39 100 Loss: 2.154 | Acc: 20.300% (812/4000)\n",
      "40 100 Loss: 2.153 | Acc: 20.317% (833/4100)\n",
      "41 100 Loss: 2.154 | Acc: 20.262% (851/4200)\n",
      "42 100 Loss: 2.156 | Acc: 20.186% (868/4300)\n",
      "43 100 Loss: 2.154 | Acc: 20.273% (892/4400)\n",
      "44 100 Loss: 2.154 | Acc: 20.156% (907/4500)\n",
      "45 100 Loss: 2.153 | Acc: 20.130% (926/4600)\n",
      "46 100 Loss: 2.153 | Acc: 20.191% (949/4700)\n",
      "47 100 Loss: 2.154 | Acc: 20.083% (964/4800)\n",
      "48 100 Loss: 2.154 | Acc: 20.061% (983/4900)\n",
      "49 100 Loss: 2.154 | Acc: 19.960% (998/5000)\n",
      "50 100 Loss: 2.154 | Acc: 19.882% (1014/5100)\n",
      "51 100 Loss: 2.154 | Acc: 19.808% (1030/5200)\n",
      "52 100 Loss: 2.154 | Acc: 19.830% (1051/5300)\n",
      "53 100 Loss: 2.155 | Acc: 19.704% (1064/5400)\n",
      "54 100 Loss: 2.156 | Acc: 19.709% (1084/5500)\n",
      "55 100 Loss: 2.156 | Acc: 19.607% (1098/5600)\n",
      "56 100 Loss: 2.157 | Acc: 19.614% (1118/5700)\n",
      "57 100 Loss: 2.156 | Acc: 19.690% (1142/5800)\n",
      "58 100 Loss: 2.156 | Acc: 19.644% (1159/5900)\n",
      "59 100 Loss: 2.157 | Acc: 19.700% (1182/6000)\n",
      "60 100 Loss: 2.157 | Acc: 19.656% (1199/6100)\n",
      "61 100 Loss: 2.158 | Acc: 19.532% (1211/6200)\n",
      "62 100 Loss: 2.158 | Acc: 19.524% (1230/6300)\n",
      "63 100 Loss: 2.157 | Acc: 19.562% (1252/6400)\n",
      "64 100 Loss: 2.157 | Acc: 19.508% (1268/6500)\n",
      "65 100 Loss: 2.158 | Acc: 19.409% (1281/6600)\n",
      "66 100 Loss: 2.158 | Acc: 19.463% (1304/6700)\n",
      "67 100 Loss: 2.158 | Acc: 19.441% (1322/6800)\n",
      "68 100 Loss: 2.157 | Acc: 19.536% (1348/6900)\n",
      "69 100 Loss: 2.157 | Acc: 19.471% (1363/7000)\n",
      "70 100 Loss: 2.157 | Acc: 19.437% (1380/7100)\n",
      "71 100 Loss: 2.157 | Acc: 19.417% (1398/7200)\n",
      "72 100 Loss: 2.157 | Acc: 19.425% (1418/7300)\n",
      "73 100 Loss: 2.156 | Acc: 19.486% (1442/7400)\n",
      "74 100 Loss: 2.155 | Acc: 19.427% (1457/7500)\n",
      "75 100 Loss: 2.156 | Acc: 19.382% (1473/7600)\n",
      "76 100 Loss: 2.155 | Acc: 19.351% (1490/7700)\n",
      "77 100 Loss: 2.155 | Acc: 19.449% (1517/7800)\n",
      "78 100 Loss: 2.156 | Acc: 19.367% (1530/7900)\n",
      "79 100 Loss: 2.157 | Acc: 19.250% (1540/8000)\n",
      "80 100 Loss: 2.156 | Acc: 19.272% (1561/8100)\n",
      "81 100 Loss: 2.156 | Acc: 19.268% (1580/8200)\n",
      "82 100 Loss: 2.156 | Acc: 19.217% (1595/8300)\n",
      "83 100 Loss: 2.156 | Acc: 19.274% (1619/8400)\n",
      "84 100 Loss: 2.155 | Acc: 19.341% (1644/8500)\n",
      "85 100 Loss: 2.155 | Acc: 19.419% (1670/8600)\n",
      "86 100 Loss: 2.155 | Acc: 19.414% (1689/8700)\n",
      "87 100 Loss: 2.155 | Acc: 19.420% (1709/8800)\n",
      "88 100 Loss: 2.154 | Acc: 19.539% (1739/8900)\n",
      "89 100 Loss: 2.154 | Acc: 19.556% (1760/9000)\n",
      "90 100 Loss: 2.154 | Acc: 19.495% (1774/9100)\n",
      "91 100 Loss: 2.154 | Acc: 19.522% (1796/9200)\n",
      "92 100 Loss: 2.154 | Acc: 19.516% (1815/9300)\n",
      "93 100 Loss: 2.154 | Acc: 19.457% (1829/9400)\n",
      "94 100 Loss: 2.153 | Acc: 19.474% (1850/9500)\n",
      "95 100 Loss: 2.153 | Acc: 19.510% (1873/9600)\n",
      "96 100 Loss: 2.152 | Acc: 19.546% (1896/9700)\n",
      "97 100 Loss: 2.152 | Acc: 19.510% (1912/9800)\n",
      "98 100 Loss: 2.151 | Acc: 19.505% (1931/9900)\n",
      "99 100 Loss: 2.151 | Acc: 19.570% (1957/10000)\n",
      "\n",
      "Epoch: 1\n",
      "0 391 Loss: 2.171 | Acc: 21.875% (28/128)\n",
      "1 391 Loss: 2.151 | Acc: 19.531% (50/256)\n",
      "2 391 Loss: 2.169 | Acc: 20.052% (77/384)\n",
      "3 391 Loss: 2.168 | Acc: 19.336% (99/512)\n",
      "4 391 Loss: 2.165 | Acc: 21.094% (135/640)\n",
      "5 391 Loss: 2.164 | Acc: 21.615% (166/768)\n",
      "6 391 Loss: 2.159 | Acc: 21.205% (190/896)\n",
      "7 391 Loss: 2.159 | Acc: 21.191% (217/1024)\n",
      "8 391 Loss: 2.158 | Acc: 21.007% (242/1152)\n",
      "9 391 Loss: 2.162 | Acc: 21.094% (270/1280)\n",
      "10 391 Loss: 2.162 | Acc: 20.881% (294/1408)\n",
      "11 391 Loss: 2.160 | Acc: 20.768% (319/1536)\n",
      "12 391 Loss: 2.161 | Acc: 20.733% (345/1664)\n",
      "13 391 Loss: 2.161 | Acc: 20.815% (373/1792)\n",
      "14 391 Loss: 2.159 | Acc: 20.625% (396/1920)\n",
      "15 391 Loss: 2.158 | Acc: 20.557% (421/2048)\n",
      "16 391 Loss: 2.158 | Acc: 20.634% (449/2176)\n",
      "17 391 Loss: 2.157 | Acc: 20.660% (476/2304)\n",
      "18 391 Loss: 2.157 | Acc: 20.724% (504/2432)\n",
      "19 391 Loss: 2.155 | Acc: 20.664% (529/2560)\n",
      "20 391 Loss: 2.156 | Acc: 20.759% (558/2688)\n",
      "21 391 Loss: 2.153 | Acc: 21.200% (597/2816)\n",
      "22 391 Loss: 2.150 | Acc: 21.467% (632/2944)\n",
      "23 391 Loss: 2.152 | Acc: 21.257% (653/3072)\n",
      "24 391 Loss: 2.150 | Acc: 21.062% (674/3200)\n",
      "25 391 Loss: 2.154 | Acc: 20.913% (696/3328)\n",
      "26 391 Loss: 2.156 | Acc: 20.804% (719/3456)\n",
      "27 391 Loss: 2.155 | Acc: 20.898% (749/3584)\n",
      "28 391 Loss: 2.156 | Acc: 20.690% (768/3712)\n",
      "29 391 Loss: 2.155 | Acc: 20.677% (794/3840)\n",
      "30 391 Loss: 2.156 | Acc: 20.716% (822/3968)\n",
      "31 391 Loss: 2.155 | Acc: 20.923% (857/4096)\n",
      "32 391 Loss: 2.155 | Acc: 20.975% (886/4224)\n",
      "33 391 Loss: 2.153 | Acc: 21.025% (915/4352)\n",
      "34 391 Loss: 2.152 | Acc: 21.049% (943/4480)\n",
      "35 391 Loss: 2.152 | Acc: 21.202% (977/4608)\n",
      "36 391 Loss: 2.152 | Acc: 21.326% (1010/4736)\n",
      "37 391 Loss: 2.152 | Acc: 21.340% (1038/4864)\n",
      "38 391 Loss: 2.152 | Acc: 21.294% (1063/4992)\n",
      "39 391 Loss: 2.151 | Acc: 21.406% (1096/5120)\n",
      "40 391 Loss: 2.150 | Acc: 21.380% (1122/5248)\n",
      "41 391 Loss: 2.150 | Acc: 21.391% (1150/5376)\n",
      "42 391 Loss: 2.150 | Acc: 21.421% (1179/5504)\n",
      "43 391 Loss: 2.150 | Acc: 21.449% (1208/5632)\n",
      "44 391 Loss: 2.150 | Acc: 21.372% (1231/5760)\n",
      "45 391 Loss: 2.149 | Acc: 21.298% (1254/5888)\n",
      "46 391 Loss: 2.149 | Acc: 21.343% (1284/6016)\n",
      "47 391 Loss: 2.150 | Acc: 21.208% (1303/6144)\n",
      "48 391 Loss: 2.149 | Acc: 21.269% (1334/6272)\n",
      "49 391 Loss: 2.147 | Acc: 21.312% (1364/6400)\n",
      "50 391 Loss: 2.147 | Acc: 21.262% (1388/6528)\n",
      "51 391 Loss: 2.148 | Acc: 21.184% (1410/6656)\n",
      "52 391 Loss: 2.148 | Acc: 21.138% (1434/6784)\n",
      "53 391 Loss: 2.148 | Acc: 21.123% (1460/6912)\n",
      "54 391 Loss: 2.148 | Acc: 21.108% (1486/7040)\n",
      "55 391 Loss: 2.147 | Acc: 21.066% (1510/7168)\n",
      "56 391 Loss: 2.147 | Acc: 21.053% (1536/7296)\n",
      "57 391 Loss: 2.148 | Acc: 20.986% (1558/7424)\n",
      "58 391 Loss: 2.146 | Acc: 21.120% (1595/7552)\n",
      "59 391 Loss: 2.145 | Acc: 21.107% (1621/7680)\n",
      "60 391 Loss: 2.144 | Acc: 21.247% (1659/7808)\n",
      "61 391 Loss: 2.144 | Acc: 21.169% (1680/7936)\n",
      "62 391 Loss: 2.143 | Acc: 21.255% (1714/8064)\n",
      "63 391 Loss: 2.143 | Acc: 21.191% (1736/8192)\n",
      "64 391 Loss: 2.143 | Acc: 21.166% (1761/8320)\n",
      "65 391 Loss: 2.143 | Acc: 21.200% (1791/8448)\n",
      "66 391 Loss: 2.144 | Acc: 21.152% (1814/8576)\n",
      "67 391 Loss: 2.144 | Acc: 21.163% (1842/8704)\n",
      "68 391 Loss: 2.144 | Acc: 21.184% (1871/8832)\n",
      "69 391 Loss: 2.143 | Acc: 21.228% (1902/8960)\n",
      "70 391 Loss: 2.143 | Acc: 21.182% (1925/9088)\n",
      "71 391 Loss: 2.143 | Acc: 21.246% (1958/9216)\n",
      "72 391 Loss: 2.143 | Acc: 21.244% (1985/9344)\n",
      "73 391 Loss: 2.142 | Acc: 21.263% (2014/9472)\n",
      "74 391 Loss: 2.142 | Acc: 21.260% (2041/9600)\n",
      "75 391 Loss: 2.142 | Acc: 21.269% (2069/9728)\n",
      "76 391 Loss: 2.142 | Acc: 21.307% (2100/9856)\n",
      "77 391 Loss: 2.142 | Acc: 21.274% (2124/9984)\n",
      "78 391 Loss: 2.142 | Acc: 21.292% (2153/10112)\n",
      "79 391 Loss: 2.142 | Acc: 21.367% (2188/10240)\n",
      "80 391 Loss: 2.141 | Acc: 21.412% (2220/10368)\n",
      "81 391 Loss: 2.142 | Acc: 21.437% (2250/10496)\n",
      "82 391 Loss: 2.141 | Acc: 21.386% (2272/10624)\n",
      "83 391 Loss: 2.142 | Acc: 21.345% (2295/10752)\n",
      "84 391 Loss: 2.142 | Acc: 21.342% (2322/10880)\n",
      "85 391 Loss: 2.141 | Acc: 21.348% (2350/11008)\n",
      "86 391 Loss: 2.140 | Acc: 21.318% (2374/11136)\n",
      "87 391 Loss: 2.141 | Acc: 21.342% (2404/11264)\n",
      "88 391 Loss: 2.140 | Acc: 21.375% (2435/11392)\n",
      "89 391 Loss: 2.140 | Acc: 21.345% (2459/11520)\n",
      "90 391 Loss: 2.141 | Acc: 21.300% (2481/11648)\n",
      "91 391 Loss: 2.141 | Acc: 21.272% (2505/11776)\n",
      "92 391 Loss: 2.141 | Acc: 21.253% (2530/11904)\n",
      "93 391 Loss: 2.141 | Acc: 21.277% (2560/12032)\n",
      "94 391 Loss: 2.141 | Acc: 21.324% (2593/12160)\n",
      "95 391 Loss: 2.141 | Acc: 21.265% (2613/12288)\n",
      "96 391 Loss: 2.141 | Acc: 21.231% (2636/12416)\n",
      "97 391 Loss: 2.141 | Acc: 21.221% (2662/12544)\n",
      "98 391 Loss: 2.141 | Acc: 21.157% (2681/12672)\n",
      "99 391 Loss: 2.141 | Acc: 21.195% (2713/12800)\n",
      "100 391 Loss: 2.141 | Acc: 21.241% (2746/12928)\n",
      "101 391 Loss: 2.140 | Acc: 21.285% (2779/13056)\n",
      "102 391 Loss: 2.140 | Acc: 21.276% (2805/13184)\n",
      "103 391 Loss: 2.140 | Acc: 21.312% (2837/13312)\n",
      "104 391 Loss: 2.140 | Acc: 21.295% (2862/13440)\n",
      "105 391 Loss: 2.140 | Acc: 21.315% (2892/13568)\n",
      "106 391 Loss: 2.140 | Acc: 21.335% (2922/13696)\n",
      "107 391 Loss: 2.140 | Acc: 21.318% (2947/13824)\n",
      "108 391 Loss: 2.139 | Acc: 21.302% (2972/13952)\n",
      "109 391 Loss: 2.139 | Acc: 21.314% (3001/14080)\n",
      "110 391 Loss: 2.140 | Acc: 21.256% (3020/14208)\n",
      "111 391 Loss: 2.139 | Acc: 21.240% (3045/14336)\n",
      "112 391 Loss: 2.140 | Acc: 21.211% (3068/14464)\n",
      "113 391 Loss: 2.139 | Acc: 21.224% (3097/14592)\n",
      "114 391 Loss: 2.139 | Acc: 21.277% (3132/14720)\n",
      "115 391 Loss: 2.139 | Acc: 21.316% (3165/14848)\n",
      "116 391 Loss: 2.138 | Acc: 21.301% (3190/14976)\n",
      "117 391 Loss: 2.138 | Acc: 21.352% (3225/15104)\n",
      "118 391 Loss: 2.137 | Acc: 21.402% (3260/15232)\n",
      "119 391 Loss: 2.137 | Acc: 21.419% (3290/15360)\n",
      "120 391 Loss: 2.137 | Acc: 21.410% (3316/15488)\n",
      "121 391 Loss: 2.137 | Acc: 21.395% (3341/15616)\n",
      "122 391 Loss: 2.136 | Acc: 21.430% (3374/15744)\n",
      "123 391 Loss: 2.136 | Acc: 21.390% (3395/15872)\n",
      "124 391 Loss: 2.136 | Acc: 21.387% (3422/16000)\n",
      "125 391 Loss: 2.136 | Acc: 21.385% (3449/16128)\n",
      "126 391 Loss: 2.135 | Acc: 21.401% (3479/16256)\n",
      "127 391 Loss: 2.136 | Acc: 21.436% (3512/16384)\n",
      "128 391 Loss: 2.135 | Acc: 21.433% (3539/16512)\n",
      "129 391 Loss: 2.135 | Acc: 21.526% (3582/16640)\n",
      "130 391 Loss: 2.135 | Acc: 21.589% (3620/16768)\n",
      "131 391 Loss: 2.135 | Acc: 21.615% (3652/16896)\n",
      "132 391 Loss: 2.134 | Acc: 21.658% (3687/17024)\n",
      "133 391 Loss: 2.133 | Acc: 21.665% (3716/17152)\n",
      "134 391 Loss: 2.133 | Acc: 21.672% (3745/17280)\n",
      "135 391 Loss: 2.133 | Acc: 21.685% (3775/17408)\n",
      "136 391 Loss: 2.132 | Acc: 21.750% (3814/17536)\n",
      "137 391 Loss: 2.133 | Acc: 21.728% (3838/17664)\n",
      "138 391 Loss: 2.132 | Acc: 21.746% (3869/17792)\n",
      "139 391 Loss: 2.132 | Acc: 21.758% (3899/17920)\n",
      "140 391 Loss: 2.131 | Acc: 21.775% (3930/18048)\n",
      "141 391 Loss: 2.131 | Acc: 21.732% (3950/18176)\n",
      "142 391 Loss: 2.131 | Acc: 21.738% (3979/18304)\n",
      "143 391 Loss: 2.130 | Acc: 21.745% (4008/18432)\n",
      "144 391 Loss: 2.130 | Acc: 21.762% (4039/18560)\n",
      "145 391 Loss: 2.130 | Acc: 21.736% (4062/18688)\n",
      "146 391 Loss: 2.130 | Acc: 21.700% (4083/18816)\n",
      "147 391 Loss: 2.130 | Acc: 21.685% (4108/18944)\n",
      "148 391 Loss: 2.130 | Acc: 21.681% (4135/19072)\n",
      "149 391 Loss: 2.130 | Acc: 21.661% (4159/19200)\n",
      "150 391 Loss: 2.130 | Acc: 21.642% (4183/19328)\n",
      "151 391 Loss: 2.130 | Acc: 21.639% (4210/19456)\n",
      "152 391 Loss: 2.129 | Acc: 21.661% (4242/19584)\n",
      "153 391 Loss: 2.129 | Acc: 21.657% (4269/19712)\n",
      "154 391 Loss: 2.130 | Acc: 21.663% (4298/19840)\n",
      "155 391 Loss: 2.129 | Acc: 21.705% (4334/19968)\n",
      "156 391 Loss: 2.129 | Acc: 21.671% (4355/20096)\n",
      "157 391 Loss: 2.129 | Acc: 21.692% (4387/20224)\n",
      "158 391 Loss: 2.129 | Acc: 21.703% (4417/20352)\n",
      "159 391 Loss: 2.129 | Acc: 21.714% (4447/20480)\n",
      "160 391 Loss: 2.128 | Acc: 21.705% (4473/20608)\n",
      "161 391 Loss: 2.128 | Acc: 21.740% (4508/20736)\n",
      "162 391 Loss: 2.128 | Acc: 21.746% (4537/20864)\n",
      "163 391 Loss: 2.128 | Acc: 21.775% (4571/20992)\n",
      "164 391 Loss: 2.127 | Acc: 21.795% (4603/21120)\n",
      "165 391 Loss: 2.127 | Acc: 21.795% (4631/21248)\n",
      "166 391 Loss: 2.127 | Acc: 21.800% (4660/21376)\n",
      "167 391 Loss: 2.127 | Acc: 21.815% (4691/21504)\n",
      "168 391 Loss: 2.127 | Acc: 21.787% (4713/21632)\n",
      "169 391 Loss: 2.127 | Acc: 21.769% (4737/21760)\n",
      "170 391 Loss: 2.126 | Acc: 21.802% (4772/21888)\n",
      "171 391 Loss: 2.126 | Acc: 21.825% (4805/22016)\n",
      "172 391 Loss: 2.126 | Acc: 21.821% (4832/22144)\n",
      "173 391 Loss: 2.126 | Acc: 21.808% (4857/22272)\n",
      "174 391 Loss: 2.126 | Acc: 21.790% (4881/22400)\n",
      "175 391 Loss: 2.126 | Acc: 21.782% (4907/22528)\n",
      "176 391 Loss: 2.126 | Acc: 21.765% (4931/22656)\n",
      "177 391 Loss: 2.126 | Acc: 21.721% (4949/22784)\n",
      "178 391 Loss: 2.126 | Acc: 21.727% (4978/22912)\n",
      "179 391 Loss: 2.125 | Acc: 21.749% (5011/23040)\n",
      "180 391 Loss: 2.125 | Acc: 21.746% (5038/23168)\n",
      "181 391 Loss: 2.125 | Acc: 21.746% (5066/23296)\n",
      "182 391 Loss: 2.125 | Acc: 21.798% (5106/23424)\n",
      "183 391 Loss: 2.125 | Acc: 21.807% (5136/23552)\n",
      "184 391 Loss: 2.125 | Acc: 21.795% (5161/23680)\n",
      "185 391 Loss: 2.125 | Acc: 21.799% (5190/23808)\n",
      "186 391 Loss: 2.125 | Acc: 21.771% (5211/23936)\n",
      "187 391 Loss: 2.125 | Acc: 21.775% (5240/24064)\n",
      "188 391 Loss: 2.125 | Acc: 21.796% (5273/24192)\n",
      "189 391 Loss: 2.124 | Acc: 21.813% (5305/24320)\n",
      "190 391 Loss: 2.124 | Acc: 21.801% (5330/24448)\n",
      "191 391 Loss: 2.124 | Acc: 21.830% (5365/24576)\n",
      "192 391 Loss: 2.124 | Acc: 21.826% (5392/24704)\n",
      "193 391 Loss: 2.124 | Acc: 21.827% (5420/24832)\n",
      "194 391 Loss: 2.124 | Acc: 21.819% (5446/24960)\n",
      "195 391 Loss: 2.124 | Acc: 21.819% (5474/25088)\n",
      "196 391 Loss: 2.124 | Acc: 21.812% (5500/25216)\n",
      "197 391 Loss: 2.124 | Acc: 21.828% (5532/25344)\n",
      "198 391 Loss: 2.124 | Acc: 21.828% (5560/25472)\n",
      "199 391 Loss: 2.124 | Acc: 21.816% (5585/25600)\n",
      "200 391 Loss: 2.124 | Acc: 21.821% (5614/25728)\n",
      "201 391 Loss: 2.123 | Acc: 21.867% (5654/25856)\n",
      "202 391 Loss: 2.123 | Acc: 21.867% (5682/25984)\n",
      "203 391 Loss: 2.123 | Acc: 21.871% (5711/26112)\n",
      "204 391 Loss: 2.123 | Acc: 21.890% (5744/26240)\n",
      "205 391 Loss: 2.123 | Acc: 21.932% (5783/26368)\n",
      "206 391 Loss: 2.123 | Acc: 21.920% (5808/26496)\n",
      "207 391 Loss: 2.123 | Acc: 21.924% (5837/26624)\n",
      "208 391 Loss: 2.123 | Acc: 21.909% (5861/26752)\n",
      "209 391 Loss: 2.123 | Acc: 21.923% (5893/26880)\n",
      "210 391 Loss: 2.123 | Acc: 21.945% (5927/27008)\n",
      "211 391 Loss: 2.122 | Acc: 21.956% (5958/27136)\n",
      "212 391 Loss: 2.122 | Acc: 21.974% (5991/27264)\n",
      "213 391 Loss: 2.122 | Acc: 21.992% (6024/27392)\n",
      "214 391 Loss: 2.122 | Acc: 21.991% (6052/27520)\n",
      "215 391 Loss: 2.122 | Acc: 21.994% (6081/27648)\n",
      "216 391 Loss: 2.121 | Acc: 22.012% (6114/27776)\n",
      "217 391 Loss: 2.121 | Acc: 22.004% (6140/27904)\n",
      "218 391 Loss: 2.121 | Acc: 22.021% (6173/28032)\n",
      "219 391 Loss: 2.121 | Acc: 22.003% (6196/28160)\n",
      "220 391 Loss: 2.121 | Acc: 21.970% (6215/28288)\n",
      "221 391 Loss: 2.121 | Acc: 21.970% (6243/28416)\n",
      "222 391 Loss: 2.121 | Acc: 21.977% (6273/28544)\n",
      "223 391 Loss: 2.121 | Acc: 21.997% (6307/28672)\n",
      "224 391 Loss: 2.120 | Acc: 22.003% (6337/28800)\n",
      "225 391 Loss: 2.120 | Acc: 21.986% (6360/28928)\n",
      "226 391 Loss: 2.120 | Acc: 21.964% (6382/29056)\n",
      "227 391 Loss: 2.120 | Acc: 22.002% (6421/29184)\n",
      "228 391 Loss: 2.120 | Acc: 21.977% (6442/29312)\n",
      "229 391 Loss: 2.120 | Acc: 21.980% (6471/29440)\n",
      "230 391 Loss: 2.120 | Acc: 21.987% (6501/29568)\n",
      "231 391 Loss: 2.120 | Acc: 21.986% (6529/29696)\n",
      "232 391 Loss: 2.120 | Acc: 21.979% (6555/29824)\n",
      "233 391 Loss: 2.120 | Acc: 22.005% (6591/29952)\n",
      "234 391 Loss: 2.119 | Acc: 22.018% (6623/30080)\n",
      "235 391 Loss: 2.119 | Acc: 22.017% (6651/30208)\n",
      "236 391 Loss: 2.119 | Acc: 21.997% (6673/30336)\n",
      "237 391 Loss: 2.119 | Acc: 21.983% (6697/30464)\n",
      "238 391 Loss: 2.119 | Acc: 21.989% (6727/30592)\n",
      "239 391 Loss: 2.119 | Acc: 21.992% (6756/30720)\n",
      "240 391 Loss: 2.119 | Acc: 22.005% (6788/30848)\n",
      "241 391 Loss: 2.119 | Acc: 22.011% (6818/30976)\n",
      "242 391 Loss: 2.119 | Acc: 22.029% (6852/31104)\n",
      "243 391 Loss: 2.118 | Acc: 22.074% (6894/31232)\n",
      "244 391 Loss: 2.118 | Acc: 22.098% (6930/31360)\n",
      "245 391 Loss: 2.118 | Acc: 22.097% (6958/31488)\n",
      "246 391 Loss: 2.118 | Acc: 22.103% (6988/31616)\n",
      "247 391 Loss: 2.117 | Acc: 22.133% (7026/31744)\n",
      "248 391 Loss: 2.117 | Acc: 22.148% (7059/31872)\n",
      "249 391 Loss: 2.117 | Acc: 22.156% (7090/32000)\n",
      "250 391 Loss: 2.117 | Acc: 22.171% (7123/32128)\n",
      "251 391 Loss: 2.117 | Acc: 22.166% (7150/32256)\n",
      "252 391 Loss: 2.117 | Acc: 22.147% (7172/32384)\n",
      "253 391 Loss: 2.117 | Acc: 22.133% (7196/32512)\n",
      "254 391 Loss: 2.117 | Acc: 22.123% (7221/32640)\n",
      "255 391 Loss: 2.117 | Acc: 22.116% (7247/32768)\n",
      "256 391 Loss: 2.116 | Acc: 22.133% (7281/32896)\n",
      "257 391 Loss: 2.116 | Acc: 22.129% (7308/33024)\n",
      "258 391 Loss: 2.116 | Acc: 22.122% (7334/33152)\n",
      "259 391 Loss: 2.116 | Acc: 22.118% (7361/33280)\n",
      "260 391 Loss: 2.115 | Acc: 22.123% (7391/33408)\n",
      "261 391 Loss: 2.115 | Acc: 22.134% (7423/33536)\n",
      "262 391 Loss: 2.115 | Acc: 22.142% (7454/33664)\n",
      "263 391 Loss: 2.115 | Acc: 22.132% (7479/33792)\n",
      "264 391 Loss: 2.115 | Acc: 22.117% (7502/33920)\n",
      "265 391 Loss: 2.115 | Acc: 22.125% (7533/34048)\n",
      "266 391 Loss: 2.114 | Acc: 22.162% (7574/34176)\n",
      "267 391 Loss: 2.114 | Acc: 22.172% (7606/34304)\n",
      "268 391 Loss: 2.113 | Acc: 22.197% (7643/34432)\n",
      "269 391 Loss: 2.113 | Acc: 22.208% (7675/34560)\n",
      "270 391 Loss: 2.113 | Acc: 22.189% (7697/34688)\n",
      "271 391 Loss: 2.113 | Acc: 22.174% (7720/34816)\n",
      "272 391 Loss: 2.113 | Acc: 22.190% (7754/34944)\n",
      "273 391 Loss: 2.113 | Acc: 22.194% (7784/35072)\n",
      "274 391 Loss: 2.113 | Acc: 22.207% (7817/35200)\n",
      "275 391 Loss: 2.112 | Acc: 22.240% (7857/35328)\n",
      "276 391 Loss: 2.112 | Acc: 22.242% (7886/35456)\n",
      "277 391 Loss: 2.111 | Acc: 22.226% (7909/35584)\n",
      "278 391 Loss: 2.111 | Acc: 22.250% (7946/35712)\n",
      "279 391 Loss: 2.111 | Acc: 22.260% (7978/35840)\n",
      "280 391 Loss: 2.111 | Acc: 22.275% (8012/35968)\n",
      "281 391 Loss: 2.110 | Acc: 22.274% (8040/36096)\n",
      "282 391 Loss: 2.110 | Acc: 22.303% (8079/36224)\n",
      "283 391 Loss: 2.110 | Acc: 22.318% (8113/36352)\n",
      "284 391 Loss: 2.110 | Acc: 22.346% (8152/36480)\n",
      "285 391 Loss: 2.110 | Acc: 22.334% (8176/36608)\n",
      "286 391 Loss: 2.110 | Acc: 22.340% (8207/36736)\n",
      "287 391 Loss: 2.109 | Acc: 22.352% (8240/36864)\n",
      "288 391 Loss: 2.109 | Acc: 22.367% (8274/36992)\n",
      "289 391 Loss: 2.109 | Acc: 22.384% (8309/37120)\n",
      "290 391 Loss: 2.109 | Acc: 22.388% (8339/37248)\n",
      "291 391 Loss: 2.109 | Acc: 22.373% (8362/37376)\n",
      "292 391 Loss: 2.108 | Acc: 22.398% (8400/37504)\n",
      "293 391 Loss: 2.108 | Acc: 22.388% (8425/37632)\n",
      "294 391 Loss: 2.108 | Acc: 22.386% (8453/37760)\n",
      "295 391 Loss: 2.108 | Acc: 22.379% (8479/37888)\n",
      "296 391 Loss: 2.108 | Acc: 22.393% (8513/38016)\n",
      "297 391 Loss: 2.108 | Acc: 22.389% (8540/38144)\n",
      "298 391 Loss: 2.108 | Acc: 22.392% (8570/38272)\n",
      "299 391 Loss: 2.108 | Acc: 22.375% (8592/38400)\n",
      "300 391 Loss: 2.108 | Acc: 22.358% (8614/38528)\n",
      "301 391 Loss: 2.108 | Acc: 22.356% (8642/38656)\n",
      "302 391 Loss: 2.108 | Acc: 22.367% (8675/38784)\n",
      "303 391 Loss: 2.107 | Acc: 22.379% (8708/38912)\n",
      "304 391 Loss: 2.107 | Acc: 22.387% (8740/39040)\n",
      "305 391 Loss: 2.107 | Acc: 22.396% (8772/39168)\n",
      "306 391 Loss: 2.107 | Acc: 22.387% (8797/39296)\n",
      "307 391 Loss: 2.106 | Acc: 22.400% (8831/39424)\n",
      "308 391 Loss: 2.106 | Acc: 22.419% (8867/39552)\n",
      "309 391 Loss: 2.106 | Acc: 22.429% (8900/39680)\n",
      "310 391 Loss: 2.106 | Acc: 22.430% (8929/39808)\n",
      "311 391 Loss: 2.106 | Acc: 22.448% (8965/39936)\n",
      "312 391 Loss: 2.106 | Acc: 22.464% (9000/40064)\n",
      "313 391 Loss: 2.106 | Acc: 22.487% (9038/40192)\n",
      "314 391 Loss: 2.105 | Acc: 22.473% (9061/40320)\n",
      "315 391 Loss: 2.105 | Acc: 22.491% (9097/40448)\n",
      "316 391 Loss: 2.105 | Acc: 22.501% (9130/40576)\n",
      "317 391 Loss: 2.105 | Acc: 22.504% (9160/40704)\n",
      "318 391 Loss: 2.104 | Acc: 22.531% (9200/40832)\n",
      "319 391 Loss: 2.104 | Acc: 22.542% (9233/40960)\n",
      "320 391 Loss: 2.104 | Acc: 22.564% (9271/41088)\n",
      "321 391 Loss: 2.104 | Acc: 22.564% (9300/41216)\n",
      "322 391 Loss: 2.104 | Acc: 22.552% (9324/41344)\n",
      "323 391 Loss: 2.103 | Acc: 22.574% (9362/41472)\n",
      "324 391 Loss: 2.103 | Acc: 22.587% (9396/41600)\n",
      "325 391 Loss: 2.103 | Acc: 22.604% (9432/41728)\n",
      "326 391 Loss: 2.103 | Acc: 22.608% (9463/41856)\n",
      "327 391 Loss: 2.103 | Acc: 22.628% (9500/41984)\n",
      "328 391 Loss: 2.102 | Acc: 22.637% (9533/42112)\n",
      "329 391 Loss: 2.102 | Acc: 22.644% (9565/42240)\n",
      "330 391 Loss: 2.102 | Acc: 22.670% (9605/42368)\n",
      "331 391 Loss: 2.102 | Acc: 22.689% (9642/42496)\n",
      "332 391 Loss: 2.102 | Acc: 22.675% (9665/42624)\n",
      "333 391 Loss: 2.101 | Acc: 22.701% (9705/42752)\n",
      "334 391 Loss: 2.101 | Acc: 22.703% (9735/42880)\n",
      "335 391 Loss: 2.101 | Acc: 22.700% (9763/43008)\n",
      "336 391 Loss: 2.101 | Acc: 22.689% (9787/43136)\n",
      "337 391 Loss: 2.101 | Acc: 22.682% (9813/43264)\n",
      "338 391 Loss: 2.101 | Acc: 22.670% (9837/43392)\n",
      "339 391 Loss: 2.101 | Acc: 22.675% (9868/43520)\n",
      "340 391 Loss: 2.101 | Acc: 22.665% (9893/43648)\n",
      "341 391 Loss: 2.101 | Acc: 22.661% (9920/43776)\n",
      "342 391 Loss: 2.100 | Acc: 22.652% (9945/43904)\n",
      "343 391 Loss: 2.100 | Acc: 22.652% (9974/44032)\n",
      "344 391 Loss: 2.100 | Acc: 22.656% (10005/44160)\n",
      "345 391 Loss: 2.100 | Acc: 22.665% (10038/44288)\n",
      "346 391 Loss: 2.100 | Acc: 22.670% (10069/44416)\n",
      "347 391 Loss: 2.099 | Acc: 22.681% (10103/44544)\n",
      "348 391 Loss: 2.099 | Acc: 22.665% (10125/44672)\n",
      "349 391 Loss: 2.099 | Acc: 22.696% (10168/44800)\n",
      "350 391 Loss: 2.099 | Acc: 22.714% (10205/44928)\n",
      "351 391 Loss: 2.099 | Acc: 22.723% (10238/45056)\n",
      "352 391 Loss: 2.099 | Acc: 22.734% (10272/45184)\n",
      "353 391 Loss: 2.099 | Acc: 22.753% (10310/45312)\n",
      "354 391 Loss: 2.098 | Acc: 22.746% (10336/45440)\n",
      "355 391 Loss: 2.098 | Acc: 22.755% (10369/45568)\n",
      "356 391 Loss: 2.098 | Acc: 22.757% (10399/45696)\n",
      "357 391 Loss: 2.098 | Acc: 22.768% (10433/45824)\n",
      "358 391 Loss: 2.098 | Acc: 22.769% (10463/45952)\n",
      "359 391 Loss: 2.098 | Acc: 22.773% (10494/46080)\n",
      "360 391 Loss: 2.098 | Acc: 22.786% (10529/46208)\n",
      "361 391 Loss: 2.097 | Acc: 22.807% (10568/46336)\n",
      "362 391 Loss: 2.097 | Acc: 22.816% (10601/46464)\n",
      "363 391 Loss: 2.097 | Acc: 22.834% (10639/46592)\n",
      "364 391 Loss: 2.097 | Acc: 22.849% (10675/46720)\n",
      "365 391 Loss: 2.096 | Acc: 22.859% (10709/46848)\n",
      "366 391 Loss: 2.096 | Acc: 22.865% (10741/46976)\n",
      "367 391 Loss: 2.096 | Acc: 22.852% (10764/47104)\n",
      "368 391 Loss: 2.096 | Acc: 22.834% (10785/47232)\n",
      "369 391 Loss: 2.096 | Acc: 22.829% (10812/47360)\n",
      "370 391 Loss: 2.096 | Acc: 22.858% (10855/47488)\n",
      "371 391 Loss: 2.096 | Acc: 22.866% (10888/47616)\n",
      "372 391 Loss: 2.095 | Acc: 22.876% (10922/47744)\n",
      "373 391 Loss: 2.095 | Acc: 22.884% (10955/47872)\n",
      "374 391 Loss: 2.095 | Acc: 22.898% (10991/48000)\n",
      "375 391 Loss: 2.095 | Acc: 22.922% (11032/48128)\n",
      "376 391 Loss: 2.095 | Acc: 22.930% (11065/48256)\n",
      "377 391 Loss: 2.094 | Acc: 22.939% (11099/48384)\n",
      "378 391 Loss: 2.094 | Acc: 22.928% (11123/48512)\n",
      "379 391 Loss: 2.094 | Acc: 22.921% (11149/48640)\n",
      "380 391 Loss: 2.094 | Acc: 22.919% (11177/48768)\n",
      "381 391 Loss: 2.094 | Acc: 22.914% (11204/48896)\n",
      "382 391 Loss: 2.094 | Acc: 22.923% (11238/49024)\n",
      "383 391 Loss: 2.094 | Acc: 22.933% (11272/49152)\n",
      "384 391 Loss: 2.094 | Acc: 22.930% (11300/49280)\n",
      "385 391 Loss: 2.093 | Acc: 22.934% (11331/49408)\n",
      "386 391 Loss: 2.093 | Acc: 22.951% (11369/49536)\n",
      "387 391 Loss: 2.093 | Acc: 22.938% (11392/49664)\n",
      "388 391 Loss: 2.093 | Acc: 22.966% (11435/49792)\n",
      "389 391 Loss: 2.093 | Acc: 22.977% (11470/49920)\n",
      "390 391 Loss: 2.093 | Acc: 22.980% (11490/50000)\n",
      "0 100 Loss: 1.940 | Acc: 30.000% (30/100)\n",
      "1 100 Loss: 1.994 | Acc: 26.500% (53/200)\n",
      "2 100 Loss: 1.992 | Acc: 26.667% (80/300)\n",
      "3 100 Loss: 1.999 | Acc: 26.000% (104/400)\n",
      "4 100 Loss: 2.004 | Acc: 25.600% (128/500)\n",
      "5 100 Loss: 1.999 | Acc: 24.667% (148/600)\n",
      "6 100 Loss: 1.999 | Acc: 24.429% (171/700)\n",
      "7 100 Loss: 2.009 | Acc: 24.625% (197/800)\n",
      "8 100 Loss: 2.016 | Acc: 24.667% (222/900)\n",
      "9 100 Loss: 2.022 | Acc: 24.400% (244/1000)\n",
      "10 100 Loss: 2.024 | Acc: 25.000% (275/1100)\n",
      "11 100 Loss: 2.027 | Acc: 25.167% (302/1200)\n",
      "12 100 Loss: 2.026 | Acc: 24.923% (324/1300)\n",
      "13 100 Loss: 2.029 | Acc: 24.143% (338/1400)\n",
      "14 100 Loss: 2.027 | Acc: 24.467% (367/1500)\n",
      "15 100 Loss: 2.026 | Acc: 24.562% (393/1600)\n",
      "16 100 Loss: 2.022 | Acc: 24.588% (418/1700)\n",
      "17 100 Loss: 2.019 | Acc: 24.778% (446/1800)\n",
      "18 100 Loss: 2.017 | Acc: 24.895% (473/1900)\n",
      "19 100 Loss: 2.016 | Acc: 24.950% (499/2000)\n",
      "20 100 Loss: 2.016 | Acc: 25.000% (525/2100)\n",
      "21 100 Loss: 2.015 | Acc: 25.318% (557/2200)\n",
      "22 100 Loss: 2.015 | Acc: 25.522% (587/2300)\n",
      "23 100 Loss: 2.017 | Acc: 25.542% (613/2400)\n",
      "24 100 Loss: 2.011 | Acc: 25.600% (640/2500)\n",
      "25 100 Loss: 2.013 | Acc: 25.577% (665/2600)\n",
      "26 100 Loss: 2.012 | Acc: 25.333% (684/2700)\n",
      "27 100 Loss: 2.011 | Acc: 25.250% (707/2800)\n",
      "28 100 Loss: 2.014 | Acc: 25.207% (731/2900)\n",
      "29 100 Loss: 2.013 | Acc: 25.133% (754/3000)\n",
      "30 100 Loss: 2.013 | Acc: 25.065% (777/3100)\n",
      "31 100 Loss: 2.014 | Acc: 25.000% (800/3200)\n",
      "32 100 Loss: 2.014 | Acc: 24.788% (818/3300)\n",
      "33 100 Loss: 2.017 | Acc: 24.676% (839/3400)\n",
      "34 100 Loss: 2.017 | Acc: 24.771% (867/3500)\n",
      "35 100 Loss: 2.014 | Acc: 24.917% (897/3600)\n",
      "36 100 Loss: 2.013 | Acc: 25.027% (926/3700)\n",
      "37 100 Loss: 2.013 | Acc: 25.053% (952/3800)\n",
      "38 100 Loss: 2.014 | Acc: 25.051% (977/3900)\n",
      "39 100 Loss: 2.013 | Acc: 25.250% (1010/4000)\n",
      "40 100 Loss: 2.012 | Acc: 25.195% (1033/4100)\n",
      "41 100 Loss: 2.013 | Acc: 25.167% (1057/4200)\n",
      "42 100 Loss: 2.014 | Acc: 25.093% (1079/4300)\n",
      "43 100 Loss: 2.011 | Acc: 25.227% (1110/4400)\n",
      "44 100 Loss: 2.011 | Acc: 25.156% (1132/4500)\n",
      "45 100 Loss: 2.010 | Acc: 25.196% (1159/4600)\n",
      "46 100 Loss: 2.010 | Acc: 25.043% (1177/4700)\n",
      "47 100 Loss: 2.011 | Acc: 25.062% (1203/4800)\n",
      "48 100 Loss: 2.010 | Acc: 25.102% (1230/4900)\n",
      "49 100 Loss: 2.011 | Acc: 25.080% (1254/5000)\n",
      "50 100 Loss: 2.011 | Acc: 25.059% (1278/5100)\n",
      "51 100 Loss: 2.011 | Acc: 25.019% (1301/5200)\n",
      "52 100 Loss: 2.010 | Acc: 25.075% (1329/5300)\n",
      "53 100 Loss: 2.012 | Acc: 24.981% (1349/5400)\n",
      "54 100 Loss: 2.013 | Acc: 24.909% (1370/5500)\n",
      "55 100 Loss: 2.013 | Acc: 24.821% (1390/5600)\n",
      "56 100 Loss: 2.015 | Acc: 24.895% (1419/5700)\n",
      "57 100 Loss: 2.013 | Acc: 25.017% (1451/5800)\n",
      "58 100 Loss: 2.012 | Acc: 25.136% (1483/5900)\n",
      "59 100 Loss: 2.013 | Acc: 25.100% (1506/6000)\n",
      "60 100 Loss: 2.013 | Acc: 25.180% (1536/6100)\n",
      "61 100 Loss: 2.014 | Acc: 25.145% (1559/6200)\n",
      "62 100 Loss: 2.014 | Acc: 25.127% (1583/6300)\n",
      "63 100 Loss: 2.012 | Acc: 25.250% (1616/6400)\n",
      "64 100 Loss: 2.012 | Acc: 25.231% (1640/6500)\n",
      "65 100 Loss: 2.013 | Acc: 25.152% (1660/6600)\n",
      "66 100 Loss: 2.013 | Acc: 25.134% (1684/6700)\n",
      "67 100 Loss: 2.012 | Acc: 25.147% (1710/6800)\n",
      "68 100 Loss: 2.011 | Acc: 25.101% (1732/6900)\n",
      "69 100 Loss: 2.011 | Acc: 25.057% (1754/7000)\n",
      "70 100 Loss: 2.012 | Acc: 24.972% (1773/7100)\n",
      "71 100 Loss: 2.011 | Acc: 25.028% (1802/7200)\n",
      "72 100 Loss: 2.011 | Acc: 25.082% (1831/7300)\n",
      "73 100 Loss: 2.010 | Acc: 25.189% (1864/7400)\n",
      "74 100 Loss: 2.009 | Acc: 25.227% (1892/7500)\n",
      "75 100 Loss: 2.010 | Acc: 25.171% (1913/7600)\n",
      "76 100 Loss: 2.008 | Acc: 25.221% (1942/7700)\n",
      "77 100 Loss: 2.008 | Acc: 25.397% (1981/7800)\n",
      "78 100 Loss: 2.008 | Acc: 25.342% (2002/7900)\n",
      "79 100 Loss: 2.010 | Acc: 25.288% (2023/8000)\n",
      "80 100 Loss: 2.009 | Acc: 25.309% (2050/8100)\n",
      "81 100 Loss: 2.009 | Acc: 25.329% (2077/8200)\n",
      "82 100 Loss: 2.009 | Acc: 25.289% (2099/8300)\n",
      "83 100 Loss: 2.010 | Acc: 25.333% (2128/8400)\n",
      "84 100 Loss: 2.009 | Acc: 25.353% (2155/8500)\n",
      "85 100 Loss: 2.010 | Acc: 25.384% (2183/8600)\n",
      "86 100 Loss: 2.010 | Acc: 25.391% (2209/8700)\n",
      "87 100 Loss: 2.010 | Acc: 25.455% (2240/8800)\n",
      "88 100 Loss: 2.008 | Acc: 25.596% (2278/8900)\n",
      "89 100 Loss: 2.008 | Acc: 25.578% (2302/9000)\n",
      "90 100 Loss: 2.008 | Acc: 25.549% (2325/9100)\n",
      "91 100 Loss: 2.008 | Acc: 25.620% (2357/9200)\n",
      "92 100 Loss: 2.007 | Acc: 25.710% (2391/9300)\n",
      "93 100 Loss: 2.008 | Acc: 25.713% (2417/9400)\n",
      "94 100 Loss: 2.007 | Acc: 25.674% (2439/9500)\n",
      "95 100 Loss: 2.007 | Acc: 25.688% (2466/9600)\n",
      "96 100 Loss: 2.006 | Acc: 25.753% (2498/9700)\n",
      "97 100 Loss: 2.006 | Acc: 25.684% (2517/9800)\n",
      "98 100 Loss: 2.005 | Acc: 25.616% (2536/9900)\n",
      "99 100 Loss: 2.005 | Acc: 25.620% (2562/10000)\n",
      "\n",
      "Epoch: 2\n",
      "0 391 Loss: 2.118 | Acc: 18.750% (24/128)\n",
      "1 391 Loss: 2.034 | Acc: 24.609% (63/256)\n",
      "2 391 Loss: 2.022 | Acc: 27.344% (105/384)\n",
      "3 391 Loss: 1.999 | Acc: 28.711% (147/512)\n",
      "4 391 Loss: 1.987 | Acc: 28.906% (185/640)\n",
      "5 391 Loss: 2.011 | Acc: 27.865% (214/768)\n",
      "6 391 Loss: 2.003 | Acc: 27.344% (245/896)\n",
      "7 391 Loss: 1.997 | Acc: 27.441% (281/1024)\n",
      "8 391 Loss: 1.990 | Acc: 27.344% (315/1152)\n",
      "9 391 Loss: 1.996 | Acc: 26.562% (340/1280)\n",
      "10 391 Loss: 1.998 | Acc: 26.562% (374/1408)\n",
      "11 391 Loss: 1.999 | Acc: 26.888% (413/1536)\n",
      "12 391 Loss: 2.006 | Acc: 26.623% (443/1664)\n",
      "13 391 Loss: 2.011 | Acc: 26.562% (476/1792)\n",
      "14 391 Loss: 2.005 | Acc: 26.615% (511/1920)\n",
      "15 391 Loss: 2.006 | Acc: 26.709% (547/2048)\n",
      "16 391 Loss: 2.002 | Acc: 26.608% (579/2176)\n",
      "17 391 Loss: 2.004 | Acc: 26.866% (619/2304)\n",
      "18 391 Loss: 2.009 | Acc: 26.727% (650/2432)\n",
      "19 391 Loss: 2.010 | Acc: 26.836% (687/2560)\n",
      "20 391 Loss: 2.010 | Acc: 26.637% (716/2688)\n",
      "21 391 Loss: 2.011 | Acc: 26.420% (744/2816)\n",
      "22 391 Loss: 2.011 | Acc: 26.393% (777/2944)\n",
      "23 391 Loss: 2.013 | Acc: 26.074% (801/3072)\n",
      "24 391 Loss: 2.010 | Acc: 26.312% (842/3200)\n",
      "25 391 Loss: 2.012 | Acc: 26.262% (874/3328)\n",
      "26 391 Loss: 2.013 | Acc: 26.215% (906/3456)\n",
      "27 391 Loss: 2.010 | Acc: 26.423% (947/3584)\n",
      "28 391 Loss: 2.012 | Acc: 26.347% (978/3712)\n",
      "29 391 Loss: 2.016 | Acc: 26.120% (1003/3840)\n",
      "30 391 Loss: 2.016 | Acc: 26.184% (1039/3968)\n",
      "31 391 Loss: 2.017 | Acc: 26.196% (1073/4096)\n",
      "32 391 Loss: 2.018 | Acc: 26.018% (1099/4224)\n",
      "33 391 Loss: 2.018 | Acc: 26.011% (1132/4352)\n",
      "34 391 Loss: 2.019 | Acc: 26.027% (1166/4480)\n",
      "35 391 Loss: 2.019 | Acc: 25.998% (1198/4608)\n",
      "36 391 Loss: 2.020 | Acc: 25.887% (1226/4736)\n",
      "37 391 Loss: 2.020 | Acc: 25.966% (1263/4864)\n",
      "38 391 Loss: 2.019 | Acc: 26.002% (1298/4992)\n",
      "39 391 Loss: 2.019 | Acc: 25.977% (1330/5120)\n",
      "40 391 Loss: 2.018 | Acc: 26.067% (1368/5248)\n",
      "41 391 Loss: 2.018 | Acc: 26.079% (1402/5376)\n",
      "42 391 Loss: 2.017 | Acc: 26.017% (1432/5504)\n",
      "43 391 Loss: 2.016 | Acc: 26.101% (1470/5632)\n",
      "44 391 Loss: 2.016 | Acc: 26.233% (1511/5760)\n",
      "45 391 Loss: 2.015 | Acc: 26.308% (1549/5888)\n",
      "46 391 Loss: 2.015 | Acc: 26.313% (1583/6016)\n",
      "47 391 Loss: 2.014 | Acc: 26.351% (1619/6144)\n",
      "48 391 Loss: 2.012 | Acc: 26.403% (1656/6272)\n",
      "49 391 Loss: 2.013 | Acc: 26.281% (1682/6400)\n",
      "50 391 Loss: 2.013 | Acc: 26.195% (1710/6528)\n",
      "51 391 Loss: 2.012 | Acc: 26.262% (1748/6656)\n",
      "52 391 Loss: 2.011 | Acc: 26.297% (1784/6784)\n",
      "53 391 Loss: 2.011 | Acc: 26.215% (1812/6912)\n",
      "54 391 Loss: 2.010 | Acc: 26.236% (1847/7040)\n",
      "55 391 Loss: 2.009 | Acc: 26.256% (1882/7168)\n",
      "56 391 Loss: 2.008 | Acc: 26.261% (1916/7296)\n",
      "57 391 Loss: 2.008 | Acc: 26.226% (1947/7424)\n",
      "58 391 Loss: 2.007 | Acc: 26.404% (1994/7552)\n",
      "59 391 Loss: 2.007 | Acc: 26.406% (2028/7680)\n",
      "60 391 Loss: 2.006 | Acc: 26.447% (2065/7808)\n",
      "61 391 Loss: 2.006 | Acc: 26.487% (2102/7936)\n",
      "62 391 Loss: 2.006 | Acc: 26.476% (2135/8064)\n",
      "63 391 Loss: 2.006 | Acc: 26.453% (2167/8192)\n",
      "64 391 Loss: 2.005 | Acc: 26.430% (2199/8320)\n",
      "65 391 Loss: 2.006 | Acc: 26.349% (2226/8448)\n",
      "66 391 Loss: 2.006 | Acc: 26.411% (2265/8576)\n",
      "67 391 Loss: 2.006 | Acc: 26.402% (2298/8704)\n",
      "68 391 Loss: 2.005 | Acc: 26.415% (2333/8832)\n",
      "69 391 Loss: 2.005 | Acc: 26.518% (2376/8960)\n",
      "70 391 Loss: 2.006 | Acc: 26.452% (2404/9088)\n",
      "71 391 Loss: 2.007 | Acc: 26.432% (2436/9216)\n",
      "72 391 Loss: 2.006 | Acc: 26.413% (2468/9344)\n",
      "73 391 Loss: 2.007 | Acc: 26.404% (2501/9472)\n",
      "74 391 Loss: 2.006 | Acc: 26.417% (2536/9600)\n",
      "75 391 Loss: 2.006 | Acc: 26.439% (2572/9728)\n",
      "76 391 Loss: 2.006 | Acc: 26.471% (2609/9856)\n",
      "77 391 Loss: 2.006 | Acc: 26.412% (2637/9984)\n",
      "78 391 Loss: 2.006 | Acc: 26.394% (2669/10112)\n",
      "79 391 Loss: 2.008 | Acc: 26.348% (2698/10240)\n",
      "80 391 Loss: 2.007 | Acc: 26.370% (2734/10368)\n",
      "81 391 Loss: 2.008 | Acc: 26.296% (2760/10496)\n",
      "82 391 Loss: 2.007 | Acc: 26.384% (2803/10624)\n",
      "83 391 Loss: 2.008 | Acc: 26.293% (2827/10752)\n",
      "84 391 Loss: 2.007 | Acc: 26.314% (2863/10880)\n",
      "85 391 Loss: 2.008 | Acc: 26.335% (2899/11008)\n",
      "86 391 Loss: 2.008 | Acc: 26.329% (2932/11136)\n",
      "87 391 Loss: 2.007 | Acc: 26.296% (2962/11264)\n",
      "88 391 Loss: 2.007 | Acc: 26.299% (2996/11392)\n",
      "89 391 Loss: 2.006 | Acc: 26.319% (3032/11520)\n",
      "90 391 Loss: 2.006 | Acc: 26.322% (3066/11648)\n",
      "91 391 Loss: 2.006 | Acc: 26.291% (3096/11776)\n",
      "92 391 Loss: 2.006 | Acc: 26.252% (3125/11904)\n",
      "93 391 Loss: 2.007 | Acc: 26.172% (3149/12032)\n",
      "94 391 Loss: 2.006 | Acc: 26.242% (3191/12160)\n",
      "95 391 Loss: 2.006 | Acc: 26.221% (3222/12288)\n",
      "96 391 Loss: 2.006 | Acc: 26.248% (3259/12416)\n",
      "97 391 Loss: 2.005 | Acc: 26.268% (3295/12544)\n",
      "98 391 Loss: 2.005 | Acc: 26.286% (3331/12672)\n",
      "99 391 Loss: 2.005 | Acc: 26.344% (3372/12800)\n",
      "100 391 Loss: 2.005 | Acc: 26.292% (3399/12928)\n",
      "101 391 Loss: 2.004 | Acc: 26.325% (3437/13056)\n",
      "102 391 Loss: 2.004 | Acc: 26.320% (3470/13184)\n",
      "103 391 Loss: 2.004 | Acc: 26.277% (3498/13312)\n",
      "104 391 Loss: 2.004 | Acc: 26.257% (3529/13440)\n",
      "105 391 Loss: 2.003 | Acc: 26.253% (3562/13568)\n",
      "106 391 Loss: 2.004 | Acc: 26.219% (3591/13696)\n",
      "107 391 Loss: 2.003 | Acc: 26.280% (3633/13824)\n",
      "108 391 Loss: 2.003 | Acc: 26.312% (3671/13952)\n",
      "109 391 Loss: 2.002 | Acc: 26.413% (3719/14080)\n",
      "110 391 Loss: 2.003 | Acc: 26.372% (3747/14208)\n",
      "111 391 Loss: 2.002 | Acc: 26.395% (3784/14336)\n",
      "112 391 Loss: 2.001 | Acc: 26.369% (3814/14464)\n",
      "113 391 Loss: 2.001 | Acc: 26.336% (3843/14592)\n",
      "114 391 Loss: 2.001 | Acc: 26.311% (3873/14720)\n",
      "115 391 Loss: 2.001 | Acc: 26.293% (3904/14848)\n",
      "116 391 Loss: 2.001 | Acc: 26.289% (3937/14976)\n",
      "117 391 Loss: 2.001 | Acc: 26.311% (3974/15104)\n",
      "118 391 Loss: 2.000 | Acc: 26.293% (4005/15232)\n",
      "119 391 Loss: 2.000 | Acc: 26.315% (4042/15360)\n",
      "120 391 Loss: 2.000 | Acc: 26.330% (4078/15488)\n",
      "121 391 Loss: 2.000 | Acc: 26.332% (4112/15616)\n",
      "122 391 Loss: 2.000 | Acc: 26.327% (4145/15744)\n",
      "123 391 Loss: 1.999 | Acc: 26.317% (4177/15872)\n",
      "124 391 Loss: 2.000 | Acc: 26.281% (4205/16000)\n",
      "125 391 Loss: 2.000 | Acc: 26.252% (4234/16128)\n",
      "126 391 Loss: 2.000 | Acc: 26.224% (4263/16256)\n",
      "127 391 Loss: 2.001 | Acc: 26.178% (4289/16384)\n",
      "128 391 Loss: 2.001 | Acc: 26.126% (4314/16512)\n",
      "129 391 Loss: 2.001 | Acc: 26.112% (4345/16640)\n",
      "130 391 Loss: 2.001 | Acc: 26.109% (4378/16768)\n",
      "131 391 Loss: 2.001 | Acc: 26.089% (4408/16896)\n",
      "132 391 Loss: 2.000 | Acc: 26.093% (4442/17024)\n",
      "133 391 Loss: 2.001 | Acc: 26.108% (4478/17152)\n",
      "134 391 Loss: 2.001 | Acc: 26.082% (4507/17280)\n",
      "135 391 Loss: 2.000 | Acc: 26.086% (4541/17408)\n",
      "136 391 Loss: 2.000 | Acc: 26.061% (4570/17536)\n",
      "137 391 Loss: 2.000 | Acc: 26.053% (4602/17664)\n",
      "138 391 Loss: 2.000 | Acc: 26.045% (4634/17792)\n",
      "139 391 Loss: 2.000 | Acc: 26.071% (4672/17920)\n",
      "140 391 Loss: 2.000 | Acc: 26.042% (4700/18048)\n",
      "141 391 Loss: 2.000 | Acc: 26.067% (4738/18176)\n",
      "142 391 Loss: 2.001 | Acc: 26.033% (4765/18304)\n",
      "143 391 Loss: 2.001 | Acc: 26.036% (4799/18432)\n",
      "144 391 Loss: 2.002 | Acc: 26.018% (4829/18560)\n",
      "145 391 Loss: 2.002 | Acc: 26.033% (4865/18688)\n",
      "146 391 Loss: 2.002 | Acc: 26.042% (4900/18816)\n",
      "147 391 Loss: 2.002 | Acc: 26.045% (4934/18944)\n",
      "148 391 Loss: 2.002 | Acc: 26.054% (4969/19072)\n",
      "149 391 Loss: 2.002 | Acc: 26.036% (4999/19200)\n",
      "150 391 Loss: 2.002 | Acc: 26.024% (5030/19328)\n",
      "151 391 Loss: 2.001 | Acc: 26.064% (5071/19456)\n",
      "152 391 Loss: 2.001 | Acc: 26.026% (5097/19584)\n",
      "153 391 Loss: 2.001 | Acc: 26.025% (5130/19712)\n",
      "154 391 Loss: 2.001 | Acc: 26.038% (5166/19840)\n",
      "155 391 Loss: 2.001 | Acc: 26.037% (5199/19968)\n",
      "156 391 Loss: 2.001 | Acc: 26.005% (5226/20096)\n",
      "157 391 Loss: 2.001 | Acc: 25.989% (5256/20224)\n",
      "158 391 Loss: 2.001 | Acc: 26.017% (5295/20352)\n",
      "159 391 Loss: 2.001 | Acc: 26.001% (5325/20480)\n",
      "160 391 Loss: 2.000 | Acc: 25.975% (5353/20608)\n",
      "161 391 Loss: 2.000 | Acc: 25.936% (5378/20736)\n",
      "162 391 Loss: 2.000 | Acc: 25.973% (5419/20864)\n",
      "163 391 Loss: 1.999 | Acc: 26.000% (5458/20992)\n",
      "164 391 Loss: 1.999 | Acc: 25.994% (5490/21120)\n",
      "165 391 Loss: 1.999 | Acc: 25.984% (5521/21248)\n",
      "166 391 Loss: 1.999 | Acc: 25.968% (5551/21376)\n",
      "167 391 Loss: 1.999 | Acc: 25.953% (5581/21504)\n",
      "168 391 Loss: 1.999 | Acc: 25.975% (5619/21632)\n",
      "169 391 Loss: 1.999 | Acc: 25.947% (5646/21760)\n",
      "170 391 Loss: 1.998 | Acc: 25.959% (5682/21888)\n",
      "171 391 Loss: 1.998 | Acc: 25.972% (5718/22016)\n",
      "172 391 Loss: 1.998 | Acc: 25.971% (5751/22144)\n",
      "173 391 Loss: 1.998 | Acc: 26.001% (5791/22272)\n",
      "174 391 Loss: 1.998 | Acc: 25.996% (5823/22400)\n",
      "175 391 Loss: 1.997 | Acc: 26.008% (5859/22528)\n",
      "176 391 Loss: 1.997 | Acc: 26.011% (5893/22656)\n",
      "177 391 Loss: 1.997 | Acc: 26.009% (5926/22784)\n",
      "178 391 Loss: 1.997 | Acc: 25.982% (5953/22912)\n",
      "179 391 Loss: 1.998 | Acc: 25.981% (5986/23040)\n",
      "180 391 Loss: 1.998 | Acc: 25.971% (6017/23168)\n",
      "181 391 Loss: 1.998 | Acc: 25.987% (6054/23296)\n",
      "182 391 Loss: 1.998 | Acc: 25.973% (6084/23424)\n",
      "183 391 Loss: 1.998 | Acc: 25.955% (6113/23552)\n",
      "184 391 Loss: 1.998 | Acc: 25.988% (6154/23680)\n",
      "185 391 Loss: 1.997 | Acc: 26.004% (6191/23808)\n",
      "186 391 Loss: 1.997 | Acc: 26.061% (6238/23936)\n",
      "187 391 Loss: 1.996 | Acc: 26.043% (6267/24064)\n",
      "188 391 Loss: 1.996 | Acc: 26.013% (6293/24192)\n",
      "189 391 Loss: 1.996 | Acc: 26.012% (6326/24320)\n",
      "190 391 Loss: 1.996 | Acc: 25.994% (6355/24448)\n",
      "191 391 Loss: 1.996 | Acc: 25.985% (6386/24576)\n",
      "192 391 Loss: 1.996 | Acc: 26.028% (6430/24704)\n",
      "193 391 Loss: 1.995 | Acc: 26.039% (6466/24832)\n",
      "194 391 Loss: 1.995 | Acc: 26.074% (6508/24960)\n",
      "195 391 Loss: 1.995 | Acc: 26.064% (6539/25088)\n",
      "196 391 Loss: 1.995 | Acc: 26.059% (6571/25216)\n",
      "197 391 Loss: 1.995 | Acc: 26.057% (6604/25344)\n",
      "198 391 Loss: 1.995 | Acc: 26.033% (6631/25472)\n",
      "199 391 Loss: 1.995 | Acc: 26.035% (6665/25600)\n",
      "200 391 Loss: 1.995 | Acc: 26.042% (6700/25728)\n",
      "201 391 Loss: 1.995 | Acc: 26.040% (6733/25856)\n",
      "202 391 Loss: 1.995 | Acc: 26.054% (6770/25984)\n",
      "203 391 Loss: 1.995 | Acc: 26.057% (6804/26112)\n",
      "204 391 Loss: 1.994 | Acc: 26.086% (6845/26240)\n",
      "205 391 Loss: 1.994 | Acc: 26.100% (6882/26368)\n",
      "206 391 Loss: 1.994 | Acc: 26.125% (6922/26496)\n",
      "207 391 Loss: 1.994 | Acc: 26.123% (6955/26624)\n",
      "208 391 Loss: 1.993 | Acc: 26.110% (6985/26752)\n",
      "209 391 Loss: 1.994 | Acc: 26.083% (7011/26880)\n",
      "210 391 Loss: 1.994 | Acc: 26.066% (7040/27008)\n",
      "211 391 Loss: 1.993 | Acc: 26.083% (7078/27136)\n",
      "212 391 Loss: 1.993 | Acc: 26.089% (7113/27264)\n",
      "213 391 Loss: 1.993 | Acc: 26.117% (7154/27392)\n",
      "214 391 Loss: 1.992 | Acc: 26.123% (7189/27520)\n",
      "215 391 Loss: 1.992 | Acc: 26.147% (7229/27648)\n",
      "216 391 Loss: 1.992 | Acc: 26.163% (7267/27776)\n",
      "217 391 Loss: 1.992 | Acc: 26.183% (7306/27904)\n",
      "218 391 Loss: 1.992 | Acc: 26.177% (7338/28032)\n",
      "219 391 Loss: 1.992 | Acc: 26.161% (7367/28160)\n",
      "220 391 Loss: 1.991 | Acc: 26.160% (7400/28288)\n",
      "221 391 Loss: 1.991 | Acc: 26.144% (7429/28416)\n",
      "222 391 Loss: 1.991 | Acc: 26.128% (7458/28544)\n",
      "223 391 Loss: 1.991 | Acc: 26.158% (7500/28672)\n",
      "224 391 Loss: 1.991 | Acc: 26.188% (7542/28800)\n",
      "225 391 Loss: 1.990 | Acc: 26.200% (7579/28928)\n",
      "226 391 Loss: 1.990 | Acc: 26.205% (7614/29056)\n",
      "227 391 Loss: 1.990 | Acc: 26.206% (7648/29184)\n",
      "228 391 Loss: 1.990 | Acc: 26.255% (7696/29312)\n",
      "229 391 Loss: 1.989 | Acc: 26.257% (7730/29440)\n",
      "230 391 Loss: 1.990 | Acc: 26.251% (7762/29568)\n",
      "231 391 Loss: 1.989 | Acc: 26.259% (7798/29696)\n",
      "232 391 Loss: 1.989 | Acc: 26.267% (7834/29824)\n",
      "233 391 Loss: 1.988 | Acc: 26.302% (7878/29952)\n",
      "234 391 Loss: 1.988 | Acc: 26.277% (7904/30080)\n",
      "235 391 Loss: 1.988 | Acc: 26.261% (7933/30208)\n",
      "236 391 Loss: 1.988 | Acc: 26.289% (7975/30336)\n",
      "237 391 Loss: 1.988 | Acc: 26.287% (8008/30464)\n",
      "238 391 Loss: 1.988 | Acc: 26.268% (8036/30592)\n",
      "239 391 Loss: 1.988 | Acc: 26.273% (8071/30720)\n",
      "240 391 Loss: 1.988 | Acc: 26.261% (8101/30848)\n",
      "241 391 Loss: 1.988 | Acc: 26.282% (8141/30976)\n",
      "242 391 Loss: 1.988 | Acc: 26.292% (8178/31104)\n",
      "243 391 Loss: 1.988 | Acc: 26.271% (8205/31232)\n",
      "244 391 Loss: 1.988 | Acc: 26.250% (8232/31360)\n",
      "245 391 Loss: 1.988 | Acc: 26.245% (8264/31488)\n",
      "246 391 Loss: 1.988 | Acc: 26.253% (8300/31616)\n",
      "247 391 Loss: 1.988 | Acc: 26.235% (8328/31744)\n",
      "248 391 Loss: 1.988 | Acc: 26.252% (8367/31872)\n",
      "249 391 Loss: 1.988 | Acc: 26.238% (8396/32000)\n",
      "250 391 Loss: 1.987 | Acc: 26.242% (8431/32128)\n",
      "251 391 Loss: 1.987 | Acc: 26.234% (8462/32256)\n",
      "252 391 Loss: 1.987 | Acc: 26.217% (8490/32384)\n",
      "253 391 Loss: 1.987 | Acc: 26.227% (8527/32512)\n",
      "254 391 Loss: 1.987 | Acc: 26.225% (8560/32640)\n",
      "255 391 Loss: 1.987 | Acc: 26.227% (8594/32768)\n",
      "256 391 Loss: 1.987 | Acc: 26.249% (8635/32896)\n",
      "257 391 Loss: 1.986 | Acc: 26.278% (8678/33024)\n",
      "258 391 Loss: 1.986 | Acc: 26.288% (8715/33152)\n",
      "259 391 Loss: 1.986 | Acc: 26.292% (8750/33280)\n",
      "260 391 Loss: 1.986 | Acc: 26.302% (8787/33408)\n",
      "261 391 Loss: 1.986 | Acc: 26.315% (8825/33536)\n",
      "262 391 Loss: 1.985 | Acc: 26.343% (8868/33664)\n",
      "263 391 Loss: 1.985 | Acc: 26.379% (8914/33792)\n",
      "264 391 Loss: 1.984 | Acc: 26.412% (8959/33920)\n",
      "265 391 Loss: 1.984 | Acc: 26.422% (8996/34048)\n",
      "266 391 Loss: 1.984 | Acc: 26.440% (9036/34176)\n",
      "267 391 Loss: 1.984 | Acc: 26.446% (9072/34304)\n",
      "268 391 Loss: 1.984 | Acc: 26.478% (9117/34432)\n",
      "269 391 Loss: 1.984 | Acc: 26.464% (9146/34560)\n",
      "270 391 Loss: 1.983 | Acc: 26.482% (9186/34688)\n",
      "271 391 Loss: 1.983 | Acc: 26.482% (9220/34816)\n",
      "272 391 Loss: 1.983 | Acc: 26.459% (9246/34944)\n",
      "273 391 Loss: 1.983 | Acc: 26.480% (9287/35072)\n",
      "274 391 Loss: 1.983 | Acc: 26.486% (9323/35200)\n",
      "275 391 Loss: 1.984 | Acc: 26.466% (9350/35328)\n",
      "276 391 Loss: 1.984 | Acc: 26.472% (9386/35456)\n",
      "277 391 Loss: 1.983 | Acc: 26.475% (9421/35584)\n",
      "278 391 Loss: 1.983 | Acc: 26.470% (9453/35712)\n",
      "279 391 Loss: 1.983 | Acc: 26.468% (9486/35840)\n",
      "280 391 Loss: 1.983 | Acc: 26.460% (9517/35968)\n",
      "281 391 Loss: 1.983 | Acc: 26.457% (9550/36096)\n",
      "282 391 Loss: 1.983 | Acc: 26.469% (9588/36224)\n",
      "283 391 Loss: 1.983 | Acc: 26.466% (9621/36352)\n",
      "284 391 Loss: 1.983 | Acc: 26.478% (9659/36480)\n",
      "285 391 Loss: 1.983 | Acc: 26.494% (9699/36608)\n",
      "286 391 Loss: 1.983 | Acc: 26.497% (9734/36736)\n",
      "287 391 Loss: 1.983 | Acc: 26.508% (9772/36864)\n",
      "288 391 Loss: 1.983 | Acc: 26.508% (9806/36992)\n",
      "289 391 Loss: 1.983 | Acc: 26.498% (9836/37120)\n",
      "290 391 Loss: 1.983 | Acc: 26.503% (9872/37248)\n",
      "291 391 Loss: 1.982 | Acc: 26.541% (9920/37376)\n",
      "292 391 Loss: 1.982 | Acc: 26.544% (9955/37504)\n",
      "293 391 Loss: 1.982 | Acc: 26.555% (9993/37632)\n",
      "294 391 Loss: 1.982 | Acc: 26.536% (10020/37760)\n",
      "295 391 Loss: 1.981 | Acc: 26.576% (10069/37888)\n",
      "296 391 Loss: 1.981 | Acc: 26.591% (10109/38016)\n",
      "297 391 Loss: 1.981 | Acc: 26.594% (10144/38144)\n",
      "298 391 Loss: 1.981 | Acc: 26.617% (10187/38272)\n",
      "299 391 Loss: 1.981 | Acc: 26.625% (10224/38400)\n",
      "300 391 Loss: 1.981 | Acc: 26.625% (10258/38528)\n",
      "301 391 Loss: 1.981 | Acc: 26.622% (10291/38656)\n",
      "302 391 Loss: 1.980 | Acc: 26.624% (10326/38784)\n",
      "303 391 Loss: 1.980 | Acc: 26.634% (10364/38912)\n",
      "304 391 Loss: 1.980 | Acc: 26.650% (10404/39040)\n",
      "305 391 Loss: 1.980 | Acc: 26.634% (10432/39168)\n",
      "306 391 Loss: 1.980 | Acc: 26.626% (10463/39296)\n",
      "307 391 Loss: 1.980 | Acc: 26.631% (10499/39424)\n",
      "308 391 Loss: 1.980 | Acc: 26.641% (10537/39552)\n",
      "309 391 Loss: 1.979 | Acc: 26.666% (10581/39680)\n",
      "310 391 Loss: 1.979 | Acc: 26.678% (10620/39808)\n",
      "311 391 Loss: 1.979 | Acc: 26.690% (10659/39936)\n",
      "312 391 Loss: 1.979 | Acc: 26.682% (10690/40064)\n",
      "313 391 Loss: 1.979 | Acc: 26.687% (10726/40192)\n",
      "314 391 Loss: 1.978 | Acc: 26.699% (10765/40320)\n",
      "315 391 Loss: 1.978 | Acc: 26.701% (10800/40448)\n",
      "316 391 Loss: 1.978 | Acc: 26.710% (10838/40576)\n",
      "317 391 Loss: 1.978 | Acc: 26.717% (10875/40704)\n",
      "318 391 Loss: 1.978 | Acc: 26.717% (10909/40832)\n",
      "319 391 Loss: 1.978 | Acc: 26.721% (10945/40960)\n",
      "320 391 Loss: 1.978 | Acc: 26.743% (10988/41088)\n",
      "321 391 Loss: 1.977 | Acc: 26.742% (11022/41216)\n",
      "322 391 Loss: 1.977 | Acc: 26.763% (11065/41344)\n",
      "323 391 Loss: 1.977 | Acc: 26.780% (11106/41472)\n",
      "324 391 Loss: 1.977 | Acc: 26.791% (11145/41600)\n",
      "325 391 Loss: 1.977 | Acc: 26.785% (11177/41728)\n",
      "326 391 Loss: 1.977 | Acc: 26.787% (11212/41856)\n",
      "327 391 Loss: 1.977 | Acc: 26.784% (11245/41984)\n",
      "328 391 Loss: 1.976 | Acc: 26.793% (11283/42112)\n",
      "329 391 Loss: 1.976 | Acc: 26.795% (11318/42240)\n",
      "330 391 Loss: 1.976 | Acc: 26.803% (11356/42368)\n",
      "331 391 Loss: 1.976 | Acc: 26.817% (11396/42496)\n",
      "332 391 Loss: 1.976 | Acc: 26.842% (11441/42624)\n",
      "333 391 Loss: 1.976 | Acc: 26.841% (11475/42752)\n",
      "334 391 Loss: 1.976 | Acc: 26.831% (11505/42880)\n",
      "335 391 Loss: 1.975 | Acc: 26.825% (11537/43008)\n",
      "336 391 Loss: 1.975 | Acc: 26.827% (11572/43136)\n",
      "337 391 Loss: 1.975 | Acc: 26.831% (11608/43264)\n",
      "338 391 Loss: 1.975 | Acc: 26.823% (11639/43392)\n",
      "339 391 Loss: 1.975 | Acc: 26.847% (11684/43520)\n",
      "340 391 Loss: 1.975 | Acc: 26.851% (11720/43648)\n",
      "341 391 Loss: 1.975 | Acc: 26.837% (11748/43776)\n",
      "342 391 Loss: 1.975 | Acc: 26.843% (11785/43904)\n",
      "343 391 Loss: 1.975 | Acc: 26.849% (11822/44032)\n",
      "344 391 Loss: 1.974 | Acc: 26.852% (11858/44160)\n",
      "345 391 Loss: 1.974 | Acc: 26.858% (11895/44288)\n",
      "346 391 Loss: 1.974 | Acc: 26.857% (11929/44416)\n",
      "347 391 Loss: 1.974 | Acc: 26.872% (11970/44544)\n",
      "348 391 Loss: 1.974 | Acc: 26.883% (12009/44672)\n",
      "349 391 Loss: 1.974 | Acc: 26.886% (12045/44800)\n",
      "350 391 Loss: 1.974 | Acc: 26.887% (12080/44928)\n",
      "351 391 Loss: 1.974 | Acc: 26.893% (12117/45056)\n",
      "352 391 Loss: 1.974 | Acc: 26.890% (12150/45184)\n",
      "353 391 Loss: 1.973 | Acc: 26.898% (12188/45312)\n",
      "354 391 Loss: 1.973 | Acc: 26.901% (12224/45440)\n",
      "355 391 Loss: 1.973 | Acc: 26.920% (12267/45568)\n",
      "356 391 Loss: 1.972 | Acc: 26.948% (12314/45696)\n",
      "357 391 Loss: 1.972 | Acc: 26.953% (12351/45824)\n",
      "358 391 Loss: 1.972 | Acc: 26.959% (12388/45952)\n",
      "359 391 Loss: 1.972 | Acc: 26.975% (12430/46080)\n",
      "360 391 Loss: 1.972 | Acc: 26.987% (12470/46208)\n",
      "361 391 Loss: 1.972 | Acc: 26.985% (12504/46336)\n",
      "362 391 Loss: 1.972 | Acc: 26.991% (12541/46464)\n",
      "363 391 Loss: 1.971 | Acc: 27.007% (12583/46592)\n",
      "364 391 Loss: 1.971 | Acc: 27.027% (12627/46720)\n",
      "365 391 Loss: 1.971 | Acc: 27.021% (12659/46848)\n",
      "366 391 Loss: 1.971 | Acc: 27.007% (12687/46976)\n",
      "367 391 Loss: 1.971 | Acc: 27.027% (12731/47104)\n",
      "368 391 Loss: 1.971 | Acc: 27.022% (12763/47232)\n",
      "369 391 Loss: 1.971 | Acc: 27.021% (12797/47360)\n",
      "370 391 Loss: 1.971 | Acc: 27.043% (12842/47488)\n",
      "371 391 Loss: 1.971 | Acc: 27.046% (12878/47616)\n",
      "372 391 Loss: 1.971 | Acc: 27.053% (12916/47744)\n",
      "373 391 Loss: 1.970 | Acc: 27.076% (12962/47872)\n",
      "374 391 Loss: 1.970 | Acc: 27.071% (12994/48000)\n",
      "375 391 Loss: 1.970 | Acc: 27.067% (13027/48128)\n",
      "376 391 Loss: 1.970 | Acc: 27.060% (13058/48256)\n",
      "377 391 Loss: 1.970 | Acc: 27.065% (13095/48384)\n",
      "378 391 Loss: 1.970 | Acc: 27.090% (13142/48512)\n",
      "379 391 Loss: 1.970 | Acc: 27.093% (13178/48640)\n",
      "380 391 Loss: 1.970 | Acc: 27.118% (13225/48768)\n",
      "381 391 Loss: 1.969 | Acc: 27.137% (13269/48896)\n",
      "382 391 Loss: 1.969 | Acc: 27.156% (13313/49024)\n",
      "383 391 Loss: 1.969 | Acc: 27.157% (13348/49152)\n",
      "384 391 Loss: 1.969 | Acc: 27.167% (13388/49280)\n",
      "385 391 Loss: 1.969 | Acc: 27.166% (13422/49408)\n",
      "386 391 Loss: 1.969 | Acc: 27.174% (13461/49536)\n",
      "387 391 Loss: 1.969 | Acc: 27.187% (13502/49664)\n",
      "388 391 Loss: 1.969 | Acc: 27.217% (13552/49792)\n",
      "389 391 Loss: 1.969 | Acc: 27.204% (13580/49920)\n",
      "390 391 Loss: 1.968 | Acc: 27.206% (13603/50000)\n",
      "0 100 Loss: 1.815 | Acc: 34.000% (34/100)\n",
      "1 100 Loss: 1.872 | Acc: 32.000% (64/200)\n",
      "2 100 Loss: 1.880 | Acc: 30.667% (92/300)\n",
      "3 100 Loss: 1.889 | Acc: 29.500% (118/400)\n",
      "4 100 Loss: 1.896 | Acc: 28.600% (143/500)\n",
      "5 100 Loss: 1.889 | Acc: 28.167% (169/600)\n",
      "6 100 Loss: 1.891 | Acc: 27.857% (195/700)\n",
      "7 100 Loss: 1.899 | Acc: 28.000% (224/800)\n",
      "8 100 Loss: 1.897 | Acc: 27.889% (251/900)\n",
      "9 100 Loss: 1.901 | Acc: 28.300% (283/1000)\n",
      "10 100 Loss: 1.903 | Acc: 28.636% (315/1100)\n",
      "11 100 Loss: 1.905 | Acc: 28.667% (344/1200)\n",
      "12 100 Loss: 1.909 | Acc: 28.462% (370/1300)\n",
      "13 100 Loss: 1.911 | Acc: 27.786% (389/1400)\n",
      "14 100 Loss: 1.905 | Acc: 28.267% (424/1500)\n",
      "15 100 Loss: 1.904 | Acc: 28.000% (448/1600)\n",
      "16 100 Loss: 1.902 | Acc: 28.118% (478/1700)\n",
      "17 100 Loss: 1.897 | Acc: 28.556% (514/1800)\n",
      "18 100 Loss: 1.896 | Acc: 28.737% (546/1900)\n",
      "19 100 Loss: 1.896 | Acc: 28.750% (575/2000)\n",
      "20 100 Loss: 1.897 | Acc: 28.571% (600/2100)\n",
      "21 100 Loss: 1.896 | Acc: 28.773% (633/2200)\n",
      "22 100 Loss: 1.897 | Acc: 28.739% (661/2300)\n",
      "23 100 Loss: 1.897 | Acc: 28.750% (690/2400)\n",
      "24 100 Loss: 1.893 | Acc: 28.960% (724/2500)\n",
      "25 100 Loss: 1.895 | Acc: 28.769% (748/2600)\n",
      "26 100 Loss: 1.895 | Acc: 28.556% (771/2700)\n",
      "27 100 Loss: 1.895 | Acc: 28.286% (792/2800)\n",
      "28 100 Loss: 1.899 | Acc: 28.069% (814/2900)\n",
      "29 100 Loss: 1.898 | Acc: 28.133% (844/3000)\n",
      "30 100 Loss: 1.896 | Acc: 28.419% (881/3100)\n",
      "31 100 Loss: 1.897 | Acc: 28.500% (912/3200)\n",
      "32 100 Loss: 1.898 | Acc: 28.485% (940/3300)\n",
      "33 100 Loss: 1.901 | Acc: 28.382% (965/3400)\n",
      "34 100 Loss: 1.901 | Acc: 28.514% (998/3500)\n",
      "35 100 Loss: 1.897 | Acc: 28.694% (1033/3600)\n",
      "36 100 Loss: 1.896 | Acc: 28.811% (1066/3700)\n",
      "37 100 Loss: 1.895 | Acc: 28.711% (1091/3800)\n",
      "38 100 Loss: 1.896 | Acc: 28.667% (1118/3900)\n",
      "39 100 Loss: 1.895 | Acc: 28.750% (1150/4000)\n",
      "40 100 Loss: 1.894 | Acc: 28.537% (1170/4100)\n",
      "41 100 Loss: 1.896 | Acc: 28.452% (1195/4200)\n",
      "42 100 Loss: 1.897 | Acc: 28.488% (1225/4300)\n",
      "43 100 Loss: 1.893 | Acc: 28.659% (1261/4400)\n",
      "44 100 Loss: 1.893 | Acc: 28.622% (1288/4500)\n",
      "45 100 Loss: 1.892 | Acc: 28.674% (1319/4600)\n",
      "46 100 Loss: 1.892 | Acc: 28.638% (1346/4700)\n",
      "47 100 Loss: 1.892 | Acc: 28.729% (1379/4800)\n",
      "48 100 Loss: 1.891 | Acc: 28.755% (1409/4900)\n",
      "49 100 Loss: 1.891 | Acc: 28.640% (1432/5000)\n",
      "50 100 Loss: 1.892 | Acc: 28.549% (1456/5100)\n",
      "51 100 Loss: 1.893 | Acc: 28.635% (1489/5200)\n",
      "52 100 Loss: 1.891 | Acc: 28.642% (1518/5300)\n",
      "53 100 Loss: 1.894 | Acc: 28.630% (1546/5400)\n",
      "54 100 Loss: 1.895 | Acc: 28.618% (1574/5500)\n",
      "55 100 Loss: 1.896 | Acc: 28.607% (1602/5600)\n",
      "56 100 Loss: 1.898 | Acc: 28.754% (1639/5700)\n",
      "57 100 Loss: 1.896 | Acc: 28.828% (1672/5800)\n",
      "58 100 Loss: 1.895 | Acc: 28.864% (1703/5900)\n",
      "59 100 Loss: 1.895 | Acc: 28.900% (1734/6000)\n",
      "60 100 Loss: 1.895 | Acc: 28.902% (1763/6100)\n",
      "61 100 Loss: 1.896 | Acc: 28.839% (1788/6200)\n",
      "62 100 Loss: 1.896 | Acc: 28.794% (1814/6300)\n",
      "63 100 Loss: 1.894 | Acc: 28.875% (1848/6400)\n",
      "64 100 Loss: 1.894 | Acc: 28.892% (1878/6500)\n",
      "65 100 Loss: 1.895 | Acc: 28.864% (1905/6600)\n",
      "66 100 Loss: 1.895 | Acc: 28.866% (1934/6700)\n",
      "67 100 Loss: 1.894 | Acc: 28.912% (1966/6800)\n",
      "68 100 Loss: 1.893 | Acc: 28.855% (1991/6900)\n",
      "69 100 Loss: 1.892 | Acc: 28.871% (2021/7000)\n",
      "70 100 Loss: 1.893 | Acc: 28.817% (2046/7100)\n",
      "71 100 Loss: 1.892 | Acc: 28.889% (2080/7200)\n",
      "72 100 Loss: 1.892 | Acc: 28.863% (2107/7300)\n",
      "73 100 Loss: 1.890 | Acc: 28.946% (2142/7400)\n",
      "74 100 Loss: 1.890 | Acc: 28.973% (2173/7500)\n",
      "75 100 Loss: 1.890 | Acc: 29.000% (2204/7600)\n",
      "76 100 Loss: 1.888 | Acc: 29.039% (2236/7700)\n",
      "77 100 Loss: 1.888 | Acc: 29.167% (2275/7800)\n",
      "78 100 Loss: 1.888 | Acc: 29.165% (2304/7900)\n",
      "79 100 Loss: 1.889 | Acc: 29.113% (2329/8000)\n",
      "80 100 Loss: 1.889 | Acc: 29.160% (2362/8100)\n",
      "81 100 Loss: 1.889 | Acc: 29.122% (2388/8200)\n",
      "82 100 Loss: 1.890 | Acc: 29.108% (2416/8300)\n",
      "83 100 Loss: 1.891 | Acc: 29.071% (2442/8400)\n",
      "84 100 Loss: 1.890 | Acc: 29.106% (2474/8500)\n",
      "85 100 Loss: 1.891 | Acc: 29.128% (2505/8600)\n",
      "86 100 Loss: 1.891 | Acc: 29.103% (2532/8700)\n",
      "87 100 Loss: 1.890 | Acc: 29.148% (2565/8800)\n",
      "88 100 Loss: 1.889 | Acc: 29.225% (2601/8900)\n",
      "89 100 Loss: 1.889 | Acc: 29.278% (2635/9000)\n",
      "90 100 Loss: 1.888 | Acc: 29.209% (2658/9100)\n",
      "91 100 Loss: 1.888 | Acc: 29.185% (2685/9200)\n",
      "92 100 Loss: 1.887 | Acc: 29.237% (2719/9300)\n",
      "93 100 Loss: 1.889 | Acc: 29.202% (2745/9400)\n",
      "94 100 Loss: 1.889 | Acc: 29.200% (2774/9500)\n",
      "95 100 Loss: 1.888 | Acc: 29.271% (2810/9600)\n",
      "96 100 Loss: 1.887 | Acc: 29.309% (2843/9700)\n",
      "97 100 Loss: 1.887 | Acc: 29.276% (2869/9800)\n",
      "98 100 Loss: 1.887 | Acc: 29.182% (2889/9900)\n",
      "99 100 Loss: 1.887 | Acc: 29.160% (2916/10000)\n",
      "\n",
      "Epoch: 3\n",
      "0 391 Loss: 1.936 | Acc: 32.812% (42/128)\n",
      "1 391 Loss: 1.947 | Acc: 31.641% (81/256)\n",
      "2 391 Loss: 1.906 | Acc: 32.552% (125/384)\n",
      "3 391 Loss: 1.918 | Acc: 32.227% (165/512)\n",
      "4 391 Loss: 1.910 | Acc: 32.344% (207/640)\n",
      "5 391 Loss: 1.928 | Acc: 31.120% (239/768)\n",
      "6 391 Loss: 1.921 | Acc: 31.250% (280/896)\n",
      "7 391 Loss: 1.911 | Acc: 31.543% (323/1024)\n",
      "8 391 Loss: 1.903 | Acc: 31.076% (358/1152)\n",
      "9 391 Loss: 1.897 | Acc: 31.016% (397/1280)\n",
      "10 391 Loss: 1.893 | Acc: 31.534% (444/1408)\n",
      "11 391 Loss: 1.908 | Acc: 31.055% (477/1536)\n",
      "12 391 Loss: 1.909 | Acc: 30.889% (514/1664)\n",
      "13 391 Loss: 1.913 | Acc: 30.804% (552/1792)\n",
      "14 391 Loss: 1.918 | Acc: 30.312% (582/1920)\n",
      "15 391 Loss: 1.925 | Acc: 30.078% (616/2048)\n",
      "16 391 Loss: 1.922 | Acc: 30.377% (661/2176)\n",
      "17 391 Loss: 1.920 | Acc: 30.078% (693/2304)\n",
      "18 391 Loss: 1.918 | Acc: 30.263% (736/2432)\n",
      "19 391 Loss: 1.917 | Acc: 30.156% (772/2560)\n",
      "20 391 Loss: 1.917 | Acc: 30.283% (814/2688)\n",
      "21 391 Loss: 1.915 | Acc: 30.149% (849/2816)\n",
      "22 391 Loss: 1.917 | Acc: 29.925% (881/2944)\n",
      "23 391 Loss: 1.913 | Acc: 29.915% (919/3072)\n",
      "24 391 Loss: 1.913 | Acc: 29.969% (959/3200)\n",
      "25 391 Loss: 1.913 | Acc: 30.018% (999/3328)\n",
      "26 391 Loss: 1.913 | Acc: 30.122% (1041/3456)\n",
      "27 391 Loss: 1.912 | Acc: 30.162% (1081/3584)\n",
      "28 391 Loss: 1.911 | Acc: 30.092% (1117/3712)\n",
      "29 391 Loss: 1.911 | Acc: 30.130% (1157/3840)\n",
      "30 391 Loss: 1.909 | Acc: 30.192% (1198/3968)\n",
      "31 391 Loss: 1.909 | Acc: 30.078% (1232/4096)\n",
      "32 391 Loss: 1.908 | Acc: 29.995% (1267/4224)\n",
      "33 391 Loss: 1.911 | Acc: 29.779% (1296/4352)\n",
      "34 391 Loss: 1.912 | Acc: 29.754% (1333/4480)\n",
      "35 391 Loss: 1.908 | Acc: 29.861% (1376/4608)\n",
      "36 391 Loss: 1.907 | Acc: 29.962% (1419/4736)\n",
      "37 391 Loss: 1.908 | Acc: 29.873% (1453/4864)\n",
      "38 391 Loss: 1.910 | Acc: 29.888% (1492/4992)\n",
      "39 391 Loss: 1.909 | Acc: 29.980% (1535/5120)\n",
      "40 391 Loss: 1.907 | Acc: 29.992% (1574/5248)\n",
      "41 391 Loss: 1.907 | Acc: 29.985% (1612/5376)\n",
      "42 391 Loss: 1.903 | Acc: 29.978% (1650/5504)\n",
      "43 391 Loss: 1.903 | Acc: 29.865% (1682/5632)\n",
      "44 391 Loss: 1.903 | Acc: 29.826% (1718/5760)\n",
      "45 391 Loss: 1.905 | Acc: 29.688% (1748/5888)\n",
      "46 391 Loss: 1.906 | Acc: 29.638% (1783/6016)\n",
      "47 391 Loss: 1.905 | Acc: 29.655% (1822/6144)\n",
      "48 391 Loss: 1.904 | Acc: 29.895% (1875/6272)\n",
      "49 391 Loss: 1.904 | Acc: 29.891% (1913/6400)\n",
      "50 391 Loss: 1.903 | Acc: 29.856% (1949/6528)\n",
      "51 391 Loss: 1.902 | Acc: 29.958% (1994/6656)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     train(epoch, resnet_tn)\n\u001b[1;32m      9\u001b[0m     test(epoch, resnet_tn)\n\u001b[1;32m     10\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[56], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epoch, net)\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m outputs \u001b[39m=\u001b[39m net(inputs)\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     16\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tdcomp1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[22], line 89\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     88\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)))\n\u001b[0;32m---> 89\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(out)\n\u001b[1;32m     90\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(out)\n\u001b[1;32m     91\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/tdcomp1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tdcomp1/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tdcomp1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[22], line 27\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     28\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out))\n\u001b[1;32m     29\u001b[0m     out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortcut(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/tdcomp1/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tltorch/factorized_layers/factorized_convolution.py:193\u001b[0m, in \u001b[0;36mFactorizedConv.forward\u001b[0;34m(self, x, indices)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_layers \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    192\u001b[0m     \u001b[39mif\u001b[39;00m indices \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 193\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_fun(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight(), bias\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, stride\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride\u001b[39m.\u001b[39;49mtolist(), \n\u001b[1;32m    194\u001b[0m                                 padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding\u001b[39m.\u001b[39;49mtolist(), dilation\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation\u001b[39m.\u001b[39;49mtolist())\n\u001b[1;32m    195\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mOnly one convolution was parametrized (n_layers=1) but tried to access \u001b[39m\u001b[39m{\u001b[39;00mindices\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tltorch/functional/convolution.py:49\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(x, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     47\u001b[0m             weight \u001b[39m=\u001b[39m tl\u001b[39m.\u001b[39mmoveaxis(weight\u001b[39m.\u001b[39mto_tensor(), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     48\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m             weight \u001b[39m=\u001b[39m weight\u001b[39m.\u001b[39;49mto_tensor()\n\u001b[1;32m     50\u001b[0m         \u001b[39mreturn\u001b[39;00m _CONVOLUTION[weight\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m](x, weight, bias\u001b[39m=\u001b[39mbias, stride\u001b[39m=\u001b[39mstride, padding\u001b[39m=\u001b[39mpadding, \n\u001b[1;32m     51\u001b[0m                                              dilation\u001b[39m=\u001b[39mdilation, groups\u001b[39m=\u001b[39mgroups)\n\u001b[1;32m     52\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tltorch/factorized_tensors/factorized_tensors.py:308\u001b[0m, in \u001b[0;36mTuckerTensor.to_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mto_tensor\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 308\u001b[0m     \u001b[39mreturn\u001b[39;00m tl\u001b[39m.\u001b[39;49mtucker_to_tensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecomposition)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorly/tucker_tensor.py:69\u001b[0m, in \u001b[0;36mtucker_to_tensor\u001b[0;34m(tucker_tensor, skip_factor, transpose_factors)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Converts the Tucker tensor into a full tensor\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m   full tensor of shape ``(factors[0].shape[0], ..., factors[-1].shape[0])``\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m core, factors \u001b[39m=\u001b[39m tucker_tensor\n\u001b[0;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m multi_mode_dot(core, factors, skip\u001b[39m=\u001b[39;49mskip_factor, transpose\u001b[39m=\u001b[39;49mtranspose_factors)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorly/backend/__init__.py:206\u001b[0m, in \u001b[0;36mBackendManager.dispatch_backend_method.<locals>.wrapped_backend_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_backend_method\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    203\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A dynamically dispatched method\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[39m    Returns the queried method from the currently set backend\"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\n\u001b[1;32m    207\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_THREAD_LOCAL_DATA\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbackend\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backend), name\n\u001b[1;32m    208\u001b[0m     )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorly/tenalg/core_tenalg/n_mode_product.py:132\u001b[0m, in \u001b[0;36mmulti_mode_dot\u001b[0;34m(tensor, matrix_or_vec_list, modes, skip, transpose)\u001b[0m\n\u001b[1;32m    130\u001b[0m     res \u001b[39m=\u001b[39m mode_dot(res, T\u001b[39m.\u001b[39mconj(T\u001b[39m.\u001b[39mtranspose(matrix_or_vec)), mode \u001b[39m-\u001b[39m decrement)\n\u001b[1;32m    131\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m     res \u001b[39m=\u001b[39m mode_dot(res, matrix_or_vec, mode \u001b[39m-\u001b[39;49m decrement)\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m T\u001b[39m.\u001b[39mndim(matrix_or_vec) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    135\u001b[0m     decrement \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorly/tenalg/core_tenalg/n_mode_product.py:73\u001b[0m, in \u001b[0;36mmode_dot\u001b[0;34m(tensor, matrix_or_vector, mode, transpose)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCan only take n_mode_product with a vector or a matrix.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProvided array of dimension \u001b[39m\u001b[39m{\u001b[39;00mT\u001b[39m.\u001b[39mndim(matrix_or_vector)\u001b[39m}\u001b[39;00m\u001b[39m not in [1, 2].\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[0;32m---> 73\u001b[0m res \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mdot(matrix_or_vector, unfold(tensor, mode))\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m vec:  \u001b[39m# We contracted with a vector, leading to a vector\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m vec_to_tensor(res, shape\u001b[39m=\u001b[39mnew_shape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorly/base.py:53\u001b[0m, in \u001b[0;36munfold\u001b[0;34m(tensor, mode)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munfold\u001b[39m(tensor, mode):\n\u001b[1;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the mode-`mode` unfolding of `tensor` with modes starting at `0`.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m        unfolded_tensor of shape ``(tensor.shape[mode], -1)``\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m tl\u001b[39m.\u001b[39;49mreshape(tl\u001b[39m.\u001b[39;49mmoveaxis(tensor, mode, \u001b[39m0\u001b[39;49m), (tensor\u001b[39m.\u001b[39;49mshape[mode], \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorly/backend/__init__.py:206\u001b[0m, in \u001b[0;36mBackendManager.dispatch_backend_method.<locals>.wrapped_backend_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_backend_method\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    203\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A dynamically dispatched method\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \n\u001b[1;32m    205\u001b[0m \u001b[39m    Returns the queried method from the currently set backend\"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\n\u001b[1;32m    207\u001b[0m         \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_THREAD_LOCAL_DATA\u001b[39m.\u001b[39;49m\u001b[39m__dict__\u001b[39;49m\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbackend\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backend), name\n\u001b[1;32m    208\u001b[0m     )(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet_tn = resnet_tn.to(device)\n",
    "\n",
    "optimizer = optim.SGD(resnet_tn.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train(epoch, resnet_tn)\n",
    "    test(epoch, resnet_tn)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdcomp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
