{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating deep neural networks with tensor decompositions\n",
    "\n",
    "This notebook demonstrates how to use TensorLy for accelerating convolutional layers in PyTorch. It is based on this [blog post](https://jacobgil.github.io/deeplearning/tensor-decompositions-deep-learning).\n",
    "\n",
    "We will show how to use Tucker and CP decompositions to find low rank approximations of convolutional layers.\n",
    "Akin to prunning, this will allow use to reduce the number of parameters as well as make them faster. While the low-rank approximation can decrease the accuracy, this can be restored by training further.\n",
    "\n",
    "Let's load a pretrained VGG-19 network, using torch vision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model = models.vgg19(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG-19 network has two parts:\n",
    "\n",
    "- A convolutional part, stored in model.features. It's composed of 2D Convolutions, ReLU and max pooling layers.\n",
    "   \n",
    "   We are going to modify this part. \n",
    " \n",
    " \n",
    "- A fully connected part, stored in model.classifier. \n",
    "   \n",
    "   It's composed of two large fully connected layers, and then a third fully connected layer that performs the final classification.\n",
    "\n",
    "Lets look at one of the convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "layer = model.features._modules['0']\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This layer performs 64 different convolutions (the output channels), \n",
    "\n",
    "where each convolution has a 3x3 spatial size and is done on an image with 3 channels (the input channels).\n",
    "\n",
    "If we call our convolutional kernel K, the input image X and the layer bias b, then the output from the layer, V will be:\n",
    "\n",
    "$$ V(x, y, t) = \\sum_i \\sum_j \\sum_sK(i, j, s, t)X(x-i, x-j, s) + b(t) $$ \n",
    "\n",
    "\n",
    "The weights of the layer are stored in a 4 dimensional tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(layer.weight.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorLy to apply tensor decompositions on this 4 dimensional weight tensor.\n",
    "\n",
    "\n",
    "## CP Decomposition on convolutional layers\n",
    "\n",
    "This part is based on the paper titled [Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition](https://arxiv.org/abs/1412.6553)\n",
    "\n",
    "The idea is to approximate our 4 dimensional convolutional kernel with a CP decomposition of rank R:\n",
    "\n",
    "$ K(i, j, s, t) = \\sum_{r=1}^R K^x_r(i)K^y_r(j)K^s_r(s)K^t_r(t) $.\n",
    "\n",
    "Plugging this into the formula for the convolutional layer output form above:\n",
    "\n",
    "$ V(x, y, t) = \\sum_r\\sum_i \\sum_j \\sum_sK^x_r(i)K^y_r(i)K^s_r(s)K^t_r(t)X(x-i, y-j, s) $\n",
    "$ = \\sum_rK^t_r(t) \\sum_i \\sum_j K^x_r(i)K^y_r(j)\\sum_sK^s_r(s)X(x-i, y-j, s) $ \n",
    "\n",
    "This gives us a recipe to do the convlution:\n",
    "\n",
    " 1. First do a point wise (1x1xS) convolution with the kernek $K(r)$.\n",
    " \n",
    " This reduces the number of input channels from S to R.\n",
    " The convolutions will next be done on a smaller number of channels, making them faster.\n",
    "\n",
    " 2. Perform seperable convolutions in the spatial dimensions with $K^x_r,K^y_r$. We can do this in PyTorch using grouped convolutions.\n",
    "\n",
    "    **Like in [mobilenets](https://arxiv.org/abs/1704.04861) the convolutions are depthwise seperable, done in each channel separately.**\n",
    "    **Unlike mobilenets the convolutions are also separable in the spatial dimensions.**\n",
    "\n",
    " 3. Do another pointwise convolution to change the number of channels from R to T\n",
    " 4. Finally, add the bias.\n",
    " \n",
    " \n",
    "Notice the combination of pointwise and depthwise convolutions like in mobilenets. While with mobilenets you have to train a network from scratch to get this structure, here we can decompose an existing layer into this form.\n",
    "\n",
    "As with mobile nets, to get the most speedup you will need a platform that has an efficient implementation of depthwise separable convolutions.\n",
    "\n",
    "Now we can write a function that receives a PyTorch Conv2D layer, and creates the CP decompisition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n",
      "Using pytorch backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorly as tl\n",
    "from tensorly.decomposition import parafac\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tl.set_backend('pytorch')\n",
    "\n",
    "def cp_decomposition_conv_layer(layer, rank):\n",
    "    \"\"\" Gets a conv layer and a target rank, \n",
    "        returns a nn.Sequential object with the decomposition\n",
    "    \"\"\"\n",
    "    # Perform CP decomposition on the layer weight tensorly. \n",
    "    last, first, vertical, horizontal = \\\n",
    "        parafac(layer.weight.data, rank=rank, init='svd')\n",
    "\n",
    "    pointwise_s_to_r_layer = torch.nn.Conv2d(in_channels=first.shape[0], \\\n",
    "            out_channels=first.shape[1], kernel_size=1, stride=1, padding=0, \n",
    "            dilation=layer.dilation, bias=False)\n",
    "\n",
    "    depthwise_vertical_layer = torch.nn.Conv2d(in_channels=vertical.shape[1], \n",
    "            out_channels=vertical.shape[1], kernel_size=(vertical.shape[0], 1),\n",
    "            stride=1, padding=(layer.padding[0], 0), dilation=layer.dilation,\n",
    "            groups=vertical.shape[1], bias=False)\n",
    "\n",
    "    depthwise_horizontal_layer = \\\n",
    "        torch.nn.Conv2d(in_channels=horizontal.shape[1],\n",
    "            out_channels=horizontal.shape[1], \n",
    "            kernel_size=(1, horizontal.shape[0]), stride=layer.stride,\n",
    "            padding=(0, layer.padding[0]), \n",
    "            dilation=layer.dilation, groups=horizontal.shape[1], bias=False)\n",
    "\n",
    "    pointwise_r_to_t_layer = torch.nn.Conv2d(in_channels=last.shape[1], \\\n",
    "            out_channels=last.shape[0], kernel_size=1, stride=1,\n",
    "            padding=0, dilation=layer.dilation, bias=True)\n",
    "\n",
    "    pointwise_r_to_t_layer.bias.data = layer.bias.data\n",
    "\n",
    "    depthwise_horizontal_layer.weight.data = \\\n",
    "        torch.transpose(horizontal, 1, 0).unsqueeze(1).unsqueeze(1)\n",
    "    depthwise_vertical_layer.weight.data = \\\n",
    "        torch.transpose(vertical, 1, 0).unsqueeze(1).unsqueeze(-1)\n",
    "    pointwise_s_to_r_layer.weight.data = \\\n",
    "        torch.transpose(first, 1, 0).unsqueeze(-1).unsqueeze(-1)\n",
    "    pointwise_r_to_t_layer.weight.data = last.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    new_layers = [pointwise_s_to_r_layer, depthwise_vertical_layer, \\\n",
    "                    depthwise_horizontal_layer, pointwise_r_to_t_layer]\n",
    "    \n",
    "    return nn.Sequential(*new_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result when applying this to our pretrained layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before the decomposition:\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "* After the decomposition:\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): Conv2d(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), groups=16, bias=False)\n",
      "  (2): Conv2d(16, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), groups=16, bias=False)\n",
      "  (3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('* Before the decomposition:')\n",
    "print(layer)\n",
    "layer_cp_decomposed = cp_decomposition_conv_layer(layer, rank=16)\n",
    "print('\\n* After the decomposition:')\n",
    "print(layer_cp_decomposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tucker decomposition on convolutional layers\n",
    "\n",
    "The idea here is from the paper titled [*Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications*](https://arxiv.org/abs/1511.06530).\n",
    "\n",
    "Getting back to our kernel tensor K, we can approximate it using the Tucker decomposition:\n",
    "$ K(i, j, s, t) = \\sum_{r_1=1}^{R_1}\\sum_{r_2=1}^{R_2}\\sum_{r_3=1}^{R_3}\\sum_{r_4=1}^{R_4}\\sigma_{r_1 r_2 r_3 r_4} K^x_{r1}(i)K^y_{r2}(j)K^s_{r3}(s)K^t_{r4}(t) $\n",
    "\n",
    "The Tucker decomposition has the useful property that it doesn't have to be decomposed along all the axis (modes).\n",
    "\n",
    "Since the spatial dimensions are already quite small (3x3), it doesn't make a lot of sense to decompose those dimensions.\n",
    "\n",
    "We can perform the decomposition along the input and output channels instead (a mode-2 decomposition):\n",
    "\n",
    "$K(i, j, s, t) = \\sum_{r_3=1}^{R_3}\\sum_{r_4=1}^{R_4}\\sigma_{i j r_3 r_4}(j)K^s_{r3}(s)K^t_{r4}(t)$\n",
    "\n",
    "Like for CP decomposition, lets write the convolution formula and plug in the kernel decomposition: \n",
    "$ V(x, y, t) = \\sum_i \\sum_j \\sum_s\\sum_{r_3=1}^{R_3}\\sum_{r_4=1}^{R_4}\\sigma_{(i)(j) r_3 r_4}K^s_{r3}(s)K^t_{r4}(t)X(x-i, y-j, s) = \\sum_i \\sum_j \\sum_{r_4=1}^{R_4}\\sum_{r_3=1}^{R_3}K^t_{r4}(t)\\sigma_{(i)(j) r_3 r_4} \\sum_s\\ K^s_{r3}(s)X(x-i, y-j, s) $ \n",
    "\n",
    "This gives us the following recipe for doing the convolution with Tucker Decomposition:\n",
    "\n",
    " 1. Point wise convolution with $K^s_{r3}(s)$ for reducing the number of channels from S to $R_3$.\n",
    "\n",
    " 2. Regular (not separable) convolution with $\\sigma_{(i)(j) r_3 r_4} $.\n",
    " Instead of S input channels and T output channels like the original layer had,\n",
    " this convolution has $R_3$ input channels and $R_4$ output channels. If these ranks are smaller than S and T, this is were the speed gain comes from.\n",
    "\n",
    " 3. Pointwise convolution with $K^t_{r4}(t)$ to get back to T output channels like the original convolution.\n",
    " 4. Add the bias.\n",
    "\n",
    "Now we can write a function that receives a PyTorch Conv2D layer, and creates the tucker decompisition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorly.decomposition import partial_tucker\n",
    "\n",
    "def tucker_decomposition_conv_layer(layer, ranks):\n",
    "    \"\"\" Gets a conv layer, \n",
    "        returns a nn.Sequential object with the Tucker decomposition.\n",
    "    \"\"\"\n",
    "    core, [last, first] = \\\n",
    "        partial_tucker(layer.weight.data, \\\n",
    "            modes=[0, 1], ranks=ranks, init='svd')\n",
    "\n",
    "    # A pointwise convolution that reduces the channels from S to R3\n",
    "    first_layer = torch.nn.Conv2d(in_channels=first.shape[0], \\\n",
    "            out_channels=first.shape[1], kernel_size=1,\n",
    "            stride=1, padding=0, dilation=layer.dilation, bias=False)\n",
    "\n",
    "    # A regular 2D convolution layer with R3 input channels \n",
    "    # and R3 output channels\n",
    "    core_layer = torch.nn.Conv2d(in_channels=core.shape[1], \\\n",
    "            out_channels=core.shape[0], kernel_size=layer.kernel_size,\n",
    "            stride=layer.stride, padding=layer.padding, dilation=layer.dilation,\n",
    "            bias=False)\n",
    "\n",
    "    # A pointwise convolution that increases the channels from R4 to T\n",
    "    last_layer = torch.nn.Conv2d(in_channels=last.shape[1], \\\n",
    "                                 out_channels=last.shape[0], kernel_size=1, stride=1,\n",
    "                                 padding=0, dilation=layer.dilation, bias=True)\n",
    "\n",
    "    last_layer.bias.data = layer.bias.data\n",
    "\n",
    "\n",
    "    first_layer.weight.data = \\\n",
    "        torch.transpose(first, 1, 0).unsqueeze(-1).unsqueeze(-1)\n",
    "    last_layer.weight.data = last.unsqueeze(-1).unsqueeze(-1)\n",
    "    core_layer.weight.data = core\n",
    "\n",
    "\n",
    "    new_layers = [first_layer, core_layer, last_layer]\n",
    "    return nn.Sequential(*new_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this to our same pretrained layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Before the decomposition:\n",
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "\n",
      "* After the decomposition:\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('* Before the decomposition:')\n",
    "print(layer)\n",
    "layer_tucker_decomposed = tucker_decomposition_conv_layer(layer, ranks=[16, 16])\n",
    "print('\\n* After the decomposition:')\n",
    "print(layer_tucker_decomposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Being careful with strides and dilations\n",
    "\n",
    "Sometimes the convolutional layer will have a stride.\n",
    "In this case, the last convolution among the decomposed parts, should have the original layer's stride, and the rest should have just a stride of one.\n",
    "Since the last part of these decompositions is a 1x1 convolution, we can set the stride of the layer before it instead, to make it a bit faster. \n",
    "\n",
    "In case of dilations, all the spatial layers should preserve the dilation rate of the original layer.\n",
    "\n",
    "\n",
    "## Accelerating the entire network\n",
    "To accelerate the network, we can loop over the layers, and replace convolutional layers with their decomposition.\n",
    "To show the decomposition is reducing the numebr of parameters, we can count the number of parameters in the network before and after the decomposition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters before the decomposition: 20024384\n",
      "Number of parameters after the decomposition: 7278656\n",
      "Ratio: 0.3634896334389113\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "def count_params(network):\n",
    "    return np.sum(np.prod(p.size()) for p in network.parameters())\n",
    "\n",
    "# Apply the transformation to all the Convolutional layers\n",
    "for index, module in enumerate(model.features._modules):\n",
    "    if index > 0:\n",
    "        layer = model.features._modules[module]\n",
    "        if type(layer) is torch.nn.Conv2d:\n",
    "            ranks = [layer.weight.size(0)//2, layer.weight.size(1)//2]\n",
    "            model.features._modules[module] = tucker_decomposition_conv_layer(layer, ranks)\n",
    "\n",
    "# Load another net without modification for comparison\n",
    "original_model = models.vgg19(pretrained=True).eval()\n",
    "\n",
    "params_before = count_params(original_model.features)\n",
    "params_after = count_params(model.features)\n",
    "\n",
    "print('Number of parameters before the decomposition: {params_before}'.format(params_before=params_before))\n",
    "print('Number of parameters after the decomposition: {params_after}'.format(params_after=params_after))\n",
    "print('Ratio: {ratio}'.format(ratio=float(params_after)/params_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our compressed network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the network still is sensible after the decomposition, we will forward pass an image and check what is the highest scoring category before, and after the decomposition.\n",
    "\n",
    "Let's take as an example this image:\n",
    "\n",
    "![cat image](../images/cat.thumb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define some helper functions to load the data etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def image_from_url(url):\n",
    "    \"\"\"A function for loading an image from a URL, and preprocessing it \n",
    "    for the VGG19 network\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    \n",
    "    resize = transforms.Resize((224, 224))\n",
    "    normalize = transforms.NorJmalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    to_tensor = transforms.ToTensor()    \n",
    "    \n",
    "    return transforms.Compose([resize, transforms.ToTensor(), normalize])(img)\n",
    "\n",
    "def image_from_disk(path):\n",
    "    \"\"\"A function for loading an image from a URL, and preprocessing it \n",
    "    for the VGG19 network\"\"\"\n",
    "    img = Image.open(path)\n",
    "    \n",
    "    resize = transforms.Resize((224, 224))\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    to_tensor = transforms.ToTensor()    \n",
    "    \n",
    "    return transforms.Compose([resize, transforms.ToTensor(), normalize])(img)\n",
    "\n",
    "def predict(net, img):\n",
    "    \"\"\"Passes an image through the net and returns the prediction\"\"\"\n",
    "    input = Variable(img.unsqueeze(0), volatile=True)\n",
    "    softmax = torch.nn.Softmax(1)\n",
    "    output = softmax(net(input))\n",
    "    return output.data.numpy()\n",
    "\n",
    "# Use this if you want to test the model on an image from the web:\n",
    "#url = \"http://www.image-net.org/nodes/10/02123045/6c/6c34fe7c9d846c33a2f1a9b47a766b44ab4ec70d.thumb\"\n",
    "#img = image_from_url(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply both the original model and the model after decomposition to our cat image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction BEFORE decomposition: 'tabby, tabby cat' with score: 0.602\n",
      "Prediction AFTER  decomposition: 'tabby, tabby cat' with score: 0.419\n"
     ]
    }
   ],
   "source": [
    "img = image_from_disk('../images/cat.thumb')\n",
    "\n",
    "with open('../resources/imagenet_classes.json', 'r') as f:\n",
    "    imagenet_classes = json.load(f)\n",
    "\n",
    "output = predict(original_model, img)\n",
    "prob, category = np.max(output), np.argmax(output)\n",
    "category = imagenet_classes[str(category)]\n",
    "\n",
    "print(\"Prediction BEFORE decomposition: '{category}' with score: {prob:.03}\".format(category=category, prob=prob))\n",
    "\n",
    "output = predict(model, img)\n",
    "prob, category = np.max(output), np.argmax(output)\n",
    "category = imagenet_classes[str(category)]\n",
    "\n",
    "print(\"Prediction AFTER  decomposition: '{category}' with score: {prob:.03}\".format(category=category, prob=prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cat image](../images/cat.thumb)\n",
    "\n",
    "Note that, although the class predicted is the same, the associated score is lower. To restore the original accuracy, we would have to retrain (fine-tune) the network for a few iterations on the original dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
