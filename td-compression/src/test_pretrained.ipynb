{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import lightning.pytorch as pl\n",
    "import lightning.pytorch.loggers as pl_loggers\n",
    "from torchvision import models\n",
    "import copy\n",
    "import tltorch\n",
    "import tensorly as tl\n",
    "from models import resnet, vgg\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 4\n",
    "# reproducibility\n",
    "SEED = 42\n",
    "# reproducibility\n",
    "pl.seed_everything(42)\n",
    "# allow tf32 (TENSOR CORES)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "torch.backends.cudnn.deterministic = True  # deterministic cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet.ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names = ['layer1.0.conv1', 'layer1.0.conv2', 'layer1.1.conv1', 'layer1.1.conv2', 'layer2.0.conv1', 'layer2.0.conv2', 'layer2.1.conv1', 'layer2.1.conv2', 'layer3.0.conv1', 'layer3.0.conv2', 'layer3.1.conv1', 'layer3.1.conv2', 'layer4.0.conv1', 'layer4.0.conv2', 'layer4.1.conv1', 'layer4.1.conv2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorize_layer(\n",
    "    module,\n",
    "    factorization='tucker',\n",
    "    rank=None,\n",
    "    decompose_weights=False,\n",
    "    vbmf=0,\n",
    "    implementation='reconstructed'\n",
    "):\n",
    "    init_std = None if decompose_weights else 0.01\n",
    "    #decomposition_kwargs = {'init': 'random'} if factorization == 'cp' else {}\n",
    "    fixed_rank_modes = 'spatial' if factorization == 'tucker' else None\n",
    "    # implementation see: https://github.com/tensorly/torch/blob/d27d58f16101b7ecc431372eb218ceda59d8b043/tltorch/functional/convolution.py#L286\n",
    "    \n",
    "    if rank is None and vbmf == 0 and factorization != 'tucker':\n",
    "        raise ValueError('rank must be specified for non-tucker factorization')\n",
    "    \n",
    "    if not decompose_weights:\n",
    "        vbmf = 0 \n",
    "\n",
    "    if type(module) == torch.nn.modules.conv.Conv2d:\n",
    "        # rank selection\n",
    "        \n",
    "        if rank is not None:\n",
    "            ranks = rank\n",
    "        else:\n",
    "            weights = module.weight.data\n",
    "            ranks = [weights.shape[0]//3, weights.shape[1]//3, weights.shape[2], weights.shape[3]]\n",
    "        \n",
    "        # factorize from conv layer\n",
    "        fact_module = tltorch.FactorizedConv.from_conv(\n",
    "            module,\n",
    "            rank=ranks,\n",
    "            decompose_weights=decompose_weights,\n",
    "            factorization=factorization,\n",
    "            fixed_rank_modes=fixed_rank_modes,\n",
    "            implementation=implementation\n",
    "        )\n",
    "    elif type(module) == torch.nn.modules.linear.Linear:\n",
    "        fact_module = tltorch.FactorizedLinear.from_linear(\n",
    "            module,\n",
    "            n_tensorized_modes=3,\n",
    "            rank=rank,\n",
    "            factorization=factorization,\n",
    "            decompose_weights=decompose_weights,\n",
    "            fixed_rank_modes=fixed_rank_modes\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(type(module))\n",
    "    \n",
    "    if init_std:\n",
    "        print('Initializing with std')\n",
    "        fact_module.weight.normal_(0, init_std)\n",
    "    \n",
    "    return fact_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factorizing: layer1.0.conv1\n",
      "factorizing: layer1.0.conv2\n",
      "factorizing: layer1.1.conv1\n",
      "factorizing: layer1.1.conv2\n",
      "factorizing: layer2.0.conv1\n",
      "factorizing: layer2.0.conv2\n",
      "factorizing: layer2.1.conv1\n",
      "factorizing: layer2.1.conv2\n",
      "factorizing: layer3.0.conv1\n",
      "factorizing: layer3.0.conv2\n",
      "factorizing: layer3.1.conv1\n",
      "factorizing: layer3.1.conv2\n",
      "factorizing: layer4.0.conv1\n",
      "factorizing: layer4.0.conv2\n",
      "factorizing: layer4.1.conv1\n",
      "factorizing: layer4.1.conv2\n"
     ]
    }
   ],
   "source": [
    "fact_model = copy.deepcopy(model)\n",
    "tn_decomp = 'tt'\n",
    "rank = 0.8\n",
    "decompose_weights = True\n",
    "implementation = 'reconstructed'\n",
    "\n",
    "# factorize resnet\n",
    "for i, (name, module) in enumerate(model.named_modules()):\n",
    "    if name in layer_names:\n",
    "        \n",
    "        print(f'factorizing: {name}')\n",
    "        fact_module = factorize_layer(\n",
    "            module=module, \n",
    "            factorization=tn_decomp, \n",
    "            rank=rank,\n",
    "            decompose_weights=decompose_weights,\n",
    "            implementation=implementation\n",
    "        )\n",
    "        layer, block, conv = name.split('.')\n",
    "        conv_to_replace = getattr(getattr(fact_model, layer), block)\n",
    "        setattr(conv_to_replace, conv, fact_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(1, 144, 13, 144, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(64, 3, 3, 64), rank=(1, 64, 13, 39, 1))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(1, 144, 13, 144, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(64, 3, 3, 64), rank=(1, 64, 13, 39, 1))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(1, 144, 13, 144, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(64, 3, 3, 64), rank=(1, 64, 13, 39, 1))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=64, out_channels=64, kernel_size=(3, 3), rank=(1, 144, 13, 144, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(64, 3, 3, 64), rank=(1, 64, 13, 39, 1))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=64, out_channels=128, kernel_size=(3, 3), rank=(1, 139, 12, 272, 1), order=2, stride=[2, 2], padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(64, 3, 3, 128), rank=(1, 64, 12, 36, 1))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=128, out_channels=128, kernel_size=(3, 3), rank=(1, 338, 15, 338, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(128, 3, 3, 128), rank=(1, 128, 15, 45, 1))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=128, out_channels=128, kernel_size=(3, 3), rank=(1, 338, 15, 338, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(128, 3, 3, 128), rank=(1, 128, 15, 45, 1))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=128, out_channels=128, kernel_size=(3, 3), rank=(1, 338, 15, 338, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(128, 3, 3, 128), rank=(1, 128, 15, 45, 1))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=128, out_channels=256, kernel_size=(3, 3), rank=(1, 310, 14, 613, 1), order=2, stride=[2, 2], padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(128, 3, 3, 256), rank=(1, 128, 14, 42, 1))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=256, out_channels=256, kernel_size=(3, 3), rank=(1, 763, 18, 763, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(256, 3, 3, 256), rank=(1, 256, 18, 54, 1))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=256, out_channels=256, kernel_size=(3, 3), rank=(1, 763, 18, 763, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(256, 3, 3, 256), rank=(1, 256, 18, 54, 1))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=256, out_channels=256, kernel_size=(3, 3), rank=(1, 763, 18, 763, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(256, 3, 3, 256), rank=(1, 256, 18, 54, 1))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=256, out_channels=512, kernel_size=(3, 3), rank=(1, 668, 15, 1328, 1), order=2, stride=[2, 2], padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(256, 3, 3, 512), rank=(1, 256, 15, 45, 1))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=512, out_channels=512, kernel_size=(3, 3), rank=(1, 1656, 19, 1656, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(512, 3, 3, 512), rank=(1, 512, 19, 57, 1))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): FactorizedConv(\n",
       "        in_channels=512, out_channels=512, kernel_size=(3, 3), rank=(1, 1656, 19, 1656, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(512, 3, 3, 512), rank=(1, 512, 19, 57, 1))\n",
       "      )\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): FactorizedConv(\n",
       "        in_channels=512, out_channels=512, kernel_size=(3, 3), rank=(1, 1656, 19, 1656, 1), order=2, padding=[1, 1], bias=False\n",
       "        (weight): TTTensor(shape=(512, 3, 3, 512), rank=(1, 512, 19, 57, 1))\n",
       "      )\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.66 ms ± 444 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model(torch.randn(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.5 ms ± 715 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fact_model(torch.randn(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdcomp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
